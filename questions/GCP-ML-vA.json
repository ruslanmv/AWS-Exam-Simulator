[
    {
        "question": "You are building an ML model to detect anomalies in  real-time sensor data. You will use Pub/Sub to han dle incoming requests. You want to store the results fo r analytics and visualization. How should you confi gure the pipeline?",
        "options": [
            "A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery",
            "B. 1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable",
            "C. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions",
            "D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage"
        ],
        "correct": "A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery",
        "explanation": "C. Downsample the data with upweighting to create a sample with 10% positive examples.\n\nExplanation:\n\nThe correct answer is C. Downsample the data with upweighting to create a sample with 10% positive examples. This approach involves reducing the number of negative examples while keeping all the positive examples and assigning a higher weight to the positive examples. This method is effective in addressing class imbalance issues, especially when the positive class is rare.\n\nOption A is incorrect because generating artificial positive examples may not accurately represent the underlying data distribution and may lead to overfitting.\n\nOption B is incorrect because a convolutional neural network with max pooling and softmax activation is not specifically designed to handle class imbalance problems. While it may work for some classification tasks, it is not the best approach for this particular problem.\n\nOption D is incorrect because removing negative examples until the numbers of positive and negative examples are equal can lead to loss of valuable information and may not accurately represent the underlying data distribution.\n\nIn summary, downsampling the data with upweighting is the most effective approach to resolve the class imbalance problem in this scenario.",
        "references": ""
    },
    {
        "question": "Your organization wants to make its internal shuttl e service route more efficient. The shuttles curren tly stop at all pick-up points across the city every 30 minutes  between 7 am and 10 am. The development team has already built an application on Google Kubernetes E ngine that requires users to confirm their presence  and shuttle station one day in advance. What approach s hould you take?",
        "options": [
            "A. 1. Build a tree-based regression model that predi cts how many passengers will be picked up at each s huttle",
            "B. 1. Build a tree-based classification model that p redicts whether the shuttle should pick up passenge rs at",
            "C. 1. Define the optimal route as the shortest route  that passes by all shuttle stations with confirmed",
            "D. 1. Build a reinforcement learning model with tree -based classification models that predict the prese nce of"
        ],
        "correct": "C. 1. Define the optimal route as the shortest route  that passes by all shuttle stations with confirmed",
        "explanation": "Explanation:\n\nThe correct answer is C because it defines the optimal route as the shortest route that passes by all shuttle stations with confirmed passengers. This approach is efficient because it takes into account the actual demand for the shuttle service and optimizes the route accordingly.\n\nOption A is incorrect because building a tree-based regression model to predict the number of passengers at each shuttle stop may not directly address the problem of optimizing the route. While it may provide some insights, it may not lead to the most efficient route.\n\nOption B is also incorrect because building a tree-based classification model to predict whether the shuttle should pick up passengers at each stop may not consider the actual demand and may lead to inefficient routes.\n\nOption D is incorrect because building a reinforcement learning model with tree-based classification models may be overly complex and may not provide a straightforward solution to the problem. Reinforcement learning models are typically used in more complex scenarios where the goal is to learn from interactions with an environment, which is not the case here.\n\nIn summary, the correct answer is C because it provides a simple and efficient solution to the problem by defining the optimal route as the shortest route that passes by all shuttle stations with confirmed passengers.",
        "references": ""
    },
    {
        "question": "You were asked to investigate failures of a product ion line component based on sensor readings. After receiving the dataset, you discover that less than 1% of the readings are positive examples representi ng failure incidents. You have tried to train several classifi cation models, but none of them converge. How shoul d you resolve the class imbalance problem?",
        "options": [
            "A. Use the class distribution to generate 10% positi ve examples.",
            "B. Use a convolutional neural network with max pooling  and softmax activation. C. Downsample the data with upweighting to create a sa mple with 10% positive examples.",
            "D. Remove negative examples until the numbers of pos itive and negative examples are equal."
        ],
        "correct": "",
        "explanation": "Explanation:\nThe correct answer is A. Use the AI Platform custom containers feature to receive training jobs using any framework. This is because AI Platform provides a managed service that allows data scientists to submit training jobs using any framework, including Keras, PyTorch, theano, Scikit-learn, and custom libraries. The custom containers feature of AI Platform allows data scientists to package their training code and dependencies into a container that can be run on the managed service, without requiring administrative expertise.\n\nOption B is incorrect because Kubeflow is a platform for machine learning that provides a set of tools for building, deploying, and managing machine learning workflows. While Kubeflow can be used to run training jobs, it requires administrative expertise to set up and manage, which is not what the question is looking for.\n\nOption C is incorrect because creating a library of VM images on Compute Engine and publishing them on a centralized repository does not provide a managed service for submitting training jobs. This approach would still require administrative expertise to manage the VM images and the underlying infrastructure.\n\nOption D is incorrect because Slurm is a workload manager that is typically used in high-performance computing environments, and is not well-suited for machine learning workloads. Additionally, setting up Slurm would require administrative expertise, which is not what the question is looking for.",
        "references": ""
    },
    {
        "question": "You want to rebuild your ML pipeline for structured  data on Google Cloud. You are using PySpark to con duct data transformations at scale, but your pipelines a re taking over 12 hours to run. To speed up develop ment and pipeline run time, you want to use a serverless too l and SQL syntax. You have already moved your raw d ata into Cloud Storage. How should you build the pipeli ne on Google Cloud while meeting the speed and processing requirements?",
        "options": [
            "A. Use Data Fusion's GUI to build the transformation  pipelines, and then write the data into BigQuery.",
            "B. Convert your PySpark into SparkSQL queries to tra nsform the data, and then run your pipeline on Data proc",
            "C. Ingest your data into Cloud SQL, convert your PyS park commands into SQL queries to transform the dat a,",
            "D. Ingest your data into BigQuery using BigQuery Loa d, convert your PySpark commands into BigQuery SQL"
        ],
        "correct": "D. Ingest your data into BigQuery using BigQuery Loa d, convert your PySpark commands into BigQuery SQL",
        "explanation": "Explanation:\n\nThe correct answer is A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery. Here's why:\n\n1. Dataflow: Pub/Sub is used to handle incoming requests, and Dataflow is a fully-managed service for processing and analyzing data in real-time. It's the best choice for processing the sensor data in real-time.\n\n2. AI Platform: Since the task involves building an ML model to detect anomalies, AI Platform is the correct choice. It provides a managed platform for building, deploying, and managing machine learning models.\n\n3. BigQuery: After processing and analyzing the data, the results need to be stored for analytics and visualization. BigQuery is a fully-managed enterprise data warehouse that allows you to easily analyze all your data using SQL-like queries. It's the best choice for storing the results for analytics and visualization.\n\nThe other options are incorrect because:\n\nB. DataProc is a fully-managed service for running Apache Spark and Hadoop workloads, which is not relevant to this task. AutoML is a suite of machine learning tools, but it's not a managed platform for building, deploying, and managing machine learning models like AI Platform. Cloud Bigtable is a NoSQL database service, but it's not suitable for storing results for analytics and visualization.\n\nC. BigQuery is not used for processing real-time sensor data, and AutoML is not a managed platform for building, deploying, and managing machine learning models. Cloud",
        "references": ""
    },
    {
        "question": "You manage a team of data scientists who use a clou d-based backend system to submit training jobs. Thi s system has become very difficult to administer, and  you want to use a managed service instead. The dat a scientists you work with use many different framewo rks, including Keras, PyTorch, theano, Scikit-learn , and custom libraries. What should you do?",
        "options": [
            "A. Use the AI Platform custom containers feature to receive training jobs using any framework.",
            "B. Configure Kubeflow to run on Google Kubernetes En gine and receive training jobs through TF Job.",
            "C. Create a library of VM images on Compute Engine, and publish these images on a centralized repositor y.",
            "D. Set up Slurm workload manager to receive jobs tha t can be scheduled to run on your cloud infrastruct ure."
        ],
        "correct": "A. Use the AI Platform custom containers feature to receive training jobs using any framework.",
        "explanation": "Explanation:\nThe correct answer is D. Ingest your data into BigQuery using BigQuery Load, convert your PySpark commands into BigQuery SQL.\n\nThis is because BigQuery is a serverless, fully-managed enterprise data warehouse that allows you to analyze all your data using SQL-like queries. By ingesting your data into BigQuery, you can leverage its scalability and performance to speed up your pipeline run time. Additionally, converting your PySpark commands into BigQuery SQL allows you to take advantage of BigQuery's SQL-like syntax, which is more efficient and scalable than PySpark.\n\nOption A is incorrect because Data Fusion is a fully-managed, cloud-based data integration service that allows you to integrate data from multiple sources, but it is not designed for large-scale data processing and analytics. While it provides a GUI for building transformation pipelines, it is not the best choice for speeding up pipeline run time.\n\nOption B is incorrect because while SparkSQL can be used to transform data, running it on Data Proc (a fully-managed Spark service) would still require managing Spark clusters and would not provide the same level of scalability and performance as BigQuery.\n\nOption C is incorrect because Cloud SQL is a fully-managed relational database service that allows you to run MySQL, PostgreSQL, and SQL Server databases in the cloud, but it is not designed for large-scale data processing and analytics. Converting PySpark commands into SQL queries would not take advantage of Cloud SQL's strengths, and would not provide the same level",
        "references": ""
    },
    {
        "question": "end ML pipeline on Google Cloud to classify whether  an image contains your company's product. Expectin g the release of new products You work for an online reta il company that is creating a visual search engine.  You have set up an end-to- retraining functionality in the pipeline so that ne w data can be fed into your ML models. You also wan t to use AI Platform's continuous evaluation service to ensure that the models have high accuracy on in the near f uture, you configured a your test dataset. What should you do?",
        "options": [
            "A. Keep the original test dataset unchanged even if newer products are incorporated into retraining.",
            "B. Extend your test dataset with images of the newer  products when they are introduced to retraining.",
            "C. Replace your test dataset with images of the newe r products when they are introduced to retraining.",
            "D. Update your test dataset with images of the newer  products when your evaluation metrics drop below a  pre-"
        ],
        "correct": "B. Extend your test dataset with images of the newer  products when they are introduced to retraining.",
        "explanation": "Explanation:\nThe correct answer is B. Extend your test dataset with images of the newer products when they are introduced to retraining. This is because the test dataset should be representative of the data that the model will encounter in the real world. As new products are introduced, the test dataset should be updated to include images of these products. This ensures that the model is evaluated on a dataset that is relevant to its intended use case.\n\nOption A is incorrect because keeping the original test dataset unchanged would mean that the model is not evaluated on the new products, which could lead to a decrease in accuracy.\n\nOption C is incorrect because replacing the test dataset with images of the newer products would mean that the model is only evaluated on the new products, and not on the original products. This could lead to a biased evaluation of the model.\n\nOption D is incorrect because updating the test dataset only when the evaluation metrics drop below a certain threshold would mean that the model is not continuously evaluated on the new products. This could lead to a delay in detecting any issues with the model's accuracy.\n\nTherefore, the correct answer is B, which ensures that the test dataset is continuously updated to include images of new products, and that the model is evaluated on a representative dataset.",
        "references": ""
    },
    {
        "question": "You need to build classification workflows over sev eral structured datasets currently stored in BigQue ry. Because you will be performing the classification s everal times, you want to complete the following st eps without writing code: exploratory data analysis, feature selection, model  building, training, and hyperparameter tuning and serving. What should you do?",
        "options": [
            "A. Configure AutoML Tables to perform the classifica tion task.",
            "B. Run a BigQuery ML task to perform logistic regres sion for the classification.",
            "C. Use AI Platform Notebooks to run the classificati on model with pandas library.",
            "D. Use AI Platform to run the classification model j ob configured for hyperparameter tuning."
        ],
        "correct": "A. Configure AutoML Tables to perform the classifica tion task.",
        "explanation": "Explanation:\n\nThe correct answer is A, Configure AutoML Tables to perform the classification task. AutoML Tables is a fully managed service that allows users to train high-quality machine learning models on structured data, including BigQuery datasets, without writing code. It provides an automated workflow for exploratory data analysis, feature selection, model building, training, and hyperparameter tuning, making it an ideal choice for this scenario.\n\nOption B, Run a BigQuery ML task to perform logistic regression for the classification, is incorrect because while BigQuery ML allows users to train machine learning models on BigQuery data, it requires writing SQL code and does not provide an automated workflow for exploratory data analysis, feature selection, and hyperparameter tuning.\n\nOption C, Use AI Platform Notebooks to run the classification model with pandas library, is incorrect because while AI Platform Notebooks provides a managed notebook environment for data science, it requires writing code using libraries like pandas and scikit-learn, which does not meet the requirement of not writing code.\n\nOption D, Use AI Platform to run the classification model job configured for hyperparameter tuning, is incorrect because while AI Platform provides a managed platform for machine learning, it requires writing code and configuring the job for hyperparameter tuning, which does not meet the requirement of not writing code.\n\nOverall, AutoML Tables is the best choice for this scenario because it provides an automated workflow for building classification models on structured data without requiring users to write code.",
        "references": ""
    },
    {
        "question": "You work for a public transportation company and ne ed to build a model to estimate delay times for mul tiple transportation routes. Predictions are served direc tly to users in an app in real time. Because differ ent seasons and population increases impact the data relevance,  you will retrain the model every month. You want t o follow Google-recommended best practices. How should you c onfigure the end-to-end architecture of the predict ive model?",
        "options": [
            "A. Configure Kubeflow Pipelines to schedule your mul ti-step workflow from training to deploying your mo del.",
            "B. Use a model trained and deployed on BigQuery ML, and trigger retraining with the scheduled query fea ture",
            "C. Write a Cloud Functions script that launches a tr aining and deploying job on AI Platform that is tri ggered by"
        ],
        "correct": "A. Configure Kubeflow Pipelines to schedule your mul ti-step workflow from training to deploying your mo del.",
        "explanation": "Explanation:\nThe correct answer is A. Configure Kubeflow Pipelines to schedule your multi-step workflow from training to deploying your model.\n\nKubeflow Pipelines is a cloud-native, open-source platform that allows you to deploy, manage, and scale machine learning (ML) workflows. It provides a flexible and extensible way to automate the ML workflow, from data preparation to model deployment. In this scenario, Kubeflow Pipelines is the best choice because it allows you to schedule the multi-step workflow from training to deploying the model, which is a critical requirement for this use case.\n\nOption B, using BigQuery ML, is not suitable for this scenario because BigQuery ML is primarily designed for data warehousing and analytics, not for real-time prediction serving. Although BigQuery ML can be used for machine learning, it is not optimized for real-time prediction serving, and it would not be able to handle the high volume of requests from the app.\n\nOption C, using Cloud Functions, is also not suitable because Cloud Functions is a serverless compute service that is designed for short-lived, stateless functions. It is not designed to handle complex, multi-step workflows like the one required in this scenario.\n\nTherefore, the correct answer is A. Configure Kubeflow Pipelines to schedule your multi-step workflow from training to deploying your model.",
        "references": ""
    },
    {
        "question": "You are developing ML models with AI Platform for i mage segmentation on CT scans. You frequently updat e your model architectures based on the newest availa ble research papers, and have to rerun training on the same dataset to benchmark their performance. You wa nt to minimize computation costs and manual intervention while having version control for your code. What should you do?",
        "options": [
            "A. Use Cloud Functions to identify changes to your c ode in Cloud Storage and trigger a retraining job.",
            "B. Use the gcloud command-line tool to submit traini ng jobs on AI Platform when you update your code.",
            "C. Use Cloud Build linked with Cloud Source Reposito ries to trigger retraining when new code is pushed to the",
            "D. Create an automated workflow in Cloud Composer th at runs daily and looks for changes in code in Clou d"
        ],
        "correct": "C. Use Cloud Build linked with Cloud Source Reposito ries to trigger retraining when new code is pushed to the",
        "explanation": "Explanation:\nThe correct answer is D. Sparse categorical cross-entropy. This is because the dataset has a label map with three classes: ['drivers_license', 'passport', 'credit_card']. The sparse categorical cross-entropy loss function is suitable for multi-class classification problems where the labels are not one-hot encoded. In this case, the labels are not one-hot encoded, so sparse categorical cross-entropy is the correct choice.\n\nOption A, categorical hinge, is incorrect because it is a loss function used for multi-class classification problems where the labels are one-hot encoded. Since the labels in this problem are not one-hot encoded, categorical hinge is not the correct choice.\n\nOption B, binary cross-entropy, is incorrect because it is a loss function used for binary classification problems, not multi-class classification problems. Since there are three classes in this problem, binary cross-entropy is not the correct choice.\n\nOption C, categorical cross-entropy, is incorrect because it is a loss function used for multi-class classification problems where the labels are one-hot encoded. Since the labels in this problem are not one-hot encoded, categorical cross-entropy is not the correct choice.\n\nTherefore, the correct answer is D. Sparse categorical cross-entropy.",
        "references": ""
    },
    {
        "question": "redicts whether images contain a driver's license, passport, or credit card. The data engineering team  already built the pipeline and generated a dataset composed  of 10,000 images with driver's Your team needs to build a model that p redit cards. You now have to train a model with the  following label map: [`drivers_license', `passport ', `credit_card']. Which loss function should you use?  licenses, 1,000 images with passports, and 1,000 i mages with c",
        "options": [
            "A. Categorical hinge",
            "B. Binary cross-entropy",
            "C. Categorical cross-entropy",
            "D. Sparse categorical cross-entropy"
        ],
        "correct": "D. Sparse categorical cross-entropy",
        "explanation": "Explanation:\n\nThe correct answer is C. Use Cloud Build linked with Cloud Source Repositories to trigger retraining when new code is pushed to the repository. This is because Cloud Build is a fully managed continuous integration and continuous delivery (CI/CD) service that allows you to automate your build, test, and deployment pipeline. By linking it with Cloud Source Repositories, you can trigger a retraining job on AI Platform whenever new code is pushed to the repository, minimizing manual intervention and computation costs.\n\nOption A is incorrect because Cloud Functions is a serverless compute service that runs small code snippets in response to events, but it's not designed for running complex machine learning training jobs. Additionally, it would require manual intervention to trigger the retraining job.\n\nOption B is incorrect because using the gcloud command-line tool to submit training jobs on AI Platform would require manual intervention and would not provide version control for your code.\n\nOption D is incorrect because creating an automated workflow in Cloud Composer would require setting up a daily schedule to look for changes in code, which would not be triggered by new code pushes and would not provide real-time version control. Additionally, Cloud Composer is a workflow orchestration service that is designed for more complex workflows, and it would be an overkill for this specific use case.\n\nIn summary, using Cloud Build linked with Cloud Source Repositories provides a seamless and automated way to trigger retraining jobs on AI Platform whenever new code is pushed to the repository, minimizing manual intervention and computation costs while providing",
        "references": ""
    },
    {
        "question": "will use Recommendations AI to build, test, and dep loy your system. How should you develop recommendations that increase revenue while followi ng best practices?",
        "options": [
            "A. Use the \"Other Products You May Like\" recommendat ion type to increase the click-through rate.",
            "B. Use the \"Frequently Bought Together\" recommendati on type to increase the shopping cart size for each",
            "C. Import your user events and then your product cat alog to make sure you have the highest quality even t",
            "D. Because it will take time to collect and record p roduct data, use placeholder values for the product  catalog"
        ],
        "correct": "B. Use the \"Frequently Bought Together\" recommendati on type to increase the shopping cart size for each",
        "explanation": "Explanation:\nThe correct answer is B. Use the \"Frequently Bought Together\" recommendation type to increase the shopping cart size for each. This type of recommendation is designed to suggest products that are often purchased together, which can increase the average order value and revenue. By recommending products that are frequently bought together, you can encourage customers to add more items to their shopping cart, increasing the overall revenue.\n\nOption A is incorrect because the \"Other Products You May Like\" recommendation type is more focused on increasing the click-through rate, rather than directly increasing revenue. While it may lead to more sales, it is not as effective in increasing the average order value.\n\nOption C is incorrect because importing user events and then the product catalog is an important step in setting up Recommendations AI, but it is not directly related to increasing revenue. This step is more focused on collecting data to improve the accuracy of the recommendations.\n\nOption D is incorrect because using placeholder values for the product catalog is not a best practice and can lead to inaccurate recommendations. It is important to use real product data to ensure that the recommendations are relevant and effective.\n\nIn summary, the correct answer is B because it is the recommendation type that is most directly focused on increasing revenue by suggesting products that are frequently bought together.",
        "references": ""
    },
    {
        "question": "You are designing an architecture with a serverless  ML system to enrich customer support tickets with informative metadata before they are routed to a su pport agent. You need a set of models to predict ti cket priority, predict ticket resolution time, and perfo rm sentiment analysis to help agents make strategic  decisions when they process support requests. Tickets are not  expected to have any domain-specific terms or jarg on. The proposed architecture has the following flow: Which endpoints should the Enrichment Cloud Functio ns call?",
        "options": [
            "A. 1 = AI Platform, 2 = AI Platform, 3 = AutoML Visi on",
            "B. 1 = AI Platform, 2 = AI Platform, 3 = AutoML Natu ral Language",
            "C. 1 = AI Platform, 2 = AI Platform, 3 = Cloud Natur al Language API",
            "D. 1 = Cloud Natural Language API, 2 = AI Platform, 3 = Cloud Vision API"
        ],
        "correct": "C. 1 = AI Platform, 2 = AI Platform, 3 = Cloud Natur al Language API",
        "explanation": "Explanation:\nThe correct answer is C. 1 = AI Platform, 2 = AI Platform, 3 = Cloud Natural Language API.\n\nThe proposed architecture is a serverless ML system that enriches customer support tickets with informative metadata. There are three tasks to be performed: predicting ticket priority, predicting ticket resolution time, and performing sentiment analysis.\n\nFor predicting ticket priority and predicting ticket resolution time, AI Platform is the best choice. AI Platform is a managed platform that enables data scientists and machine learning engineers to build, deploy, and manage machine learning models.\n\nFor performing sentiment analysis, Cloud Natural Language API is the best choice. Cloud Natural Language API is a text analysis service that can analyze text and extract insights such as sentiment, entities, and syntax.\n\nThe other options are incorrect because:\n\nA. AutoML Vision is a computer vision service that can analyze images and detect objects, which is not relevant to this scenario.\n\nB. AutoML Natural Language is a service that can analyze text and extract insights, but it is not the best choice for sentiment analysis.\n\nD. Cloud Vision API is a computer vision service that can analyze images and detect objects, which is not relevant to this scenario.\n\nTherefore, the correct answer is C. 1 = AI Platform, 2 = AI Platform, 3 = Cloud Natural Language API.",
        "references": ""
    },
    {
        "question": "You have trained a deep neural network model on Goo gle Cloud. The model has low loss on the training d ata, but is performing worse on the validation data. You  want the model to be resilient to overfitting. Whi ch strategy should you use when retraining the model?",
        "options": [
            "A. Apply a dropout parameter of 0.2, and decrease th e learning rate by a factor of 10.",
            "B. Apply a L2 regularization parameter of 0.4, and d ecrease the learning rate by a factor of 10.",
            "C. Run a hyperparameter tuning job on AI Platform to  optimize for the L2 regularization and dropout",
            "D. Run a hyperparameter tuning job on AI Platform to  optimize for the learning rate, and increase the n umber"
        ],
        "correct": "C. Run a hyperparameter tuning job on AI Platform to  optimize for the L2 regularization and dropout",
        "explanation": "Explanation:\nThe correct answer is B. Lack of model retraining. This issue is most likely causing the steady decline in model accuracy because the model has not been retrained on new data since its deployment to production. The model's accuracy has deteriorated over time because it is not adapting to changes in the market. Without retraining, the model becomes outdated and less accurate.\n\nOption A, Poor data quality, is incorrect because poor data quality would have affected the model's accuracy from the beginning, not caused a steady decline over time. The issue is not with the quality of the data, but rather with the model's inability to adapt to changes in the data.\n\nOption C, Too few layers in the model for capturing information, is incorrect because the number of layers in the model would not cause a steady decline in accuracy over time. The issue is not with the model's architecture, but rather with its inability to adapt to changes in the data.\n\nOption D, Incorrect data split ratio during model training, evaluation, validation, and test, is incorrect because an incorrect data split ratio would have affected the model's accuracy from the beginning, not caused a steady decline over time. The issue is not with how the data was split, but rather with the model's inability to adapt to changes in the data.",
        "references": ""
    },
    {
        "question": "You built and manage a production system that is re sponsible for predicting sales numbers. Model accur acy is crucial, because the production model is required t o keep up with market changes. Since being deployed  to production, the model hasn't changed; however the a ccuracy of the model has steadily deteriorated. Wha t issue is most likely causing the steady decline in model accuracy?",
        "options": [
            "A. Poor data quality",
            "B. Lack of model retraining",
            "C. Too few layers in the model for capturing informa tion",
            "D. Incorrect data split ratio during model training,  evaluation, validation, and test"
        ],
        "correct": "B. Lack of model retraining",
        "explanation": "Explanation:\n\nThe correct answer is C. Run a hyperparameter tuning job on AI Platform to optimize for the L2 regularization and dropout. \n\nThis is because the problem described is a classic case of overfitting, where the model performs well on the training data but poorly on the validation data. Overfitting occurs when a model is too complex and learns the noise in the training data, rather than the underlying patterns.\n\nTo combat overfitting, regularization techniques such as L2 regularization and dropout can be used. L2 regularization adds a penalty term to the loss function to discourage large weights, while dropout randomly sets a fraction of the neurons to zero during training, effectively creating an ensemble of different sub-networks.\n\nHowever, the optimal values for these regularization techniques (i.e., the L2 regularization parameter and dropout rate) are often unknown and require tuning. This is where hyperparameter tuning comes in. By running a hyperparameter tuning job on AI Platform, you can automatically search for the optimal values of these regularization techniques to prevent overfitting.\n\nOption A is incorrect because applying a dropout parameter of 0.2 and decreasing the learning rate by a factor of 10 may help with overfitting, but it does not guarantee the optimal values for these hyperparameters.\n\nOption B is also incorrect for the same reason as Option A. Applying an L2 regularization parameter of 0.4 and decreasing the learning rate by a factor of 10 may help with overfitting, but it does",
        "references": ""
    },
    {
        "question": "You have been asked to develop an input pipeline fo r an ML training model that processes images from disparate sources at a low latency. You discover th at your input data does not fit in memory. How shou ld you create a dataset following Google-recommended best practices?",
        "options": [
            "A. Create a tf.data.Dataset.prefetch transformation.",
            "B. Convert the images to tf.Tensor objects, and then  run Dataset.from_tensor_slices().",
            "C. Convert the images to tf.Tensor objects, and then  run tf.data.Dataset.from_tensors().",
            "D. Convert the images into TFRecords, store the imag es in Cloud Storage, and then use the tf.data API t o"
        ],
        "correct": "D. Convert the images into TFRecords, store the imag es in Cloud Storage, and then use the tf.data API t o",
        "explanation": "Explanation:\n\nThe correct answer is D. Convert the images into TFRecords, store the images in Cloud Storage, and then use the tf.data API to create a dataset. \n\nThis is because TFRecords are a recommended format for storing large datasets in Google Cloud, especially when dealing with images. By converting the images into TFRecords, you can efficiently store and read the data in a format that is optimized for TensorFlow. \n\nStoring the TFRecords in Cloud Storage allows you to decouple the data storage from the processing pipeline, which enables scalability and flexibility. \n\nFinally, using the tf.data API to create a dataset from the TFRecords enables you to create a pipeline that can efficiently process the data in parallel, which is essential for low-latency processing of large datasets.\n\nThe other options are incorrect because:\n\nA. Creating a tf.data.Dataset.prefetch transformation is a good practice for optimizing the performance of the input pipeline, but it does not address the issue of storing and reading large datasets.\n\nB. Converting the images to tf.Tensor objects and then running Dataset.from_tensor_slices() is not a recommended approach because it requires loading the entire dataset into memory, which is not feasible when the dataset does not fit in memory.\n\nC. Converting the images to tf.Tensor objects and then running tf.data.Dataset.from_tensors() is also not a recommended approach because it creates a dataset from a single tensor, which is not suitable for processing large datasets.\n\nTherefore, option D is the correct answer because it follows Google",
        "references": ""
    },
    {
        "question": "y prediction model. Your model's features include r egion, location, historical demand, and seasonal po pularity. You You are an ML engineer at a large grocery retailer with stores in multiple regions. You have been aske d to create an inventor want the algorithm to learn from  new inventory data on a daily basis. Which algorit hms should you use to build the model?",
        "options": [
            "A. Classification",
            "B. Reinforcement Learning",
            "C. Recurrent Neural Networks (RNN)",
            "D. Convolutional Neural Networks (CNN)"
        ],
        "correct": "C. Recurrent Neural Networks (RNN)",
        "explanation": "Explanation:\nThe correct answer is C. Recurrent Neural Networks (RNN). This is because the problem involves time-series data, where the model needs to learn from new inventory data on a daily basis. RNNs are well-suited for this type of problem, as they can handle sequential data and learn patterns over time.\n\nOption A, Classification, is incorrect because classification algorithms are typically used for categorical data, whereas this problem involves continuous data (inventory levels).\n\nOption B, Reinforcement Learning, is also incorrect because reinforcement learning is typically used for decision-making problems, where an agent learns to take actions in an environment to maximize a reward. This problem does not involve decision-making, but rather predicting inventory levels.\n\nOption D, Convolutional Neural Networks (CNN), is incorrect because CNNs are typically used for image and signal processing tasks, whereas this problem involves time-series data.\n\nIn summary, RNNs are the best choice for this problem because they can handle sequential data and learn patterns over time, making them well-suited for time-series forecasting tasks.",
        "references": ""
    },
    {
        "question": "You are building a real-time prediction engine that  streams files which may contain Personally Identif iable Information (PII) to Google Cloud. You want to use the Cloud Data Loss Prevention (DLP) API to scan th e files. How should you ensure that the PII is not accessibl e by unauthorized individuals?",
        "options": [
            "A. Stream all files to Google Cloud, and then write the data to BigQuery. Periodically conduct a bulk s can of",
            "B. Stream all files to Google Cloud, and write batch es of the data to BigQuery. While the data is being  written",
            "C. Create two buckets of data: Sensitive and Non-sen sitive. Write all data to the Non-sensitive bucket.",
            "D. Create three buckets of data: Quarantine, Sensiti ve, and Non-sensitive. Write all data to the Quaran tine"
        ],
        "correct": "A. Stream all files to Google Cloud, and then write the data to BigQuery. Periodically conduct a bulk s can of",
        "explanation": "Explanation:\n\nThe correct answer is D. Submit the data for training without performing any manual transformations. Use the columns that have a time signal as separate features.\n\nThe reason for this is that AutoML Tables is designed to automatically handle and interpret time signals in the data. By submitting the data without manual transformations, AutoML Tables can identify the time signals in the data and use them to improve the accuracy of the predictions.\n\nOption A is incorrect because manually combining all columns that contain a time signal into an array can lead to loss of information and may not allow AutoML Tables to fully utilize the time signals in the data.\n\nOption B is incorrect because submitting the data for training without performing any manual transformations and allowing AutoML Tables to handle the time signals may not be sufficient, as AutoML Tables may not be able to identify the time signals correctly.\n\nOption C is incorrect because indicating an appropriate timestamp column is not necessary, as AutoML Tables can automatically identify the time signals in the data.\n\nTherefore, the correct approach is to submit the data for training without performing any manual transformations and use the columns that have a time signal as separate features, allowing AutoML Tables to automatically handle and interpret the time signals in the data.",
        "references": ""
    },
    {
        "question": "You work for a large hotel chain and have been aske d to assist the marketing team in gathering predict ions for a targeted marketing strategy. You need to make pre dictions about user lifetime value (LTV) over the n ext 20 days so that marketing can be adjusted accordingly.  The customer dataset is in BigQuery, and you are preparing the tabular data for training with AutoML  Tables. This data has a time signal that is spread  across multiple columns. How should you ensure that AutoML fits the best mod el to your data?",
        "options": [
            "A. Manually combine all columns that contain a time signal into an array. AIlow AutoML to interpret thi s array",
            "B. Submit the data for training without performing a ny manual transformations. AIlow AutoML to handle t he",
            "C. Submit the data for training without performing a ny manual transformations, and indicate an appropri ate",
            "D. Submit the data for training without performing a ny manual transformations. Use the columns that hav e a"
        ],
        "correct": "D. Submit the data for training without performing a ny manual transformations. Use the columns that hav e a",
        "explanation": "Explanation: \n\nThe correct answer is B. Using Cloud Build, set an automated trigger to execute the unit tests when changes are pushed to your development branch. \n\nThis is because Cloud Build is a service that allows you to automate your build, test, and deployment pipeline. It can be triggered automatically when changes are pushed to your source repository, such as Cloud Source Repositories. \n\nIn this scenario, you want to automate the execution of unit tests with each new push to your development branch. This can be achieved by setting up an automated trigger in Cloud Build that executes the unit tests when changes are pushed to your development branch. \n\nOption A is incorrect because it suggests writing a script that sequentially performs the push to your development branch and executes the unit tests. This approach is not automated and would require manual intervention to execute the script. \n\nOption C and D are also incorrect because they suggest setting up a Cloud Logging sink to a Pub/Sub topic that captures interactions with Cloud Source Repositories. While this would allow you to capture and analyze logs related to your source repository, it would not automate the execution of unit tests.",
        "references": ""
    },
    {
        "question": "You have written unit tests for a Kubeflow Pipeline  that require custom libraries. You want to automat e the execution of unit tests with each new push to your development branch in Cloud Source Repositories. Wh at should you do?",
        "options": [
            "A. Write a script that sequentially performs the pus h to your development branch and executes the unit tests",
            "B. Using Cloud Build, set an automated trigger to ex ecute the unit tests when changes are pushed to you r",
            "C. Set up a Cloud Logging sink to a Pub/Sub topic th at captures interactions with Cloud Source Reposito ries.",
            "D. Set up a Cloud Logging sink to a Pub/Sub topic th at captures interactions with Cloud Source Reposito ries."
        ],
        "correct": "B. Using Cloud Build, set an automated trigger to ex ecute the unit tests when changes are pushed to you r",
        "explanation": "Explanation: The correct answer is B. Modify the `scale-tier' parameter. The scale-tier parameter determines the number of machines used for training. Increasing the scale-tier will increase the number of machines used, which can significantly reduce the training time. This is because the model will be trained in parallel across multiple machines, reducing the overall training time. \n\nThe other options are incorrect because: \n\nOption A is incorrect because modifying the `epochs' parameter will not significantly reduce the training time. Increasing the number of epochs may even increase the training time. \n\nOption C is incorrect because modifying the `batch size' parameter may not significantly reduce the training time. Increasing the batch size may even increase the training time due to increased memory usage. \n\nOption D is incorrect because modifying the `learning rate' parameter will not significantly reduce the training time. The learning rate determines how quickly the model learns from the data, but it does not affect the training time directly.",
        "references": ""
    },
    {
        "question": "You are training an LSTM-based model on AI Platform  to summarize text using the following job submissi on script: gcloud ai-platform jobs submit training $JOB_NAME \\ --package-path $TRAINER_PACKAGE_PATH \\ --module-name $MAIN_TRAINER_MODULE \\ --job-dir $JOB_DIR \\ --region $REGION \\ --scale-tier basic \\ -- \\ --epochs 20 \\ --batch_size=32 \\ --learning_rate=0.001 \\ You want to ensure that training time is minimized without significantly compromising the accuracy of your model. What should you do?",
        "options": [
            "A. Modify the `epochs' parameter.",
            "B. Modify the `scale-tier' parameter.",
            "C. Modify the `batch size' parameter.",
            "D. Modify the `learning rate' parameter."
        ],
        "correct": "B. Modify the `scale-tier' parameter.",
        "explanation": "Explanation: \n\nThe correct answer is A. Stream all files to Google Cloud, and then write the data to BigQuery. Periodically conduct a bulk scan of the data using the Cloud DLP API. This approach ensures that the PII is not accessible by unauthorized individuals because the data is not being written to a bucket that can be accessed by unauthorized individuals. Instead, the data is being written to BigQuery, which has robust security features, including access controls and encryption. Additionally, the periodic bulk scan of the data using the Cloud DLP API ensures that any PII is detected and can be handled appropriately.\n\nOption B is incorrect because writing batches of data to BigQuery while it's being written does not ensure that the PII is not accessible by unauthorized individuals. This approach may lead to PII being written to a bucket that can be accessed by unauthorized individuals.\n\nOption C is incorrect because creating two buckets of data, Sensitive and Non-sensitive, and writing all data to the Non-sensitive bucket does not ensure that the PII is not accessible by unauthorized individuals. This approach may lead to PII being written to the Non-sensitive bucket, which can be accessed by unauthorized individuals.\n\nOption D is incorrect because creating three buckets of data, Quarantine, Sensitive, and Non-sensitive, and writing all data to the Quarantine bucket does not ensure that the PII is not accessible by unauthorized individuals. This approach may lead to PII being written to the Quarantine bucket, which can be accessed",
        "references": ""
    },
    {
        "question": "You have deployed multiple versions of an image cla ssification model on AI Platform. You want to monit or the performance of the model versions over time. How sh ould you perform this comparison?",
        "options": [
            "A. Compare the loss performance for each model on a held-out dataset.",
            "B. Compare the loss performance for each model on th e validation data.",
            "C. Compare the receiver operating characteristic (RO C) curve for each model using the What-If Tool.",
            "D. Compare the mean average precision across the mod els using the Continuous Evaluation feature."
        ],
        "correct": "D. Compare the mean average precision across the mod els using the Continuous Evaluation feature.",
        "explanation": "Explanation: The correct answer is D. The prediction request should be formatted as a list of lists, where each inner list represents an instance to be classified. In this case, the request is sending three instances to be classified, each with two features. \n\nOption A is incorrect because it sends three instances with two features each, but the inner list should have the same length as the number of features in the model. \n\nOption B is incorrect because it sends one instance with six features, whereas the model expects instances with two features. \n\nOption C is incorrect because it sends two instances, one with three features and another with three features, whereas the model expects instances with two features.\n\nI hope it is correct.",
        "references": ""
    },
    {
        "question": "You trained a text classification model. You have t he following SignatureDefs: You started a TensorFlow-serving component server a nd tried to send an HTTP request to get a predictio n using: headers = {\"content-type\": \"application/json\"} json_response = requests.post('http://localhost:850 1/v1/models/text_model:predict', data=data, headers=headers) What is the correct way to write the predict reques t? A. data = json.dumps({\"signature_name\": \"seving_defa ult\", \"instances\" [[`ab', `bc', `cd']]})",
        "options": [
            "B. data = json.dumps({\"signature_name\": \"serving_def ault\", \"instances\" [[`a', `b', `c', `d', `e', `f']] })",
            "C. data = json.dumps({\"signature_name\": \"serving_def ault\", \"instances\" [[`a', `b', `c'], [`d', `e', `f' ]]})",
            "D. data = json.dumps({\"signature_name\": \"serving_def ault\", \"instances\" [[`a', `b'], [`c', `d'], [`e', ` f']]})"
        ],
        "correct": "D. data = json.dumps({\"signature_name\": \"serving_def ault\", \"instances\" [[`a', `b'], [`c', `d'], [`e', ` f']]})",
        "explanation": "Explanation:\n\nThe correct answer is D. Compare the mean average precision across the models using the Continuous Evaluation feature. \n\nContinuous Evaluation is a feature in AI Platform that allows you to continuously evaluate the performance of your machine learning models on new data as it becomes available. This feature is particularly useful when you have multiple versions of a model and want to compare their performance over time. \n\nIn this scenario, you have deployed multiple versions of an image classification model on AI Platform, and you want to monitor the performance of the model versions over time. The best way to do this is by using Continuous Evaluation to compare the mean average precision across the models. Mean average precision is a common metric used to evaluate the performance of object detection and image classification models. \n\nOption A is incorrect because comparing the loss performance for each model on a held-out dataset only gives you an idea of how well each model is performing on a specific dataset, but it doesn't provide a comprehensive comparison of the models' performance over time. \n\nOption B is also incorrect because comparing the loss performance for each model on the validation data only gives you an idea of how well each model is performing on the validation data, but it doesn't provide a comprehensive comparison of the models' performance over time. \n\nOption C is incorrect because the What-If Tool is used to visualize and analyze the performance of machine learning models, but it's not specifically designed for comparing the performance of multiple models over time.",
        "references": ""
    },
    {
        "question": "Your organization's call center has asked you to de velop a model that analyzes customer sentiments in each call. The call center receives over one million cal ls daily, and data is stored in Cloud Storage. The data collected must not leave the region in which the ca ll originated, and no Personally Identifiable Infor mation (PII) can be stored or analyzed. The data science team ha s a third-party tool for visualization and access w hich requires a SQL ANSI-2011 compliant interface. You n eed to select components for data processing and fo r analytics. How should the data pipeline be designed ?",
        "options": [
            "A. 1= Dataflow, 2= BigQuery",
            "B. 1 = Pub/Sub, 2= Datastore",
            "C. 1 = Dataflow, 2 = Cloud SQL",
            "D. 1 = Cloud Function, 2= Cloud SQL"
        ],
        "correct": "A. 1= Dataflow, 2= BigQuery",
        "explanation": "Explanation:\n\nThe correct answer is D. Decrease the number of false negatives. \n\nWhen you want to increase precision, you need to decrease the number of false positives. However, decreasing the number of false positives often comes at the cost of increasing the number of false negatives. \n\nIn object detection tasks, like detecting cars in images, the goal is to correctly identify the objects of interest (cars) while minimizing false detections. \n\nTo increase precision, you want to reduce the number of false positives (i.e., images that are incorrectly classified as containing cars). \n\nTo achieve this, you should adjust the model's final layer softmax threshold to decrease the number of false negatives (i.e., images that are incorrectly classified as not containing cars). \n\nThis is because a higher softmax threshold will make the model more conservative in its predictions, resulting in fewer false positives but potentially more false negatives. \n\nThe other options are incorrect because:\n\nA. Increasing the recall will actually decrease precision, as it will result in more false positives.\n\nB. Decreasing the recall will also decrease precision, as it will result in more false negatives.\n\nC. Increasing the number of false positives will decrease precision, which is the opposite of what you want to achieve.",
        "references": ""
    },
    {
        "question": "You are an ML engineer at a global shoe store. You manage the ML models for the company's website. You are asked to build a model that will recommend new products to the user based on their purchase behavi or and similarity with other users. What should you do? A. Build a classification model",
        "options": [
            "B. Build a knowledge-based filtering model",
            "C. Build a collaborative-based filtering model",
            "D. Build a regression model using the features as pr edictors"
        ],
        "correct": "C. Build a collaborative-based filtering model",
        "explanation": "Explanation:\nThe correct answer is A. 1= Dataflow, 2= BigQuery. \n\nHere's why:\n1. The pipeline starts with Dataflow, which is a fully-managed service for transforming and processing data in the cloud. Dataflow is a perfect fit for this use case because it can handle large volumes of data, can be used to analyze customer sentiments, and can ensure that data does not leave the region where it originated. \n2. The output from Dataflow is then sent to BigQuery, which is a fully-managed enterprise data warehouse that supports SQL ANSI-2011 compliant queries. BigQuery is a great choice because it allows for fast and scalable analysis of large datasets and has built-in support for data visualization tools. Also, BigQuery does not store PII, which meets the requirement.\n\nNow, let's discuss why the other options are incorrect:\n\nOption B is incorrect because Pub/Sub is a messaging service that is used for event-driven architecture and real-time data processing, but it's not suitable for data processing and analytics in this scenario. Datastore is a NoSQL database that is not designed for analytics and does not support SQL ANSI-2011 compliant queries.\n\nOption C is incorrect because while Dataflow is a great choice for data processing, Cloud SQL is a relational database service that is not designed for analytics and does not support SQL ANSI-2011 compliant queries. \n\nOption D is incorrect because Cloud Function is a serverless compute service that is used",
        "references": ""
    },
    {
        "question": "You work for a social media company. You need to de tect whether posted images contain cars. Each train ing example is a member of exactly one class. You have trained an object detection neural network and depl oyed the model version to AI Platform Prediction for eva luation. Before deployment, you created an evaluati on job and attached it to the AI Platform Prediction model  version. You notice that the precision is lower th an your business requirements allow. How should you adjust the model's final layer softmax threshold to increa se precision?",
        "options": [
            "A. Increase the recall.",
            "B. Decrease the recall.",
            "C. Increase the number of false positives.",
            "D. Decrease the number of false negatives."
        ],
        "correct": "D. Decrease the number of false negatives.",
        "explanation": "    A. Use the interleave option for reading data.\n    D. Set the prefetch option equal to the training batch size.",
        "references": ""
    },
    {
        "question": "You are responsible for building a unified analytic s environment across a variety of on-premises data marts. Your company is experiencing data quality and secur ity challenges when integrating data across the ser vers, caused by the use of a wide range of disconnected t ools and temporary solutions. You need a fully mana ged, cloud-native data integration service that will low er the total cost of work and reduce repetitive wor k. Some members on your team prefer a codeless interface fo r building Extract, Transform, Load (ETL) process. Which service should you use?",
        "options": [
            "A. Dataflow",
            "B. Dataprep",
            "C. Apache Flink",
            "D. Cloud Data Fusion"
        ],
        "correct": "D. Cloud Data Fusion",
        "explanation": "Explanation:\n\nThe correct answer is C. Build a collaborative-based filtering model. \n\nCollaborative filtering is a type of recommendation system that uses the behavior or preferences of similar users to make recommendations. In this case,, the goal is to recommend new products to users based on their purchase behavior and similarity with other users. Collaborative filtering is well-suited for this task because it takes into account the interactions between users and items, and can identify patterns and relationships between them.\n\nOption A, building a classification model, is incorrect because classification models are typically used for predicting categorical outcomes, such as whether a user will purchase a product or not. In this case, the goal is to recommend specific products, which requires a different type of model.\n\nOption B, building a knowledge-based filtering model, is also incorrect. Knowledge-based filtering models use explicit knowledge about the items being recommended, such as their features or attributes, to make recommendations. While this approach can be useful in certain contexts, it is not well-suited for this task because it does not take into account the behavior or preferences of similar users.\n\nOption D, building a regression model using the features as predictors, is incorrect because regression models are typically used for predicting continuous outcomes, such as the rating a user will give a product. In this case, the goal is to recommend specific products, which requires a different type of model.\n\nIn summary, collaborative filtering is the most suitable approach for building a model that recommends new products to users based on their",
        "references": ""
    },
    {
        "question": "You are an ML engineer at a regulated insurance com pany. You are asked to develop an insurance approva l model that accepts or rejects insurance application s from potential customers. What factors should you consider before building the model? A. Redaction, reproducibility, and explainability",
        "options": [
            "B. Traceability, reproducibility, and explainability",
            "C. Federated learning, reproducibility, and explaina bility",
            "D. Differential privacy, federated learning, and exp lainability"
        ],
        "correct": "B. Traceability, reproducibility, and explainability",
        "explanation": "Explanation:\nThe correct answer is D. Cloud Data Fusion. Cloud Data Fusion is a fully managed, cloud-native data integration service that provides a codeless interface for building ETL processes. It integrates data from multiple sources, transforms the data, and loads it into a target system. It is designed to lower the total cost of work and reduce repetitive work by providing a unified analytics environment.\n\nOption A, Dataflow, is a fully managed service for processing and analyzing data, but it does not provide a codeless interface for building ETL processes.\n\nOption B, Dataprep, is a service that prepares data for analysis, but it is not a data integration service and does not provide a codeless interface for building ETL processes.\n\nOption C, Apache Flink, is an open-source platform for distributed stream and batch processing, but it is not a cloud-native data integration service and does not provide a codeless interface for building ETL processes.\n\nTherefore, the correct answer is D. Cloud Data Fusion, which provides a fully managed, cloud-native data integration service with a codeless interface for building ETL processes.",
        "references": ""
    },
    {
        "question": "You are training a Resnet model on AI Platform usin g TPUs to visually categorize types of defects in automobile engines. You capture the training profil e using the Cloud TPU profiler plugin and observe t hat it is highly input-bound. You want to reduce the bottlene ck and speed up your model training process. Which modifications should you make to the tf.data datase t? (Choose two.)",
        "options": [
            "A. Use the interleave option for reading data.",
            "B. Reduce the value of the repeat parameter.",
            "C. Increase the buffer size for the shuttle option.",
            "D. Set the prefetch option equal to the training bat ch size."
        ],
        "correct": "",
        "explanation": "Explanation:\n\nThe correct answer is B. Traceability, reproducibility, and explainability. \n\nWhen building an insurance approval model, it is crucial to consider the following factors:\n\n1. **Traceability**: This refers to the ability to track the origin, processing, and usage of data throughout the model development process. In a regulated industry like insurance, it is essential to maintain a clear audit trail of how data was used to train the model.\n\n2. **Reproducibility**: This ensures that the model can be rebuilt and re-run with the same data to produce the same results. Reproducibility is critical in regulated industries, as it allows for model validation and verification.\n\n3. **Explainability**: This involves understanding how the model makes predictions and arrives at its decisions. In the context of insurance approval, explainability is vital to ensure that the model is fair, unbiased, and transparent in its decision-making process.\n\nThe other options are incorrect because:\n\nA. Redaction is not a relevant factor in building an insurance approval model. Redaction refers to the process of editing or removing sensitive information from a document, which is not directly related to model development.\n\nC. Federated learning is a distributed machine learning approach that involves training models on decentralized data. While it can be useful in certain scenarios, it is not a critical factor to consider when building an insurance approval model.\n\nD. Differential privacy is a technique used to protect sensitive information in datasets. While it is an important consideration",
        "references": ""
    },
    {
        "question": "You have trained a model on a dataset that required  computationally expensive preprocessing operations . You need to execute the same preprocessing at predictio n time. You deployed the model on AI Platform for h igh- throughput online prediction. Which architecture sh ould you use?",
        "options": [
            "A. Validate the accuracy of the model that you trained  on preprocessed data.",
            "B. Send incoming prediction requests to a Pub/Sub to pic.",
            "C. Stream incoming prediction request data into Clou d Spanner.",
            "D. Send incoming prediction requests to a Pub/Sub to pic."
        ],
        "correct": "",
        "explanation": "D. Send incoming prediction requests to a Pub/Sub topic.\n\nExplanation: \nThe correct answer is D. In this scenario, you need to execute computationally expensive preprocessing operations at prediction time. To achieve high-throughput online prediction, you should use a message queuing system like Pub/Sub to handle the incoming prediction requests. This allows you to decouple the preprocessing operations from the prediction requests, enabling you to process the requests in parallel and handle high volumes of traffic.\n\nOption A is incorrect because it does not address the requirement of executing preprocessing operations at prediction time.\n\nOption B is incorrect because sending incoming prediction requests to a Pub/Sub topic is only half of the solution. You also need to process the requests in parallel using a worker service like Cloud Run or Cloud Functions.\n\nOption C is incorrect because Cloud Spanner is a relational database service, not a message queuing system. It's not designed to handle high-throughput online prediction requests or computationally expensive preprocessing operations.",
        "references": ""
    },
    {
        "question": "Your team trained and tested a DNN regression model  with good results. Six months after deployment, th e model is performing poorly due to a change in the d istribution of the input data. How should you addre ss the input differences in production?",
        "options": [
            "A. Create alerts to monitor for skew, and retrain th e model.",
            "B. Perform feature selection on the model, and retra in the model with fewer features.",
            "C. Retrain the model, and select an L2 regularizatio n parameter with a hyperparameter tuning service.",
            "D. Perform feature selection on the model, and retra in the model on a monthly basis with fewer features ."
        ],
        "correct": "A. Create alerts to monitor for skew, and retrain th e model.",
        "explanation": "Explanation:\nThe correct answer is B. Reduce the batch size. The error ResourceExhaustedError: Out Of Memory (OOM) when allocating tensor, indicates that the GPU ran out of memory during the training process. This error usually occurs when the batch size is too large, causing the GPU to run out of memory. Reducing the batch size will reduce the memory required for training, thus resolving the OOM error.\n\nOption A, changing the optimizer, is incorrect because the optimizer does not affect the memory usage of the GPU.\n\nOption C, changing the learning rate, is also incorrect because the learning rate does not affect the memory usage of the GPU.\n\nOption D, reducing the image shape, may help reduce the memory usage, but it may also affect the accuracy of the model. Reducing the batch size is a more straightforward solution to resolve the OOM error.\n\nTherefore, the correct answer is B. Reduce the batch size.",
        "references": ""
    },
    {
        "question": "You need to train a computer vision model that pred icts the type of government ID present in a given i mage using a GPU-powered virtual machine on Compute Engi ne. You use the following parameters: Optimizer: SGD Batch size = 64 Epochs = 10 Verbose =2 During training you encounter the following error: ResourceExhaustedError: Out Of Memory (OOM) when allocating tensor. What should you do?",
        "options": [
            "A. Change the optimizer.",
            "B. Reduce the batch size.",
            "C. Change the learning rate.",
            "D. Reduce the image shape."
        ],
        "correct": "B. Reduce the batch size.",
        "explanation": "Explanation: The correct answer is A. Create alerts to monitor for skew, and retrain the model. This is because the model is performing poorly due to a change in the distribution of the input data. This is known as concept drift. To address this, we need to monitor for skew in the input data and retrain the model with the new data. This will ensure that the model is updated to reflect the changes in the input data distribution.\n\nOption B is incorrect because feature selection may not address the issue of concept drift. Feature selection is used to select the most relevant features for the model, but it does not account for changes in the distribution of the input data.\n\nOption C is incorrect because L2 regularization is used to prevent overfitting, but it does not address the issue of concept drift. Hyperparameter tuning can help improve the model's performance, but it does not address the underlying issue of changes in the input data distribution.\n\nOption D is incorrect because performing feature selection and retraining the model on a monthly basis may not address the issue of concept drift. The model may still perform poorly if the input data distribution continues to change. Additionally, retraining the model on a monthly basis may not be necessary or efficient.",
        "references": ""
    },
    {
        "question": "You developed an ML model with AI Platform, and you  want to move it to production. You serve a few tho usand queries per second and are experiencing latency iss ues. Incoming requests are served by a load balance r that distributes them across multiple Kubeflow CPU-only pods running on Google Kubernetes Engine (GKE). You r goal is to improve the serving latency without chan ging the underlying infrastructure. What should you  do?",
        "options": [
            "A. Significantly increase the max_batch_size TensorF low Serving parameter.",
            "B. Switch to the tensorflow-model-server-universal v ersion of TensorFlow Serving.",
            "C. Significantly increase the max_enqueued_batches Ten sorFlow Serving parameter. D. Recompile TensorFlow Serving using the source to support CPU-specific optimizations. Instruct GKE to"
        ],
        "correct": "",
        "explanation": "C. Significantly increase the max_enqueued_batches TensorF low Serving parameter.\n\nExplanation:\n\nThe correct answer is option C: Significantly increase the max_enqueued_batches TensorFlow Serving parameter. \n\nThe question states that you are experiencing latency issues, and the goal is to improve the serving latency without changing the underlying infrastructure. This implies that you are looking for a solution that can be achieved through configuration changes rather than infrastructure upgrades.\n\nOption C, increasing the max_enqueued_batches parameter, is the correct answer because it allows TensorFlow Serving to queue up more requests before processing them. This can help reduce latency by allowing the model to process batches of requests more efficiently. \n\nOption A, increasing the max_batch_size parameter, is incorrect because it would increase the size of each batch, which could lead to increased latency if the model takes longer to process larger batches.\n\nOption B, switching to the tensorflow-model-server-universal version of TensorFlow Serving, is incorrect because it is not related to the latency issue. This version is used for serving models on different platforms, but it does not address the latency issue.\n\nOption D, recompiling TensorFlow Serving using the source to support CPU-specific optimizations, is incorrect because it requires infrastructure changes (recompiling the TensorFlow Serving) and does not address the latency issue directly.\n\nTherefore, the correct answer is option C, significantly increasing the max_enqueued_batches TensorFlow Serving parameter.",
        "references": ""
    },
    {
        "question": "You have a demand forecasting pipeline in productio n that uses Dataflow to preprocess raw data prior t o model training and prediction. During preprocessing, you employ Z-score normalization on data stored in BigQ uery and write it back to BigQuery. New training data is  added every week. You want to make the process mor e efficient by minimizing computation time and manual  intervention. What should you do?",
        "options": [
            "A. Normalize the data using Google Kubernetes Engine .",
            "B. Translate the normalization algorithm into SQL fo r use with BigQuery.",
            "C. Use the normalizer_fn argument in TensorFlow's Fe ature Column API.",
            "D. Normalize the data with Apache Spark using the Da taproc connector for BigQuery."
        ],
        "correct": "B. Translate the normalization algorithm into SQL fo r use with BigQuery.",
        "explanation": "Explanation:\n\nThe correct answer is B because translating the normalization algorithm into SQL for use with BigQuery is the most efficient way to minimize computation time and manual intervention. By using SQL, you can leverage BigQuery's distributed processing capabilities and perform the normalization directly within the database, eliminating the need for Dataflow processing. This approach also reduces the amount of data being transferred between systems, resulting in faster processing times.\n\nOption A is incorrect because using Google Kubernetes Engine (GKE) would require setting up and managing a Kubernetes cluster, which would add complexity and overhead to the pipeline. Additionally, GKE is not designed for data processing tasks like normalization.\n\nOption C is incorrect because the normalizer_fn argument in TensorFlow's Feature Column API is used for feature engineering in machine learning models, not for data preprocessing tasks like normalization.\n\nOption D is incorrect because using Apache Spark with the Dataproc connector for BigQuery would require setting up and managing a Spark cluster, which would add complexity and overhead to the pipeline. Additionally, Spark is not necessary for this task, as BigQuery's SQL capabilities can handle the normalization efficiently.\n\nTherefore, translating the normalization algorithm into SQL for use with BigQuery is the most efficient and effective solution to minimize computation time and manual intervention.",
        "references": ""
    },
    {
        "question": "You need to design a customized deep neural network  in Keras that will predict customer purchases base d on their purchase history. You want to explore model p erformance using multiple model architectures, stor e training data, and be able to compare the evaluatio n metrics in the same dashboard. What should you do ?",
        "options": [
            "A. Create multiple models using AutoML Tables.",
            "B. Automate multiple training runs using Cloud Compo ser.",
            "C. Run multiple training jobs on AI Platform with si milar job names.",
            "D. Create an experiment in Kubeflow Pipelines to org anize multiple runs."
        ],
        "correct": "D. Create an experiment in Kubeflow Pipelines to org anize multiple runs.",
        "explanation": "C. Use the Kubeflow Pipelines domain-specific language to create a custom component that uses the Python BigQuery client library to execute the query.\n\nExplanation: \nThe correct answer is C because Kubeflow Pipelines provides a domain-specific language (DSL) for defining pipelines. This DSL allows you to create custom components that can interact with external services like BigQuery. By using the DSL to create a custom component that uses the Python BigQuery client library, you can execute the query against BigQuery and get the results as input to the next step in the pipeline in a straightforward and efficient way.\n\nOption A is incorrect because it involves manual intervention and is not automated. You would have to manually execute the query in the BigQuery console, save the results, and then use those results as input to the next step in the pipeline. This approach is not scalable and is prone to errors.\n\nOption B is also incorrect because it involves writing a custom Python script that executes the query against BigQuery. While this approach is more automated than Option A, it still requires you to write and maintain a custom script, which can be error-prone and difficult to manage. Additionally, this approach does not integrate well with the Kubeflow Pipelines workflow.",
        "references": ""
    },
    {
        "question": "You are developing a Kubeflow pipeline on Google Ku bernetes Engine. The first step in the pipeline is to issue a query against BigQuery. You plan to use the resul ts of that query as the input to the next step in y our pipeline. You want to achieve this in the easiest way possibl e. What should you do?",
        "options": [
            "A. Use the BigQuery console to execute your query, a nd then save the query results into a new BigQuery",
            "B. Write a Python script that uses the BigQuery API to execute queries against BigQuery. Execute this s cript",
            "C. Use the Kubeflow Pipelines domain-specific langua ge to create a custom component that uses the Pytho n"
        ],
        "correct": "",
        "explanation": "Explanation:\nThe correct answer is D because Kubeflow Pipelines is a platform that allows you to create, manage, and track machine learning experiments. It provides a way to organize multiple runs of a model, compare evaluation metrics, and store training data. Kubeflow Pipelines is specifically designed for machine learning workflows and provides features such as experiment tracking, hyperparameter tuning, and model serving.\n\nOption A is incorrect because AutoML Tables is a managed service that automates the process of building, deploying, and managing machine learning models, but it does not provide a way to organize multiple runs of a model or compare evaluation metrics.\n\nOption B is incorrect because Cloud Composer is a fully managed workflow orchestration service that helps you create, schedule, and monitor workflows, but it is not specifically designed for machine learning workflows and does not provide features such as experiment tracking or hyperparameter tuning.\n\nOption C is incorrect because running multiple training jobs on AI Platform with similar job names does not provide a way to organize multiple runs of a model or compare evaluation metrics in a single dashboard. AI Platform is a managed service that allows you to train, deploy, and manage machine learning models, but it does not provide a built-in way to track experiments or compare evaluation metrics.\n\nTherefore, the correct answer is D because Kubeflow Pipelines provides a way to organize multiple runs of a model, compare evaluation metrics, and store training data, making it the best option for this scenario.",
        "references": ""
    },
    {
        "question": "You are building a model to predict daily temperatu res. You split the data randomly and then transform ed the training and test datasets. Temperature data for mo del training is uploaded hourly. During testing, yo ur model performed with 97% accuracy; however, after deployi ng to production, the model's accuracy dropped to 6 6%. How can you make your production model more accurat e?",
        "options": [
            "A. Normalize the data for the training, and test dat asets as two separate steps.",
            "B. Split the training and test data based on time ra ther than a random split to avoid leakage.",
            "C. Add more data to your test set to ensure that you  have a fair distribution and sample for testing.",
            "D. Apply data transformations before splitting, and cross-validate to make sure that the transformation s are"
        ],
        "correct": "B. Split the training and test data based on time ra ther than a random split to avoid leakage.",
        "explanation": "Explanation:\nThe correct answer is B. Split the training and test data based on time rather than a random split to avoid leakage. \n\nIn this scenario, the model was trained on hourly data but performed poorly in production. This is likely due to a phenomenon known as \"data leakage\". Data leakage occurs when the model is trained on data that includes information from the future, which is not available during deployment. \n\nIn this case, the random split of the data may have included future data points in the training set, which the model learned to rely on for its predictions. When deployed to production, the model did not have access to this future data and thus performed poorly.\n\nBy splitting the training and test data based on time, you can ensure that the model is trained on past data and tested on future data, which is a more realistic scenario for production. This approach helps to avoid data leakage and ensures that the model is evaluated on its ability to make predictions on unseen data.\n\nOption A is incorrect because normalizing the data separately for the training and test datasets will not address the issue of data leakage. Normalization is a preprocessing step that is typically applied to the entire dataset before splitting it into training and test sets.\n\nOption C is incorrect because adding more data to the test set will not address the issue of data leakage. The problem is not with the size of the test set, but rather with the fact that the model is being trained on data that includes information from the future.\n\nOption D is incorrect",
        "references": ""
    },
    {
        "question": "You are developing models to classify customer supp ort emails. You created models with TensorFlow Estimators using small datasets on your on-premises  system, but you now need to train the models using  large datasets to ensure high performance. You will port your models to Google Cloud and want to minimize co de refactoring and infrastructure overhead for easier migration from on-prem to cloud. What should you do ?",
        "options": [
            "A. Use AI Platform for distributed training.",
            "B. Create a cluster on Dataproc for training.",
            "C. Create a Managed Instance Group with autoscaling.",
            "D. Use Kubeflow Pipelines to train on a Google Kuber netes Engine cluster."
        ],
        "correct": "A. Use AI Platform for distributed training.",
        "explanation": "Explanation:\n\nThe correct answer is A. Use AI Platform for distributed training. \n\nTensorFlow Estimators are a high-level API for building TensorFlow models, and AI Platform provides a managed service for distributed training of TensorFlow models. By using AI Platform, you can easily scale up your training jobs to handle large datasets without having to worry about the underlying infrastructure. This minimizes code refactoring and infrastructure overhead, making it easier to migrate from on-prem to cloud.\n\nOption B, creating a cluster on Dataproc for training, is incorrect because Dataproc is a managed service for running Apache Spark and Hadoop workloads, not TensorFlow models. While you could use Dataproc to preprocess your data, it's not the best choice for training TensorFlow models.\n\nOption C, creating a Managed Instance Group with autoscaling, is incorrect because this is an infrastructure-level solution that would require you to manage the underlying infrastructure, which goes against the goal of minimizing infrastructure overhead.\n\nOption D, using Kubeflow Pipelines to train on a Google Kubernetes Engine cluster, is incorrect because while Kubeflow Pipelines is a great tool for building and deploying machine learning pipelines, it requires more infrastructure expertise and would require more code refactoring than using AI Platform.\n\nTherefore, the correct answer is A. Use AI Platform for distributed training.",
        "references": ""
    },
    {
        "question": "You have trained a text classification model in Ten sorFlow using AI Platform. You want to use the trai ned model for batch predictions on text data stored in BigQuery while minimizing computational overhead. W hat should you do?",
        "options": [
            "A. Export the model to BigQuery ML.",
            "B. Deploy and version the model on AI Platform. C. Use Dataflow with the SavedModel to read the data f rom BigQuery.",
            "D. Submit a batch prediction job on AI Platform that  points to the model location in Cloud Storage."
        ],
        "correct": "A. Export the model to BigQuery ML.",
        "explanation": "Explanation:\nThe correct answer is A. Export the model to BigQuery ML. This is because BigQuery ML allows you to use your trained model for predictions directly within BigQuery, without having to move the data out of BigQuery or set up a separate prediction environment. This minimizes computational overhead and allows you to leverage the scalability and performance of BigQuery for your predictions.\n\nOption B is incorrect because deploying and versioning the model on AI Platform does not directly allow you to use the model for batch predictions on data stored in BigQuery. You would still need to set up a separate prediction environment and move the data out of BigQuery, which would increase computational overhead.\n\nOption C is incorrect because using Dataflow with the SavedModel to read the data from BigQuery would require setting up a separate pipeline and moving the data out of BigQuery, which would increase computational overhead.\n\nOption D is incorrect because submitting a batch prediction job on AI Platform that points to the model location in Cloud Storage would require moving the data out of BigQuery and setting up a separate prediction environment, which would increase computational overhead.\n\nTherefore, the correct answer is A. Export the model to BigQuery ML.",
        "references": ""
    },
    {
        "question": "You work with a data engineering team that has deve loped a pipeline to clean your dataset and save it in a Cloud Storage bucket. You have created an ML model and want to use the data to refresh your model as s oon as new data is available. As part of your CI/CD wor kflow, you want to automatically run a Kubeflow Pip elines training job on Google Kubernetes Engine (GKE). How  should you architect this workflow?",
        "options": [
            "A. Configure your pipeline with Dataflow, which save s the files in Cloud Storage. After the file is sav ed, start",
            "B. Use App Engine to create a lightweight python cli ent that continuously polls Cloud Storage for new f iles. As",
            "C. Configure a Cloud Storage trigger to send a messa ge to a Pub/Sub topic when a new file is available in a",
            "D. Use Cloud Scheduler to schedule jobs at a regular  interval. For the first step of the job, check the  timestamp"
        ],
        "correct": "C. Configure a Cloud Storage trigger to send a messa ge to a Pub/Sub topic when a new file is available in a",
        "explanation": "Explanation: \nThe correct answer is C. Configure a Cloud Storage trigger to send a message to a Pub/Sub topic when a new file is available in a. \n\nThis is because the requirement is to automatically run a Kubeflow Pipelines training job on Google Kubernetes Engine (GKE) as soon as new data is available.  The Cloud Storage trigger can detect new files added to the bucket and send a message to a Pub/Sub topic. Then, a Cloud Function can be triggered by this Pub/Sub topic to run the Kubeflow Pipelines training job on GKE. This architecture ensures that the ML model is refreshed as soon as new data is available.\n\nThe other options are incorrect because:\n\nA. Dataflow is a fully-managed service for transforming and processing data, but it is not designed to trigger a Kubeflow Pipelines training job on GKE. \n\nB. Using App Engine to continuously poll Cloud Storage for new files is not efficient and can result in unnecessary costs. It's better to use event-driven architecture with Cloud Storage triggers.\n\nD. Cloud Scheduler is a job scheduler that can run jobs at regular intervals, but it's not designed to trigger a job based on new data availability in Cloud Storage. It would require additional logic to check for new files, which is not efficient.\n\nI hope it is correct.",
        "references": ""
    },
    {
        "question": "You have a functioning end-to-end ML pipeline that involves tuning the hyperparameters of your ML mode l using AI Platform, and then using the best-tuned pa rameters for training. Hypertuning is taking longer  than expected and is delaying the downstream processes. You want to speed up the tuning job without signifi cantly compromising its effectiveness. Which actions shoul d you take? (Choose two.)",
        "options": [
            "A. Decrease the number of parallel trials.",
            "B. Decrease the range of floating-point values.",
            "C. Set the early stopping parameter to TRUE.",
            "D. Change the search algorithm from Bayesian search to random search."
        ],
        "correct": "",
        "explanation": "Explanation:\nThe correct answer is D. Build a notification system on Firebase.\n\nThe application is being built for a global bank with millions of customers. To serve predictions, we need a scalable solution that can handle a large number of users. Firebase is a cloud-based platform that provides a scalable and reliable solution for building web and mobile applications. It has a built-in notification system called Firebase Cloud Messaging (FCM) that allows sending targeted, personalized messages to users. \n\nOption A and B are incorrect because creating a Pub/Sub topic for each user would not be scalable and would result in a large number of topics, making it difficult to manage. Pub/Sub is a messaging service that allows asynchronous communication between independent applications, but it's not designed for sending notifications to users.\n\nOption C is incorrect because it's similar to option A and B, it's not a scalable solution.\n\nIn summary, the correct answer is D because Firebase provides a scalable and reliable solution for building web and mobile applications and has a built-in notification system that can handle a large number of users.",
        "references": ""
    },
    {
        "question": "del that predicts customers' account balances 3 day s in the future. Your team will use the results in a new feature that Your team is building an application for a global b ank that will be used by millions of customers. You  built a forecasting mo will notify users when their account  balance is likely to drop below $25. How should yo u serve your predictions?",
        "options": [
            "A. 1. Create a Pub/Sub topic for each user.",
            "B. 1. Create a Pub/Sub topic for each user.",
            "C. 1. Build a notification system on Firebase.",
            "D. 1. Build a notification system on Firebase."
        ],
        "correct": "D. 1. Build a notification system on Firebase.",
        "explanation": "C. Set the early stopping parameter to TRUE.\nD. Change the search algorithm from Bayesian search to random search.\n\nExplanation:\nThe correct answer is C and D. \n\nOption C: \nSetting the early stopping parameter to TRUE will stop the hyperparameter tuning job once it reaches a certain level of convergence. This will save time and resources by stopping the tuning process when it has reached a good enough level of performance. This is especially useful if you have a large number of trials and the tuning job is taking longer than expected.\n\nOption D: \nChanging the search algorithm from Bayesian search to random search will speed up the tuning job. Bayesian search is a more complex algorithm that tries to find the optimal hyperparameters by using a probabilistic approach. It is more accurate but takes longer to run. Random search, on the other hand, is a simpler algorithm that randomly samples the hyperparameter space. It is faster but less accurate. By switching to random search, you can speed up the tuning job, but you may compromise on the accuracy of the results.\n\nIncorrect Options:\n\nOption A: Decreasing the number of parallel trials will actually slow down the tuning job, not speed it up. This is because parallel trials allow you to explore multiple hyperparameter combinations simultaneously, which can speed up the tuning process.\n\nOption B: Decreasing the range of floating-point values will not necessarily speed up the tuning job. It may actually make the tuning job less effective by reducing the range of possible hyperparameters to search from.\n\n",
        "references": ""
    },
    {
        "question": "You work for an advertising company and want to und erstand the effectiveness of your company's latest advertising campaign. You have streamed 500 MB of c ampaign data into BigQuery. You want to query the table, and then manipulate the results of that quer y with a pandas dataframe in an AI Platform noteboo k. What should you do?",
        "options": [
            "A. Use AI Platform Notebooks' BigQuery cell magic to  query the data, and ingest the results as a pandas",
            "B. Export your table as a CSV file from BigQuery to Google Drive, and use the Google Drive API to inges t the",
            "C. Download your table from BigQuery as a local CSV file, and upload it to your AI Platform notebook in stance.",
            "D. From a bash cell in your AI Platform notebook, us e the bq extract command to export the table as a C SV"
        ],
        "correct": "A. Use AI Platform Notebooks' BigQuery cell magic to  query the data, and ingest the results as a pandas",
        "explanation": "Explanation:\nThe correct answer is A. Use AI Platform Notebooks' BigQuery cell magic to query the data, and ingest the results as a pandas. This is because AI Platform Notebooks provides a seamless integration with BigQuery through its cell magic feature. With this feature, you can query BigQuery tables directly from your notebook and ingest the results as a pandas dataframe. This approach eliminates the need for data export, upload, or download, making it a more efficient and convenient option.\n\nOption B is incorrect because exporting the table as a CSV file from BigQuery to Google Drive and then using the Google Drive API to ingest it would require additional steps and infrastructure setup. This approach would also involve data transfer between services, which could be slower and less efficient.\n\nOption C is incorrect because downloading the table from BigQuery as a local CSV file and uploading it to your AI Platform notebook instance would require manual intervention and would not utilize the integration features provided by AI Platform Notebooks.\n\nOption D is incorrect because using the bq extract command from a bash cell in your AI Platform notebook would require command-line interaction and would not provide a seamless integration with BigQuery. Additionally, this approach would also involve data export and upload, which could be slower and less efficient.",
        "references": ""
    },
    {
        "question": "You are an ML engineer at a global car manufacture.  You need to build an ML model to predict car sales  in different cities around the world. Which features o r feature crosses should you use to train city-spec ific relationships between car type and number of sales?A. Thee individual features: binned latitude, binned l ongitude, and one-hot encoded car type.",
        "options": [
            "B. One feature obtained as an element-wise product b etween latitude, longitude, and car type.",
            "C. One feature obtained as an element-wise product b etween binned latitude, binned longitude, and one-h ot",
            "D. Two feature crosses as an element-wise product: t he first between binned latitude and one-hot encode d car"
        ],
        "correct": "C. One feature obtained as an element-wise product b etween binned latitude, binned longitude, and one-h ot",
        "explanation": "Here is the explanation:\n\nThe correct answer is C. One feature obtained as an element-wise product between binned latitude, binned longitude, and one-hot encoded car type.\n\nThis is because we want to capture the city-specific relationships between car type and number of sales. By binning latitude and longitude, we can create a unique identifier for each city. Then, by taking the element-wise product with one-hot encoded car type, we can create a feature that captures the interaction between car type and city. This allows the model to learn different relationships between car type and sales for different cities.\n\nOption A is incorrect because using individual features for latitude, longitude, and car type would not capture the interactions between these features. The model would not be able to learn city-specific relationships between car type and sales.\n\nOption B is incorrect because taking the element-wise product between latitude, longitude, and car type would result in a feature that is not interpretable and would not capture the city-specific relationships. Latitude and longitude are continuous features, and taking their product would not create a meaningful feature.\n\nOption D is incorrect because creating two feature crosses, one between binned latitude and one-hot encoded car type, and another between binned longitude and one-hot encoded car type, would not capture the interactions between all three features (latitude, longitude, and car type). We need to capture the interaction between all three features to learn city-specific relationships between car type and sales.",
        "references": ""
    },
    {
        "question": "You work for a large technology company that wants to modernize their contact center. You have been as ked to develop a solution to classify incoming calls by pr oduct so that requests can be more quickly routed t o the correct support team. You have already transcribed the calls using the Speech-to-Text API. You want to minimize data preprocessing and development time. H ow should you build the model?",
        "options": [
            "A. Use the AI Platform Training built-in algorithms to create a custom model.",
            "B. Use AutoMlL Natural Language to extract custom en tities for classification.",
            "C. Use the Cloud Natural Language API to extract cus tom entities for classification.",
            "D. Build a custom model to identify the product keyw ords from the transcribed calls, and then run the k eywords"
        ],
        "correct": "B. Use AutoMlL Natural Language to extract custom en tities for classification.",
        "explanation": "Explanation:\nThe correct answer is B. Use AutoMlL Natural Language to extract custom entities for classification. This is because AutoML Natural Language is a fully managed service that allows you to train custom machine learning models on your data without requiring extensive machine learning expertise. It can automatically extract custom entities from text data, such as product names, and classify them accordingly. This approach minimizes data preprocessing and development time, as AutoML Natural Language can handle the entity extraction and classification tasks for you.\n\nOption A is incorrect because while AI Platform Training provides built-in algorithms for training custom models, it requires more development time and expertise compared to AutoML Natural Language. Additionally, AI Platform Training may require more data preprocessing, which is not ideal in this scenario.\n\nOption C is incorrect because the Cloud Natural Language API is primarily used for sentiment analysis, entity recognition, and text analysis, but it is not designed for custom entity extraction and classification like AutoML Natural Language.\n\nOption D is incorrect because building a custom model to identify product keywords from transcribed calls requires significant development time and expertise, and may also require manual data preprocessing, which is not ideal in this scenario.",
        "references": ""
    },
    {
        "question": "You are training a TensorFlow model on a structured  dataset with 100 billion records stored in several  CSV files. You need to improve the input/output executi on performance. What should you do?",
        "options": [
            "A. Load the data into BigQuery, and read the data fr om BigQuery.",
            "B. Load the data into Cloud Bigtable, and read the d ata from Bigtable.",
            "C. Convert the CSV files into shards of TFRecords, a nd store the data in Cloud Storage.",
            "D. Convert the CSV files into shards of TFRecords, a nd store the data in the Hadoop Distributed File Sy stem"
        ],
        "correct": "C. Convert the CSV files into shards of TFRecords, a nd store the data in Cloud Storage.",
        "explanation": "The correct answer is A. Use the batch prediction functionality of AI Platform.\n\nExplanation: \nThe correct answer is A because the requirement is to use the ML model on the aggregated data collected at the end of each day with minimal manual intervention. This implies that the prediction needs to be done in batches, rather than in real-time or on-demand. AI Platform's batch prediction functionality is designed for this purpose, allowing you to run predictions on large datasets in an automated and scalable manner.\n\nOption B is incorrect because creating a serving pipeline in Compute Engine would require manual intervention and would not be scalable for large datasets.\n\nOption C is incorrect because Cloud Functions are designed for real-time or on-demand processing, not for batch processing. Additionally, using Cloud Functions for prediction each time a new data point is ingested would lead to high costs and inefficient use of resources.\n\nOption D is incorrect because deploying the model on AI Platform and creating a version of it for online inference would allow for real-time predictions, but would not meet the requirement of minimal manual intervention for batch processing.",
        "references": ""
    },
    {
        "question": "As the lead ML Engineer for your company, you are r esponsible for building ML models to digitize scann ed customer forms. You have developed a TensorFlow mod el that converts the scanned images into text and stores them in Cloud Storage. You need to use your ML model on the aggregated data collected at the en d of each day with minimal manual intervention. What sho uld you do? A. Use the batch prediction functionality of AI Platfo rm.",
        "options": [
            "B. Create a serving pipeline in Compute Engine for p rediction.",
            "C. Use Cloud Functions for prediction each time a ne w data point is ingested.",
            "D. Deploy the model on AI Platform and create a vers ion of it for online inference."
        ],
        "correct": "",
        "explanation": "Explanation:\nThe correct answer is C. Convert the CSV files into shards of TFRecords, and store the data in Cloud Storage. \n\nTensorFlow is optimized to read data from TFRecords, which is a binary format optimized for TensorFlow. By converting the CSV files into shards of TFRecords, you can significantly improve the input/output execution performance. Additionally, storing the data in Cloud Storage allows for efficient data access and scalability.\n\nOption A is incorrect because BigQuery is a data warehousing solution, not optimized for TensorFlow data input. While it's possible to read data from BigQuery, it's not the most efficient solution for this use case.\n\nOption B is incorrect because Cloud Bigtable is a NoSQL database, not optimized for TensorFlow data input. Bigtable is designed for large-scale analytics and is not suitable for this use case.\n\nOption D is incorrect because Hadoop Distributed File System (HDFS) is a distributed file system, but it's not a cloud-based solution, and it's not optimized for TensorFlow data input. Additionally, HDFS requires significant infrastructure and maintenance, which is not necessary for this use case.\n\nIn summary, the correct answer is C because it leverages TensorFlow's optimized data format (TFRecords) and cloud-based storage (Cloud Storage) for efficient data access and scalability.",
        "references": ""
    },
    {
        "question": "You recently joined an enterprise-scale company tha t has thousands of datasets. You know that there ar e accurate descriptions for each table in BigQuery, a nd you are searching for the proper BigQuery table to use for a model you are building on AI Platform. How should  you find the data that you need?",
        "options": [
            "A. Use Data Catalog to search the BigQuery datasets by using keywords in the table description.",
            "B. Tag each of your model and version resources on A I Platform with the name of the BigQuery table that  was",
            "C. Maintain a lookup table in BigQuery that maps the  table descriptions to the table ID. Query the look up table",
            "D. Execute a query in BigQuery to retrieve all the e xisting table names in your project using the"
        ],
        "correct": "A. Use Data Catalog to search the BigQuery datasets by using keywords in the table description.",
        "explanation": "Explanation:\nThe correct answer is A because Data Catalog is a fully managed service that enables you to discover, manage, and optimize your enterprise data assets across Google Cloud and on-premises environments. Data Catalog provides a centralized inventory of your data assets, including BigQuery datasets and tables, and allows you to search for datasets and tables using keywords in the table description.\n\nOption B is incorrect because tagging model and version resources on AI Platform with the name of the BigQuery table does not provide a scalable solution for searching and discovering datasets. This approach would require manual effort to maintain the tags, and it would not be efficient for searching through thousands of datasets.\n\nOption C is also incorrect because maintaining a lookup table in BigQuery that maps table descriptions to table IDs would require additional storage and maintenance efforts. This approach would also not be scalable for large numbers of datasets.\n\nOption D is incorrect because executing a query in BigQuery to retrieve all existing table names in your project would not provide a scalable solution for searching and discovering datasets. This approach would require querying all tables in the project, which could be time-consuming and resource-intensive.\n\nTherefore, the correct answer is A, which uses Data Catalog to search for BigQuery datasets by using keywords in the table description. This approach provides a scalable, efficient, and centralized solution for discovering and managing data assets.",
        "references": ""
    },
    {
        "question": "cteristic curve (AUC ROC) value of 99% for training  data after just a few experiments. You haven't exp lored using You started working on a classification problem wit h time series data and achieved an area under the r eceiver operating chara any sophisticated algorithms or spe nt any time on hyperparameter tuning. What should y our next step be to identify and fix the problem?",
        "options": [
            "A. Address the model overfitting by using a less com plex algorithm.",
            "B. Address data leakage by applying nested cross-val idation during model training.",
            "C. Address data leakage by removing features highly correlated with the target value.",
            "D. Address the model overfitting by tuning the hyper parameters to reduce the AUC ROC value."
        ],
        "correct": "B. Address data leakage by applying nested cross-val idation during model training.",
        "explanation": "Explanation:\nThe correct answer is D. Embed the client on the website, deploy the gateway on App Engine, deploy the database on Memorystore. \n\nThis solution is the simplest because it leverages Memorystore, which is a Redis in-memory data store that can handle high-throughput and low-latency requirements. By deploying the database on Memorystore, we can ensure that the model can retrieve the web banners quickly and efficiently, meeting the latency requirement of 300ms@p99. \n\nOption A is incorrect because deploying the model on AI Platform Prediction alone does not provide a complete solution for handling the web banners and navigation context. \n\nOption B is incorrect because deploying the gateway on App Engine and the model on AI Platform Prediction does not provide a solution for storing and retrieving the web banners efficiently. \n\nOption C is incorrect because deploying the database on Cloud Bigtable is not suitable for this use case, as it is designed for large-scale analytics and not for low-latency, high-throughput applications.",
        "references": ""
    },
    {
        "question": "You have been asked to predict the most relevant we b banner that a user should see next. Security is important to your company. The model latency requir ements are 300ms@p99, the inventory is thousands of web banners, and your exploratory analysis has show n that navigation context is a good predictor. You want to Implement the simplest solution. How should you con figure the prediction pipeline?",
        "options": [
            "A. Embed the client on the website, and then deploy the model on AI Platform Prediction.",
            "B. Embed the client on the website, deploy the gatew ay on App Engine, and then deploy the model on AI",
            "C. Embed the client on the website, deploy the gatew ay on App Engine, deploy the database on Cloud Bigt able",
            "D. Embed the client on the website, deploy the gatew ay on App Engine, deploy the database on Memorystor e"
        ],
        "correct": "D. Embed the client on the website, deploy the gatew ay on App Engine, deploy the database on Memorystor e",
        "explanation": "Explanation:\n\nThe correct answer is D. A Deep Learning VM with more powerful CPU e2-high cpu-16 machines with all libraries pre-installed. \n\nThis is because the question states that the code does not include any manual device placement and has not been wrapped in Estimator model-level abstraction. This means that the code is not optimized for GPU or TPU usage, and it will automatically utilize the available CPU resources. \n\nOption A is incorrect because it includes a TPU, which would require manual device placement or Estimator model-level abstraction to utilize. \n\nOption B is incorrect because it includes 8 GPUs, which would also require manual device placement or Estimator model-level abstraction to utilize. \n\nOption C is incorrect because it includes 1 GPU, which would also require manual device placement or Estimator model-level abstraction to utilize.\n\nTherefore, the correct answer is Option D, which provides a powerful CPU-only environment that can automatically utilize the available CPU resources without requiring any modifications to the code.",
        "references": ""
    },
    {
        "question": "Your team is building a convolutional neural networ k (CNN)-based architecture from scratch. The prelim inary experiments running on your on-premises CPU-only in frastructure were encouraging, but have slow convergence. You have been asked to speed up model training to reduce time-to-market. You want to experiment with virtual machines (VMs) on Google Cl oud to leverage more powerful hardware. Your code d oes not include any manual device placement and has not  been wrapped in Estimator model-level abstraction. Which environment should you train your model on?",
        "options": [
            "A. AVM on Compute Engine and 1 TPU with all dependen cies installed manually.",
            "B. AVM on Compute Engine and 8 GPUs with all depende ncies installed manually.",
            "C. A Deep Learning VM with an n1-standard-2 machine and 1 GPU with all libraries pre-installed.",
            "D. A Deep Learning VM with more powerful CPU e2-high cpu-16 machines with all libraries pre-installed."
        ],
        "correct": "D. A Deep Learning VM with more powerful CPU e2-high cpu-16 machines with all libraries pre-installed.",
        "explanation": "Explanation:\n\nThe correct answer is B. Address data leakage by applying nested cross-validation during model training.\n\nWhen a model performs exceptionally well on the training data, it may indicate data leakage. Data leakage occurs when the model is trained on data that includes information about the target variable, which is not available during prediction. This can happen when the training data is not properly split or when there are features that are highly correlated with the target value.\n\nIn this scenario, achieving an AUC ROC value of 99% after just a few experiments without using sophisticated algorithms or spending time on hyperparameter tuning suggests that the model may be overfitting or that there is data leakage.\n\nOption B is the correct answer because applying nested cross-validation during model training can help identify and address data leakage. Nested cross-validation involves splitting the data into training and validation sets, and then using cross-validation to evaluate the model on the validation set. This helps to ensure that the model is not overfitting to the training data and that the performance metrics are more accurate.\n\nOption A is incorrect because using a less complex algorithm may not address the underlying issue of data leakage. Additionally, a less complex algorithm may not perform as well as a more complex algorithm on the training data.\n\nOption C is incorrect because removing features highly correlated with the target value may not address the issue of data leakage. It may also remove important features that are useful for prediction.\n\nOption D is incorrect because tuning hyperparameters to reduce the AUC ROC value may not address",
        "references": ""
    },
    {
        "question": "You work on a growing team of more than 50 data sci entists who all use AI Platform. You are designing a strategy to organize your jobs, models, and version s in a clean and scalable way. Which strategy shoul d you choose?",
        "options": [
            "A. Set up restrictive IAM permissions on the AI Plat form notebooks so that only a single user or group can",
            "B. Separate each data scientist's work into a differ ent project to ensure that the jobs, models, and ve rsions",
            "C. Use labels to organize resources into descriptive  categories. Apply a label to each created resource  so that"
        ],
        "correct": "C. Use labels to organize resources into descriptive  categories. Apply a label to each created resource  so that",
        "explanation": "Explanation:\nThe correct answer is C. Use labels to organize resources into descriptive categories. Apply a label to each created resource so that.\n\nThis strategy is recommended because it allows for a flexible and scalable way to organize resources, including jobs, models, and versions. Labels can be used to categorize resources based on various criteria such as data scientist, project, environment, or any other relevant category. This approach makes it easy to filter, search, and manage resources, and it also enables data scientists to collaborate and share resources more effectively.\n\nOption A is incorrect because setting up restrictive IAM permissions on AI Platform notebooks would limit access to resources and hinder collaboration among data scientists. This approach would not provide a scalable solution for organizing resources.\n\nOption B is also incorrect because separating each data scientist's work into a different project would lead to a proliferation of projects, making it difficult to manage and maintain resources. This approach would not provide a clean and scalable way to organize resources.\n\nIn summary, using labels to organize resources into descriptive categories is the best strategy for organizing jobs, models, and versions in a clean and scalable way, especially in a large team of data scientists.",
        "references": ""
    },
    {
        "question": "You are training a deep learning model for semantic  image segmentation with reduced training time. Whi le using a Deep Learning VM Image, you receive the fol lowing error: The resource 'projects/deeplearning-p latforn/ zones/europe-west4-c/acceleratorTypes/nvidia-tesla- k80' was not found. What should you do?",
        "options": [
            "A. Ensure that you have GPU quota in the selected re gion.",
            "B. Ensure that the required GPU is available in the selected region.",
            "C. Ensure that you have preemptible GPU quota in the  selected region.",
            "D. Ensure that the selected GPU has enough GPU memor y for the workload."
        ],
        "correct": "A. Ensure that you have GPU quota in the selected re gion.",
        "explanation": "Explanation:\nThe correct answer is A. Ensure that you have GPU quota in the selected region. This is because the error message indicates that the resource 'projects/deeplearning-platform/zones/europe-west4-c/acceleratorTypes/nvidia-tesla-k80' was not found, which means that the NVIDIA Tesla K80 GPU accelerator is not available in the europe-west4-c region. This could be due to the fact that the GPU quota for the region has been exhausted or not allocated.\n\nThe other options are incorrect because:\nOption B is incorrect because even if the required GPU is available in the selected region, the error message indicates that the resource was not found, which means that the GPU quota has not been allocated.\nOption C is incorrect because preemptible GPU quota is not related to the error message, as it is a type of quota that allows for the use of GPUs on a best-effort basis.\nOption D is incorrect because the error message does not indicate that the selected GPU has insufficient memory, but rather that the resource was not found.\n\nIn summary, the correct answer is A because the error message indicates that the GPU quota has not been allocated in the selected region, which is the root cause of the problem.",
        "references": ""
    },
    {
        "question": "Your team is working on an NLP research project to predict political affiliation of authors based on a rticles they have written. You have a large training dataset tha t is structured like this: You followed the standard 80%-10%-10% data distribu tion across the training, testing, and evaluation s ubsets. How should you distribute the training examples acr oss the train-test-eval subsets while maintaining t he 80-10- 10 proportion? A. Distribute texts randomly across the train-test-e val subsets: Train set: [TextA1, TextB2, ...] Test set: [TextA2, TextC1, TextD2, ...] Eval set: [TextB1, TextC2, TextD1, ...]",
        "options": [
            "B. Distribute authors randomly across the train-test -eval subsets: (*)",
            "C. Distribute sentences randomly across the train-te st-eval subsets:",
            "D. Distribute paragraphs of texts (i.e., chunks of c onsecutive sentences) across the train-test-eval su bsets:"
        ],
        "correct": "B. Distribute authors randomly across the train-test -eval subsets: (*)",
        "explanation": "Explanation:\n\nThe correct answer is B. Distribute authors randomly across the train-test-eval subsets. This approach ensures that the model learns to generalize across different authors, which is essential for predicting political affiliation. By distributing authors randomly, you ensure that the model sees a diverse range of writing styles, vocabularies, and biases, making it more robust and accurate.\n\nOption A is incorrect because distributing texts randomly may lead to overfitting, as the model may memorize specific texts rather than learning to generalize across authors. This approach also ignores the authorship aspect, which is crucial for predicting political affiliation.\n\nOption C is incorrect because distributing sentences randomly may not capture the author's writing style and tone, which are essential features for predicting political affiliation. Sentences may be too short to convey the author's perspective, and the model may not learn to generalize across sentences.\n\nOption D is incorrect because distributing paragraphs of texts may not be sufficient to capture the author's writing style and tone. Paragraphs may be too short or too long, and the model may not learn to generalize across paragraphs.\n\nIn summary, distributing authors randomly across the train-test-eval subsets is the correct approach because it ensures that the model learns to generalize across different authors, writing styles, and biases, making it more robust and accurate for predicting political affiliation.",
        "references": ""
    },
    {
        "question": "Your team has been tasked with creating an ML solut ion in Google Cloud to classify support requests fo r one of your platforms. You analyzed the requirements and d ecided to use TensorFlow to build the classifier so  that you have full control of the model's code, serving,  and deployment. You will use Kubeflow pipelines fo r the ML platform. To save time, you want to build on existi ng resources and use managed services instead of bu ilding a completely new model. How should you build the clas sifier?",
        "options": [
            "A. Use the Natural Language API to classify support requests.",
            "B. Use AutoML Natural Language to build the support requests classifier.",
            "C. Use an established text classification model on A I Platform to perform transfer learning.",
            "D. Use an established text classification model on A I Platform as-is to classify support requests."
        ],
        "correct": "C. Use an established text classification model on A I Platform to perform transfer learning.",
        "explanation": "Explanation:\nThe correct answer is A. Ensure that training is reproducible. This is because reproducibility is a critical aspect of production readiness. Reproducibility means that the model can be re-trained using the same data and hyperparameters, and it should produce the same results. This is essential in production environments where the model may need to be re-trained frequently. If the training process is not reproducible, it can lead to inconsistent results, which can negatively impact the business.\n\nOption B is incorrect because hyperparameter tuning is already assumed to be complete, as the question states that the team has already tested features and data model development.\n\nOption C is incorrect because model performance monitoring is important, but it is not directly related to production readiness. Model performance monitoring is more related to the ongoing maintenance and improvement of the model.\n\nOption D is incorrect because capturing feature expectations in the schema is important for data quality, but it is not directly related to production readiness. Data quality is an important aspect of the overall ML workflow, but it is not the focus of this question.\n\nIn summary, the correct answer is A. Ensure that training is reproducible because it is a critical aspect of production readiness, and it ensures that the model can be re-trained consistently and produce the same results.",
        "references": ""
    },
    {
        "question": "are asked to determine the production readiness of the ML components. The team has already tested feat ures and data, model development, and infrastructure. Wh ich additional readiness check should you recommend  to the team?",
        "options": [
            "A. Ensure that training is reproducible.",
            "B. Ensure that all hyperparameters are tuned.",
            "C. Ensure that model performance is monitored.",
            "D. Ensure that feature expectations are captured in the schema."
        ],
        "correct": "A. Ensure that training is reproducible.",
        "explanation": "Explanation:\nThe correct answer is C. Use an established text classification model on AI Platform to perform transfer learning.\n\nThis option is correct because you want to build on existing resources and use managed services instead of building a completely new model from scratch. You can use an established text classification model on AI Platform, which is a managed service provided by Google Cloud, and perform transfer learning to adapt the model to your specific use case. This approach allows you to leverage the knowledge and features learned by the pre-trained model and fine-tune it for your specific classification task.\n\nOption A is incorrect because the Natural Language API is a pre-trained model that provides a set of pre-built functionality for natural language processing tasks, but it does not allow you to have full control of the model's code, serving, and deployment. You would not be able to customize the model to your specific needs.\n\nOption B is incorrect because AutoML Natural Language is a fully managed service that allows you to train custom machine learning models without extensive machine learning expertise. While it's a great option for building a custom model, it does not allow you to have full control of the model's code, serving, and deployment.\n\nOption D is incorrect because using an established text classification model on AI Platform as-is would not allow you to adapt the model to your specific use case. You would not be able to fine-tune the model to your specific classification task, which is necessary to achieve good performance.\n\nOverall, option C is the best choice because it",
        "references": ""
    },
    {
        "question": "You work for a credit card company and have been as ked to create a custom fraud detection model based on historical data using AutoML Tables. You need to pr ioritize detection of fraudulent transactions while  minimizing false positives. Which optimization objective should you use when tr aining the model?",
        "options": [
            "A. An optimization objective that minimizes Log loss",
            "B. An optimization objective that maximizes the Prec ision at a Recall value of 0.50",
            "C. An optimization objective that maximizes the area  under the precision-recall curve (AUC PR) value",
            "D. An optimization objective that maximizes the area  under the receiver operating characteristic curve (AUC"
        ],
        "correct": "D. An optimization objective that maximizes the area  under the receiver operating characteristic curve (AUC",
        "explanation": "Explanation:\n\nThe correct answer is D. An optimization objective that maximizes the area under the receiver operating characteristic curve (AUC ROC).\n\nThe reason for this is that the AUC ROC is a measure of the model's ability to distinguish between true positives (fraudulent transactions) and false positives (non-fraudulent transactions). In this case, the goal is to prioritize detection of fraudulent transactions while minimizing false positives. The AUC ROC is a more comprehensive metric that takes into account both the true positive rate and the false positive rate, making it a better optimization objective for this problem.\n\nOption A, minimizing Log loss, is not suitable because Log loss is a measure of the difference between the predicted probabilities and the true labels, but it does not directly optimize for the trade-off between true positives and false positives.\n\nOption B, maximizing Precision at a Recall value of 0.50, is also not suitable because it only considers the precision at a specific recall value, whereas the AUC ROC takes into account the entire range of possible threshold values.\n\nOption C, maximizing the area under the precision-recall curve (AUC PR), is related to the AUC ROC, but it is more focused on the precision-recall trade-off, whereas the AUC ROC is a more comprehensive metric that takes into account both the true positive rate and the false positive rate.\n\nTherefore, the correct answer is D, maximizing the area under the receiver operating characteristic curve (AUC ROC).",
        "references": ""
    },
    {
        "question": "Your company manages a video sharing website where users can watch and upload videos. You need to create an ML model to predict which newly uploaded videos will be the most popular so that those video s can be prioritized on your company's website. Which res ult should you use to determine whether the model i s successful?",
        "options": [
            "A. The model predicts videos as popular if the user who uploads them has over 10,000 likes.",
            "B. The model predicts 97.5% of the most popular clic kbait videos measured by number of clicks.",
            "C. The model predicts 95% of the most popular videos  measured by watch time within 30 days of being",
            "D. The Pearson correlation coefficient between the l og-transformed number of views after 7 days and 30 days"
        ],
        "correct": "",
        "explanation": "D. The Pearson correlation coefficient between the log-transformed number of views after 7 days and 30 days\n\nExplanation: \n\nThe correct answer is D. The Pearson correlation coefficient between the log-transformed number of views after 7 days and 30 days. \n\nThis is because the goal of the ML model is to predict which newly uploaded videos will be the most popular. The most popular videos can be measured by the number of views they receive over time. A good ML model should be able to predict the videos that will receive a high number of views in the long run (e.g., 30 days) based on their performance in the short run (e.g., 7 days). \n\nThe Pearson correlation coefficient measures the strength and direction of the linear relationship between two continuous variables. In this case, it measures how well the log-transformed number of views after 7 days predicts the log-transformed number of views after 30 days. A high correlation coefficient (close to 1) indicates that the model is successful in predicting the most popular videos.\n\nNow, let's explain why the other options are incorrect.\n\nOption A is incorrect because it focuses on the user who uploads the video rather than the video itself. The popularity of a video is not solely determined by the user who uploads it. \n\nOption B is incorrect because it focuses on a specific type of video (clickbait) rather than all types of videos. The goal of the ML model is to predict the most popular",
        "references": ""
    },
    {
        "question": "You are working on a Neural Network-based project. The dataset provided to you has columns with differ ent ranges. While preparing the data for model training , you discover that gradient optimization is having  difficulty moving weights to a good solution. What should you do?",
        "options": [
            "A. Use feature construction to combine the strongest  features.",
            "B. Use the representation transformation (normalizat ion) technique.",
            "C. Improve the data cleaning step by removing featur es with missing values.",
            "D. Change the partitioning step to reduce the dimens ion of the test set and have a larger training set."
        ],
        "correct": "B. Use the representation transformation (normalizat ion) technique.",
        "explanation": "Please provide an explanation about the correct answer and explain why the other options are incorrect.",
        "references": ""
    },
    {
        "question": "Your data science team needs to rapidly experiment with various features, model architectures, and hyperparameters. They need to track the accuracy me trics for various experiments and use an API to que ry the metrics over time. What should they use to track an d report their experiments while minimizing manual effort?",
        "options": [
            "A. Use Kubeflow Pipelines to execute the experiments . Export the metrics file, and query the results us ing the",
            "B. Use AI Platform Training to execute the experimen ts. Write the accuracy metrics to BigQuery, and que ry",
            "C. Use AI Platform Training to execute the experimen ts. Write the accuracy metrics to Cloud Monitoring,  and",
            "D. Use AI Platform Notebooks to execute the experime nts. Collect the results in a shared Google Sheets file,"
        ],
        "correct": "A. Use Kubeflow Pipelines to execute the experiments . Export the metrics file, and query the results us ing the",
        "explanation": "Explanation:\n\nThe correct answer is B. Use the representation transformation (normalizat ion) technique. The reason for this is that when the dataset has columns with different ranges, the gradient optimization algorithm may have difficulty moving weights to a good solution. This is because the algorithm is sensitive to the scale of the features. By normalizing the features, we can ensure that all features are on the same scale, which can help the gradient optimization algorithm to converge to a good solution.\n\nOption A is incorrect because feature construction is a technique used to create new features from existing ones, but it does not address the issue of different ranges in the dataset.\n\nOption C is incorrect because removing features with missing values is a data cleaning step, but it does not address the issue of different ranges in the dataset.\n\nOption D is incorrect because changing the partitioning step to reduce the dimension of the test set and have a larger training set may affect the model's performance, but it does not address the issue of different ranges in the dataset.\n\nIn summary, normalizing the features using representation transformation is the correct solution to address the issue of different ranges in the dataset, which can help the gradient optimization algorithm to converge to a good solution.",
        "references": ""
    },
    {
        "question": "includes transactions, of which 1% are identified a s fraudulent. Which data transformation strategy wo uld likely improve the performance of your classifier?",
        "options": [
            "A. Write your data in TFRecords.",
            "B. Z-normalize all the numeric features.",
            "C. Oversample the fraudulent transaction 10 times.",
            "D. Use one-hot encoding on all categorical features."
        ],
        "correct": "C. Oversample the fraudulent transaction 10 times.",
        "explanation": "Explanation:\nThe correct answer is A. Use Kubeflow Pipelines to execute the experiments. Export the metrics file, and query the results using the API.\n\nKubeflow Pipelines is a cloud-native platform for machine learning (ML) that provides a simple, portable, and scalable way to deploy ML workflows. It allows data scientists to define, execute, and manage ML workflows, including data preparation, model training, and model deployment. Kubeflow Pipelines provides a built-in mechanism for tracking and reporting experiment metrics, including accuracy metrics, which can be exported to a file and queried using an API.\n\nOption B is incorrect because AI Platform Training is a managed service for training machine learning models, but it does not provide a built-in mechanism for tracking and reporting experiment metrics. Writing the accuracy metrics to BigQuery would require additional manual effort and would not provide a seamless way to query the results.\n\nOption C is incorrect because Cloud Monitoring is a monitoring and logging service that provides visibility into the performance and health of cloud resources, but it is not designed for tracking and reporting experiment metrics.\n\nOption D is incorrect because AI Platform Notebooks is a managed service for data science and machine learning development, but it does not provide a built-in mechanism for tracking and reporting experiment metrics. Collecting results in a shared Google Sheets file would require manual effort and would not provide a scalable and efficient way to query the results.\n\nTherefore, the correct answer is A. Use Kubeflow Pipelines to execute the experiments",
        "references": ""
    },
    {
        "question": "You are developing an ML model intended to classify  whether X-Ray images indicate bone fracture risk. You have trained on Api Resnet architecture on Vertex A I using a TPU A. accelerator, however you are unsat isfied with the trainning time and use memory usage. You w ant to quickly iterate your training code but make minimal changes to the code. You also want to minimize impa ct on the models accuracy. What should you do?",
        "options": [
            "A. Configure your model to use bfloat 16 instead flo at32",
            "B. Reduce the global batch size from 1024 to 256",
            "C. Reduce the number of layers in the model architec ture",
            "D. Reduce the dimensions of the images used un the m odel"
        ],
        "correct": "B. Reduce the global batch size from 1024 to 256",
        "explanation": "Explanation:\nThe correct answer is C. Oversample the fraudulent transaction 10 times. The reason is that the dataset is imbalanced, with only 1% of transactions being fraudulent. This imbalance can cause the classifier to be biased towards the majority class (non-fraudulent transactions) and perform poorly on the minority class (fraudulent transactions). Oversampling the minority class can help to alleviate this issue by increasing the number of fraudulent transactions in the dataset, making it more balanced. This can improve the performance of the classifier by giving it more examples of the minority class to learn from.\n\nOption A, writing data in TFRecords, is not directly related to addressing the imbalance issue and is more related to data storage and processing.\n\nOption B, Z-normalizing all numeric features, is a data preprocessing technique that can help with feature scaling, but it does not address the class imbalance issue.\n\nOption D, using one-hot encoding on all categorical features, is another data preprocessing technique that can help with feature representation, but it also does not address the class imbalance issue.\n\nTherefore, the correct answer is C, oversampling the fraudulent transactions 10 times, as it is the strategy that directly addresses the class imbalance issue and can improve the performance of the classifier.",
        "references": ""
    },
    {
        "question": "Your task is classify if a company logo is present on an image. You found out that 96% of a data does not include a logo. You are dealing with data imbalance  problem. Which metric do you use to evaluate to mo del?",
        "options": [
            "A. F1 Score",
            "B. RMSE",
            "C. F Score with higher precision weighting than reca ll",
            "D. F Score with higher recall weighted than precisio n"
        ],
        "correct": "",
        "explanation": "Explanation: \n\nThe correct answer is B. Reduce the global batch size from 1024 to 256. \n\nThe reason for this is that reducing the batch size reduces the memory usage, which can help speed up the training process. This is because the model is processing fewer samples at a time, which requires less memory. Additionally, reducing the batch size can also help with iterating the training code quickly. \n\nThe other options are incorrect because:\n\nA. Configuring the model to use bfloat16 instead of float32 may reduce memory usage, but it can also affect the model's accuracy. This is because bfloat16 has a lower precision than float32, which can lead to a loss of accuracy in the model. \n\nC. Reducing the number of layers in the model architecture can also affect the model's accuracy. This is because the model may not be able to capture the same level of complexity and nuance in the data with fewer layers. \n\nD. Reducing the dimensions of the images used in the model can also affect the model's accuracy. This is because the model may not be able to capture the same level of detail and information in the images with reduced dimensions.",
        "references": ""
    },
    {
        "question": "You need to train a regression model based on a dat aset containing 50,000 records that is stored in Bi gQuery. The data includes a total of20 categorical and nume rical features with a target variable that can incl ude negative values. You need to minimize effort and tr aining time while maximizing model performance. Wha t approach should you take to train this regression m odel? A. Create a custom TensorFlow DNN model.",
        "options": [
            "B. Use BQML XGBoost regression to train the model",
            "C. Use AutoML Tables to train the model without earl y stopping.",
            "D. Use AutoML Tables to train the model with RMSLE a s the optimization objective"
        ],
        "correct": "B. Use BQML XGBoost regression to train the model",
        "explanation": "Explanation:\nThe correct answer is B. Kubetlow Pipelines and AI Platform Prediction. \n\nKubetlow Pipelines is a platform that supports scheduled model retraining and Docker containers, which aligns with the requirements of the data science team. Additionally, AI Platform Prediction provides a service that supports autoscaling and monitoring for online prediction requests, which is also a requirement. \n\nOption A is incorrect because App Engine is a web framework and does not provide the necessary features for autoscaling and monitoring for online prediction requests. \n\nOption C is incorrect because Cloud Composer is a workflow orchestration service and BigQuery ML is a machine learning service that does not provide the necessary features for autoscaling and monitoring for online prediction requests. \n\nOption D is incorrect because AI Platform Training with custom containers provides a service for training machine learning models, but it does not provide the necessary features for autoscaling and monitoring for online prediction requests.",
        "references": ""
    },
    {
        "question": "Your data science team has requested a system that supports scheduled model retraining, Docker contain ers, and a service that supports autoscaling and monitor ing for online prediction requests. Which platform components should you choose for thi s system?",
        "options": [
            "A. Kubetlow Pipelines and App Engine",
            "B. Kubetlow Pipelines and AI Platform Prediction",
            "C. Cloud Composer, BigQuery ML , and AI Platform Pre diction",
            "D. Cloud Composer, AI Platform Training with custom containers , and App Engine"
        ],
        "correct": "B. Kubetlow Pipelines and AI Platform Prediction",
        "explanation": "Explanation:\n\nThe correct answer is actually D. Decrease the learning rate hyperparameter. \n\nWhen there is an oscillation in the loss during batch training of a neural network, it often indicates that the model is overshooting the optimal solution. This can happen when the learning rate is too high. \n\nIncreasing the learning rate (option C) would make the model update its parameters more aggressively, which would likely exacerbate the oscillation issue. \n\nIncreasing the size of the training batch (option A) may help stabilize the model, but it may not directly address the oscillation issue. \n\nDecreasing the size of the training batch (option B) could lead to more noisy gradients, which may not help with the oscillation issue either. \n\nDecreasing the learning rate (option D) would make the model update its parameters more gradually, which can help the model converge more smoothly and reduce the oscillation in the loss.",
        "references": ""
    },
    {
        "question": "You work for a global footwear retailer and need to  predict when an item will be out of stock based on  historical inventory data. Customer behavior is highly dynamic  since footwear demand is influenced by many differ ent factors. You want to serve models that are trained on all available data, but track your performance o n specific subsets of data before pushing to production. What is the most streamlined and reliable way to perfonn this validation?",
        "options": [
            "A. Use the TFX ModeiValidator tools to specify perfo rmance metrics for production readiness",
            "B. Use k-fold cross-validation as a validation strat egy to ensure that your model is ready for producti on.",
            "C. Use the last relevant week of data as a validatio n set to ensure that your model is performing accur ately on",
            "D. Use the entire dataset and treat the area under t he receiver operating characteristics curve (AUC RO C) as"
        ],
        "correct": "A. Use the TFX ModeiValidator tools to specify perfo rmance metrics for production readiness",
        "explanation": "Explanation:\n\nThe correct answer is B. Use BQML XGBoost regression to train the model.\n\nThe question requires training a regression model with a dataset containing 50,000 records stored in BigQuery. The dataset includes 20 categorical and numerical features with a target variable that can include negative values. To minimize effort and training time while maximizing model performance, using BQML XGBoost regression is the best approach.\n\nBQML XGBoost regression is a powerful and efficient algorithm for training regression models, especially with large datasets like this one. XGBoost is a popular gradient boosting algorithm that can handle both categorical and numerical features, making it suitable for this dataset. Additionally, XGBoost is known for its speed and scalability, which aligns with the requirement of minimizing effort and training time.\n\nNow, let's discuss why the other options are incorrect:\n\nA. Creating a custom TensorFlow DNN model would require significant effort and expertise in building and training a deep neural network. This approach would be time-consuming and may not be the most efficient way to train a regression model, especially when there are more suitable and efficient options available.\n\nC. Using AutoML Tables without early stopping may lead to overfitting, which would negatively impact model performance. AutoML Tables is a good option for automating the machine learning process, but it's essential to use early stopping to prevent overfitting and ensure the model generalizes well to new data.\n\nD. Using AutoML Tables with RMSLE",
        "references": ""
    },
    {
        "question": "During batch training of a neural network, you noti ce that there is an oscillation in the loss. How sh ould you adjust your model to ensure that it converges?",
        "options": [
            "A. Increase the size of the training batch B. Decrease the size of the training batch",
            "C. Increase the learning rate hyperparameter",
            "D. Decrease the learning rate hyperparameter"
        ],
        "correct": "C. Increase the learning rate hyperparameter",
        "explanation": "Explanation:\nThe correct answer is A. Use the TFX ModeiValidator tools to specify performance metrics for production readiness. TensorFlow Extended (TFX) is an end-to-end platform for machine learning (ML) that provides a suite of tools for building, validating, and deploying ML models. The TFX ModelValidator tool is specifically designed for model validation, allowing you to specify performance metrics and validation datasets to ensure that your model meets the required standards for production readiness.\n\nOption B, k-fold cross-validation, is a technique used to evaluate the performance of a model on unseen data, but it is not a streamlined way to validate models for production readiness. While k-fold cross-validation can provide an estimate of a model's performance, it does not provide a direct way to specify performance metrics for production readiness.\n\nOption C, using the last relevant week of data as a validation set, is not a reliable way to validate models. This approach is prone to overfitting, as the model may be biased towards the most recent data, and may not generalize well to future data.\n\nOption D, using the entire dataset and treating the area under the receiver operating characteristics curve (AUC-ROC) as a single metric, is not a reliable way to validate models. While AUC-ROC is a useful metric for evaluating model performance, it does not provide a comprehensive view of a model's performance on different subsets of data.\n\nIn summary, the TFX ModelValidator tool provides a streamlined and reliable way to validate",
        "references": ""
    },
    {
        "question": "You are building a linear model with over 100 input  features, all with values between -1 and I . You s uspect that many features are non-informative. You want to remo ve the non-informative features from your model whi le keeping the informative ones in their original form . Which technique should you use?",
        "options": [
            "A. Use Principal Component Analysis to eliminate the  least informative features.",
            "B. Use L l regularization to reduce the coefficients  of uninformative features to 0.",
            "C. After building your model, use Shapley values to determine which features are the most informative.",
            "D. Use an iterative dropout technique to identify wh ich features do not degrade the model when removed."
        ],
        "correct": "B. Use L l regularization to reduce the coefficients  of uninformative features to 0.",
        "explanation": "Explanation:\nThe correct answer is B. Use L1 regularization to reduce the coefficients of uninformative features to 0.\n\nL1 regularization, also known as Lasso regression, is a technique used in linear regression to reduce overfitting by adding a penalty term to the loss function. The penalty term is proportional to the absolute value of the model's coefficients. This encourages the model to set the coefficients of non-informative features to zero, effectively removing them from the model.\n\nOption A is incorrect because Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into new, orthogonal features called principal components. While PCA can help reduce the number of features, it does not remove non-informative features in their original form.\n\nOption C is incorrect because Shapley values are a technique used to explain the predictions of a machine learning model by assigning a value to each feature for a specific prediction. While Shapley values can help identify informative features, they do not remove non-informative features from the model.\n\nOption D is incorrect because iterative dropout is a technique used in neural networks to prevent overfitting by randomly dropping out neurons during training. It is not a technique used to remove non-informative features from a linear model.\n\nTherefore, the correct answer is B. Use L1 regularization to reduce the coefficients of uninformative features to 0.",
        "references": ""
    },
    {
        "question": "You are an ML engineer at a bank that has a mobile application. Management has asked you to build an M L- based biometric authentication for the app that ver ifies a customer's identity based on their fingerpr int. Fingerprints are considered highly sensitive person al information and cannot be downloaded and stored into the bank databases. Which learning strategy should you recommend to train and deploy this ML model?",
        "options": [
            "A. Differential privacy",
            "B. Federated learning",
            "C. MD 5 to encrypt data",
            "D. Data Loss Prevention API"
        ],
        "correct": "B. Federated learning",
        "explanation": "Explanation:\nThe correct answer is B. Federated learning. Federated learning is a machine learning approach that allows multiple parties to collaboratively train a shared model on their local data without sharing the data itself. This approach is particularly useful when dealing with sensitive data, such as fingerprints, that cannot be shared or downloaded. By using federated learning, the bank can train an ML model on the customer's mobile device, without collecting or storing the fingerprint data.\n\nOption A, Differential privacy, is a technique used to protect the privacy of individuals in a dataset by adding noise to the data. While it can be used to protect sensitive data, it is not the most suitable approach for this scenario, as it would not allow the bank to train an ML model on the customer's mobile device without collecting the data.\n\nOption C, MD5 to encrypt data, is a cryptographic hash function that can be used to encrypt data, but it is not a learning strategy and would not allow the bank to train an ML model on the customer's mobile device.\n\nOption D, Data Loss Prevention API, is a set of tools and technologies used to detect and prevent unauthorized access to sensitive data. While it can be used to protect sensitive data, it is not a learning strategy and would not allow the bank to train an ML model on the customer's mobile device.\n\nIn summary, federated learning is the correct answer because it allows the bank to train an ML model on the customer's mobile device without collecting or storing the sensitive",
        "references": ""
    },
    {
        "question": "You are building a linear regression model on BigQu ery ML to predict a customer's likelihood of purcha sing your company's products. Your model uses a city nam e variable as a key predictive component. In order to train and serve the model, your data must be organi zed in columns. You want to prepare your data using  the least amount of coding while maintaining the predic table variables. What should you do?",
        "options": [
            "A. Create a new view with BigQuery that does not inc lude a column with city information",
            "B. Use Dataprep to transform the state column using a one-hot encoding method, and make each city a column with binary values.",
            "C. Use Cloud Data Fusion to assign each city to a re gion labeled as 1, 2, 3, 4, or 5r and then use that  number",
            "D. Use Tensorflow to create a categorical variable w ith a vocabulary list Create the vocabulary file, a nd upload"
        ],
        "correct": "C. Use Cloud Data Fusion to assign each city to a re gion labeled as 1, 2, 3, 4, or 5r and then use that  number",
        "explanation": "Explanation: \nThe correct answer is C because it allows us to transform the city names into numerical values, which can be used in the linear regression model. By assigning each city to a region labeled as 1, 2, 3, 4, or 5, we are effectively creating a categorical variable that can be used in the model. This approach also requires the least amount of coding, as Cloud Data Fusion provides a graphical interface for data transformation.\n\nOption A is incorrect because creating a new view without the city information would remove a key predictive component from the model.\n\nOption B is incorrect because one-hot encoding would create a large number of columns, which could lead to the curse of dimensionality and make the model more complex. Additionally, one-hot encoding is not necessary in this case, as we can simply assign each city to a region.\n\nOption D is incorrect because creating a categorical variable using Tensorflow would require more coding and is not necessary in this case. Cloud Data Fusion provides a simpler and more efficient solution.",
        "references": ""
    },
    {
        "question": "You work for a toy manufacturer that has been exper iencing a large increase in demand. You need to bui ld an ML model to reduce the amount of time spent by qual ity control inspectors checking for product defects . Faster defect detection is a priority. The factory does no t have reliable Wi-Fi. Your company wants to implem ent the new ML model as soon as possible. Which model shoul d you use?",
        "options": [
            "A. AutoML Vision model",
            "B. AutoML Vision Edge mobile-versatile-! model",
            "C. AutoML Vision Edge mobile-low-latency-! model",
            "D. AutoML Vision Edge mobile-high-accuracy- I model"
        ],
        "correct": "A. AutoML Vision model",
        "explanation": "Explanation:\nThe correct answer is A. AutoML Vision model. The question states that the factory does not have reliable Wi-Fi,,assistant\n\nThe correct answer is indeed A. AutoML Vision model. Here's why:\n\nThe key constraint in this scenario is the lack of reliable Wi-Fi in the factory. This means that any model that requires a stable internet connection to function will not be suitable.\n\nAutoML Vision Edge models (options B, C, and D) are designed to run on edge devices, such as mobile phones or tablets, with limited or no internet connectivity. While this might seem like a good fit, these models are optimized for specific use cases:\n\n* Mobile-versatile-! (option B) is optimized for versatility and can adapt to different scenarios, but it may not provide the fastest defect detection.\n* Mobile-low-latency-! (option C) is optimized for low latency, but it may not provide the highest accuracy.\n* Mobile-high-accuracy-! (option D) is optimized for high accuracy, but it may not provide the fastest defect detection.\n\nIn contrast, the AutoML Vision model (option A) is a cloud-based model that can be trained and deployed quickly, which aligns with the company's priority of implementing the new ML model as soon as possible. Since the model will be deployed in the cloud, it can be accessed through a web interface or API, eliminating the need for reliable Wi-Fi in the factory.\n\nThe inspectors can simply upload",
        "references": ""
    },
    {
        "question": "You are going to train a DNN regression model with Keras APis using this code: How many trainable weights does your model have? (T he arithmetic below is correct.)",
        "options": [
            "A. 501 *256+257* 128+2 = 161154",
            "B. 500*256+256* 128+ 128*2 = 161024",
            "C. 501*256+257*128+128*2=161408",
            "D. 500*256*0 25+256* 128*0 25+ 128*2 = 40448"
        ],
        "correct": "C. 501*256+257*128+128*2=161408",
        "explanation": "Explanation:\nThe correct answer is C. 501*256+257*128+128*2=161408. \n\nThe model is a DNN regression model with one hidden layer. The input layer has 501 neurons, the hidden layer has 256 neurons, and the output layer has 128 neurons. \n\nThe number of trainable weights in the model can be calculated as follows: \n- The input layer to the hidden layer has 501*256 weights. \n- The hidden layer to the output layer has 257*128 weights (257 because the bias term is also counted). \n- The output layer has 128*2 weights (128 because of the output layer, and 2 because it's a regression model and the output is a single value). \n\nThe total number of trainable weights is the sum of these, which is 501*256+257*128+128*2=161408.\n\nOption A is incorrect because it doesn't count the bias term in the hidden layer. \n\nOption B is incorrect because it incorrectly calculates the number of weights between the hidden layer and the output layer. \n\nOption D is incorrect because it incorrectly calculates the number of weights by multiplying each layer's weights by 0.25.",
        "references": ""
    },
    {
        "question": "You recently designed and built a custom neural net work that uses critical dependencies specific to yo ur organization's framework. You need to train the mod el using a managed training service on Google Cloud . However, the ML framework and related dependencies are not supported by Al Platform Training. Also, bo th your model and your data are too large to fit in me mory on a single machine. Your ML framework of choi ce uses the scheduler, workers, and servers distributi on structure. What should you do? A. Use a built-in model available on AI Platform Tra ining",
        "options": [
            "B. Build your custom container to run jobs on AI Pla tform Training",
            "C. Build your custom containers to run distributed t raining jobs on Al Platform Training",
            "D. Reconfigure your code to a ML framework with depe ndencies that are supported by AI Platform Training"
        ],
        "correct": "C. Build your custom containers to run distributed t raining jobs on Al Platform Training",
        "explanation": "Explanation:\nThe correct answer is C. Build your custom containers to run distributed training jobs on AI Platform Training. \n\nThis option is correct because the model and data are too large to fit in memory on a single machine, and the ML framework uses a scheduler, workers, and servers distribution structure. This implies that the model requires distributed training. AI Platform Training supports custom containers, which allows you to package your model and dependencies into a container and run it on the platform. By building custom containers, you can ensure that your model and dependencies are properly installed and configured, and that the distributed training job can be executed successfully.\n\nOption A is incorrect because the question states that the ML framework and related dependencies are not supported by AI Platform Training. Using a built-in model available on AI Platform Training would not work because it would not be compatible with the custom framework and dependencies.\n\nOption B is incorrect because building a custom container to run jobs on AI Platform Training would not allow for distributed training. The model and data are too large to fit in memory on a single machine, so a single container would not be sufficient.\n\nOption D is incorrect because reconfiguring the code to a ML framework with dependencies that are supported by AI Platform Training would require significant changes to the model and framework, and may not be feasible or desirable. Building custom containers is a more straightforward and flexible solution.",
        "references": ""
    },
    {
        "question": "You are an ML engineer in the contact center of a l arge enterprise. You need to build a sentiment anal ysis tool that predicts customer sentiment from recorded phon e conversations. You need to identify the best appr oach to building a model while ensuring that the gender, ag e, and cultural differences of the customers who ca lled the contact center do not impact any stage of the model  development pipeline and results. What should you do?",
        "options": [
            "A. Extract sentiment directly from the voice recordi ngs",
            "B. Convert the speech to text and build a model base d on the words",
            "C. Convert the speech to text and extract sentiments  based on the sentences",
            "D. Convert the speech to text and extract sentiment using syntactical analysis"
        ],
        "correct": "C. Convert the speech to text and extract sentiments  based on the sentences",
        "explanation": "Explanation:\nThe correct answer is C. Categorical cross-entropy. This is because the problem is a multi-class classification problem, where the model needs to predict one of the three classes: driver's license, passport, or credit card. Categorical cross-entropy is the most suitable loss function for this type of problem.\n\nOption A, Categorical hinge, is not suitable for this problem because it is typically used in Support Vector Machines (SVMs) and is not designed for multi-class classification problems.\n\nOption B, Binary cross-entropy, is not suitable because it is designed for binary classification problems, where the model needs to predict one of two classes. In this problem, there are three classes, so binary cross-entropy is not applicable.\n\nOption D, Sparse categorical cross-entropy, is similar to categorical cross-entropy but is used when the labels are sparse, meaning that most of the labels are zero. In this problem, the labels are not sparse, so sparse categorical cross-entropy is not necessary.\n\nTherefore, the correct answer is C. Categorical cross-entropy.",
        "references": ""
    },
    {
        "question": "Your team needs to build a model that predicts whet her images contain a driver's license, passport, or  credit card. The data engineering team already built the p ipeline and generated a dataset composed of 10,000 images with driver's licenses, 1,000 images with pa ssports, and 1,000 images with credit cards. You no w have to train a model with the following label map: ['driverslicense', passport', 'credit_ card']. Whic h loss function should you use?",
        "options": [
            "A. Categorical hinge",
            "B. Binary cross-entropy",
            "C. Categorical cross-entropy",
            "D. Sparse categorical cross-entropy"
        ],
        "correct": "C. Categorical cross-entropy",
        "explanation": "Explanation:\n\nThe correct answer is C. Convert the speech to text and extract sentiments based on the sentences. This approach is the most suitable because it takes into account the context and nuances of the spoken language, which is essential for accurate sentiment analysis.\n\nOption A, extracting sentiment directly from the voice recordings, is incorrect because voice recordings contain various acoustic features that may not be directly related to sentiment. For example, tone, pitch, and volume can vary greatly between individuals, and it would be challenging to develop a model that accurately captures sentiment solely from these features.\n\nOption B, converting speech to text and building a model based on words, is also incorrect. While words are an essential component of spoken language, relying solely on individual words may not capture the context and relationships between words, which are crucial for sentiment analysis.\n\nOption D, converting speech to text and extracting sentiment using syntactical analysis, is incorrect because syntactical analysis focuses on the grammatical structure of sentences, which may not be directly related to sentiment. Sentiment is often conveyed through subtle cues, such as tone, irony, and figurative language, which may not be captured by syntactical analysis alone.\n\nIn contrast, option C, converting speech to text and extracting sentiments based on sentences, takes into account the context and relationships between words, allowing for a more accurate and nuanced sentiment analysis. This approach can help mitigate the impact of gender, age, and cultural differences on the model's performance, as it focuses on the meaning and context of the",
        "references": ""
    },
    {
        "question": "different cities around the world. Which features o r feature crosses should you use to train city-spec ific relationships between car type and number of sales?",
        "options": [
            "A. Three individual features binned latitude, binned  longitude, and one-hot encoded car type",
            "B. One feature obtained A. element-wise product betw een latitude, longitude, and car type",
            "C. One feature obtained A. element-wise product betw een binned latitude, binned longitude, and one-hot",
            "D. Two feature crosses as a element-wise product the  first between binned latitude and one-hot encoded car"
        ],
        "correct": "C. One feature obtained A. element-wise product betw een binned latitude, binned longitude, and one-hot",
        "explanation": "Explanation:\nThe correct answer is C. One feature obtained as an element-wise product between binned latitude, binned longitude, and one-hot encoded car type.\n\nHere's why:\n\n* Option A is incorrect because it uses individual features that don't capture the interaction between latitude, longitude, and car type. We need to capture the relationships between these features.\n* Option B is incorrect because it uses the raw latitude and longitude values, which can lead to poor performance due to the high dimensionality and non-linear relationships between these features and car type.\n* Option D is incorrect because it only captures the interaction between binned latitude and car type, and binned longitude and car type separately. We need to capture the interaction between all three features simultaneously.\n\nOption C is correct because it uses the element-wise product between binned latitude, binned longitude, and one-hot encoded car type. This allows the model to capture the complex relationships between these features and learn city-specific patterns. The binning of latitude and longitude helps to reduce dimensionality and non-linearity, while the one-hot encoding of car type allows the model to capture categorical relationships. The element-wise product combines these features in a way that allows the model to learn interactions between them.\n\nBy using this feature, the model can learn to predict the number of sales based on the specific city (captured by binned latitude and longitude) and car type.",
        "references": ""
    }
]