{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from dotenv import load_dotenv\n",
    "from os import environ, getenv\n",
    "from getpass import getpass\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to set environment variables\n",
    "def set_env(var: str):\n",
    "    env_var = getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass(f\"{var}: \")\n",
    "        environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "# Define IBM connection parameters\n",
    "class IbmConnectionParams(BaseModel):\n",
    "    api_key: str\n",
    "    project_id: str\n",
    "    url: str\n",
    "    credentials: dict[str, str]\n",
    "\n",
    "    def __init__(self, api_key: str, project_id: str, url: str) -> None:\n",
    "        super().__init__(api_key=api_key, project_id=project_id, url=url, credentials={\"url\": url, \"apikey\": api_key})\n",
    "\n",
    "# Load IBM connection parameters from environment variables\n",
    "def load_connection_params() -> IbmConnectionParams:\n",
    "    api_key = set_env(\"WATSONX_API_KEY\")\n",
    "    project_id = set_env(\"PROJECT_ID\")\n",
    "    url = set_env(\"WATSONX_URL\")\n",
    "\n",
    "    return IbmConnectionParams(api_key=api_key, project_id=project_id, url=url)\n",
    "\n",
    "connection_params: IbmConnectionParams = load_connection_params()\n",
    "\n",
    "# Define parameters for the model\n",
    "parameters = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "}\n",
    "\n",
    "# Initialize the WatsonxLLM model\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"meta-llama/llama-3-70b-instruct\",\n",
    "    apikey=connection_params.api_key,\n",
    "    url=connection_params.url,\n",
    "    project_id=connection_params.project_id,\n",
    "    params=parameters,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an expert in Cloud technologies. Your task is to provide an explanation about the \"\n",
    "    \"correct answer and explain why the other options are incorrect. \"\n",
    ")\n",
    "\n",
    "# Initialize the prompt template\n",
    "prompt_template = PromptTemplate(input_variables=[], template=system_prompt)\n",
    "\n",
    "# Combine the system prompt with the user's prompt\n",
    "def create_full_prompt(user_prompt: str) -> str:\n",
    "    return f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "# Function to interact with the WatsonxLLM model\n",
    "def qa(prompt):\n",
    "    full_prompt = create_full_prompt(prompt)\n",
    "    response = watsonx_llm.invoke(full_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question_data, exp=False):\n",
    "    \"\"\"Constructs a string for LLM prompt and gets an explanation.\"\"\"\n",
    "    if exp:\n",
    "       system = '''You are an expert in the Cloud. Your task is to give an explanation about the\n",
    "    correct answer and explain why the other options are wrong. The question is the following:'''\n",
    "    else: \n",
    "        system = '''You are an expert in the Cloud. Your task is to give an explanation about the\n",
    "    correct answer and explain why the other options are wrong.\n",
    "    You will recieve additional explantion that maybe can help otherwise give your explanation. \n",
    "    The question and explanation are the following:'''\n",
    "\n",
    "    context = f\"Question: {question_data['question']}\\n\"\n",
    "    for i, option in enumerate(question_data['options']):\n",
    "        context += f\"{i+1}. {option}\\n\"\n",
    "    context += f\"Correct Answer: {question_data['correct']}\\n\\n\"\n",
    "    prompt = system + context\n",
    "    print(\"prompt:\",prompt)\n",
    "    explanation = qa(prompt)\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'AWS allows users to manage their resources using a web based user interface. What is the name of this interface?', 'options': ['AWS CLI.', 'AWS API.', 'AWS SDK.', 'AWS Management Console.'], 'correct': 'AWS Management Console.'}\n",
      "prompt: You are an expert in the Cloud. Your task is to give an explanation about the\n",
      "    correct answer and explain why the other options are wrong.\n",
      "    You will recieve additional explantion that maybe can help otherwise give your explanation. \n",
      "    The question and explanation are the following:Question: AWS allows users to manage their resources using a web based user interface. What is the name of this interface?\n",
      "1. AWS CLI.\n",
      "2. AWS API.\n",
      "3. AWS SDK.\n",
      "4. AWS Management Console.\n",
      "Correct Answer: AWS Management Console.\n",
      "\n",
      "\n",
      "explanation: Explanation: AWS provides a web-based user interface called the AWS Management Console to manage AWS resources. The AWS Management Console provides a unified view of all the AWS services and enables users to easily navigate, of all the AWS resources. It provides a centralized location to access and manage all the AWS resources.\n",
      "\n",
      "Now, let's discuss why the other options are incorrect.\n",
      "\n",
      "1. AWS CLI: AWS CLI is a command-line tool that allows users to interact with AWS services using commands in the command-line shell. It is not a web-based user interface.\n",
      "\n",
      "2. AWS API: AWS API is an application programming interface that allows developers to access AWS services programmatically. It is not a user interface.\n",
      "\n",
      "3. AWS SDK: AWS SDK is a software development kit that provides a set of libraries and tools for developers to build applications that interact with AWS services. It is not a user interface.\n",
      "\n",
      "In summary, the correct answer is AWS Management Console because it is a web-based user interface that allows users to manage AWS resources.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "file_path = r'c:\\Blog\\AWS-Exam-Simulator\\questions\\CLF-C02-v1.json'  # Raw string\n",
    "with open(file_path, 'r') as file:\n",
    "    questions = json.load(file)\n",
    "    len(questions)\n",
    "for question in questions[:1]:\n",
    "    print(question)\n",
    "    # Check if the question already has an explanation\n",
    "    #if \"explanation\" not in question:\n",
    "    explanation = process_question(question)\n",
    "    print(\"explanation:\",explanation)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_file(file_path):\n",
    "    \"\"\"Reads, processes, and updates the JSON file with explanations.\"\"\"  \n",
    "    with open(file_path, 'r') as file:\n",
    "        questions = json.load(file)\n",
    "    for question in questions[:1]:\n",
    "        # Check if the question already has an explanation\n",
    "        if \"explanation\" not in question:\n",
    "            explanation = process_question(question,exp=False)\n",
    "            print(explanation)\n",
    "            question[\"explanation\"] = explanation\n",
    "        else:\n",
    "            old_explanation=question[\"explanation\"]\n",
    "            prompt= question + \" Explanation \" + str(old_explanation)\n",
    "            explanation = process_question(prompt, exp=True)\n",
    "            print(explanation)\n",
    "            question[\"explanation\"] = explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def update_json_file(file_path):\n",
    "    \"\"\"Reads, processes, and updates the JSON file with explanations.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        questions = json.load(file)\n",
    "\n",
    "    for question in questions[:1]:\n",
    "        # Check if the question already has an explanation\n",
    "        if \"explanation\" not in question:\n",
    "            explanation = process_question(question, exp=False)\n",
    "            print(f\"New Explanation: {explanation}\")\n",
    "        else:\n",
    "            old_explanation = question[\"explanation\"]\n",
    "            explanation = process_question(question, exp=True)\n",
    "            print(\"Old Explanation:\", old_explanation)\n",
    "            print(\"New Explanation:\", explanation)\n",
    "\n",
    "        # Update the question's explanation in both cases\n",
    "        question[\"explanation\"] = explanation\n",
    "\n",
    "    # Write the updated questions back to the JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(questions, file, indent=4) \n",
    "        # Format with indentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: You are an expert in the Cloud. Your task is to give an explanation about the\n",
      "    correct answer and explain why the other options are wrong. The question is the following:Question: You are building an ML model to detect anomalies in  real-time sensor data. You will use Pub/Sub to han dle incoming requests. You want to store the results fo r analytics and visualization. How should you confi gure the pipeline?\n",
      "1. A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery\n",
      "2. B. 1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable\n",
      "3. C. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions\n",
      "4. D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage\n",
      "Correct Answer: A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery\n",
      "\n",
      "\n",
      "Old Explanation: Explanation/Reference: https://cloud.google.com/solutions/building-anomaly -detection-dataflow-bigqueryml-dlp\n",
      "New Explanation: Explanation:\n",
      "The correct answer is A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery. Here's why:\n",
      "\n",
      "1. Dataflow is used for real-time data processing, which is perfect for handling incoming sensor data and detecting anomalies in real-time. It's a fully-managed service for transforming and processing data in stream and batch modes.\n",
      "\n",
      "2. AI Platform is used for building, deploying, and managing machine learning models. It provides a managed platform for the machine learning lifecycle, from data preparation to model deployment. It's the ideal choice for building and deploying the ML model.\n",
      "\n",
      "3. BigQuery is a fully-managed enterprise data warehouse that allows you to easily analyze all your data using SQL-like queries. It's the perfect choice for storing the results of the anomaly detection for analytics and visualization.\n",
      "\n",
      "Now, let's discuss why the other options are incorrect:\n",
      "\n",
      "Option B is incorrect because DataProc is a fully-managed service for running Apache Spark and Hadoop workloads, which is not suitable for real-time data processing. AutoML is a suite of machine learning tools that allows developers with limited machine learning expertise to train high-quality models, but it's not the best choice for building and deploying the ML model in this scenario. Cloud Bigtable is a fully-managed NoSQL database service that's designed for large-scale analytical and operational workloads, but it's not the best choice for storing the results of the anomaly detection for analytics and visualization.\n",
      "\n",
      "Option C is incorrect because BigQuery is not suitable for real-time data processing, and AutoML is not the best choice for building and deploying the ML model. Cloud Functions is a serverless compute service that allows you to run small code snippets in response to events, but it's not suitable for building and deploying the ML model.\n",
      "\n",
      "Option D is incorrect because BigQuery is not suitable for real-time data processing, and AI Platform is not suitable for storing the results of the anomaly detection for analytics and visualization. Cloud Storage\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "#file_path = r'c:\\Blog\\AWS-Exam-Simulator\\questions\\CLF-C02-v1.json'  # Raw string\n",
    "file_path = r'c:\\Blog\\AWS-Exam-Simulator\\questions\\GCP-ML-vA.json'  # Raw string\n",
    "update_json_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files: ['c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-102.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v1.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v2.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v3.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\CLF-C02-v1.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\DOP-C02-v1.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\DP-100-v1.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-CA.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-ML-vA.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-ML-vB.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v0624.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v1.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v2.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v3.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v4.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAA-C03-v1.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAA-C03-v2.json', 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAP-C02-v1.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "json_file_paths=[]\n",
    "current_dir = os.getcwd()\n",
    "directory = os.path.join(current_dir,\"questions\")\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            #pdf_file_paths.append(file)  # remove the .json extension\n",
    "            json_file_paths.append(os.path.join(root, file))  # include the full path\n",
    "\n",
    "print(\"JSON files:\", json_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-102.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v1.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v2.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v3.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\CLF-C02-v1.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\DOP-C02-v1.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\DP-100-v1.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-CA.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-ML-vA.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-ML-vB.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v0624.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v1.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v2.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v3.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v4.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAA-C03-v1.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAA-C03-v2.json',\n",
       " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAP-C02-v1.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_paths=[\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-ML-vA.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-ML-vB.json',  \n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAA-C03-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAA-C03-v2.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAP-C02-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v0624.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v4.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_paths=['c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-102.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v2.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\AI-900-v3.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\CLF-C02-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\DOP-C02-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\DP-100-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-CA.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\GCP-ML-vB.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v2.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v3.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01-v4.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\MLS-C01.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAA-C03-v1.json',\n",
    " 'c:\\\\Blog\\\\AWS-Exam-Simulator\\\\questions\\\\SAP-C02-v1.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# --- Main Execution ---\n",
    "for file_path in json_file_paths[:2]:\n",
    "    #file_path = r'c:\\Blog\\AWS-Exam-Simulator\\questions\\CLF-C02-v1.json'  # Raw string\n",
    "    #file_path = r'c:\\Blog\\AWS-Exam-Simulator\\questions\\GCP-ML-vA.json'  # Raw string  \n",
    "    file_name = os.path.basename(file_path) # Extract file name with extension\n",
    "    print(\"Working with:\", file_name) \n",
    "    update_json_file(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
