[
    {
        "question": "A Machine Learning Specialist is working with multiple data sources containing billions of\nrecords that need to be joined. What feature engineering and model development approach should\nthe Specialist take with a dataset this large?",
        "options": [
            "A. Use an Amazon SageMaker notebook for both feature engineering and model development",
            "B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model",
            "C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development",
            "D. Use Amazon ML for both feature engineering and model development."
        ],
        "correct": "C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development",
        "explanation": "Amazon EMR is a service that can process large amounts of data efficiently and cost-effectively. It can\nrun distributed frameworks such as Apache Spark, which can perform feature engineering on big\ndata. Amazon SageMaker SDK is a Python library that can interact with Amazon SageMaker service to\ntrain and deploy machine learning models. It can also use Amazon EMR as a data source for training\ndata.",
        "references": "Amazon EMR\nAmazon SageMaker SDK"
    },
    {
        "question": "A Machine Learning Specialist is building a supervised model that will evaluate customers'\nsatisfaction with their mobile phone service based on recent usage The model's output should infer\nwhether or not a customer is likely to switch to a competitor in the next 30 days Which of the\nfollowing modeling techniques should the Specialist use1?",
        "options": [
            "A. Time-series prediction",
            "B. Anomaly detection",
            "C. Binary classification",
            "D. Regression"
        ],
        "correct": "C. Binary classification",
        "explanation": "The modeling technique that the Machine Learning Specialist should use is binary classification.\nBinary classification is a type of supervised learning that predicts whether an input belongs to one of\ntwo possible classes. In this case, the input is the customer's recent usage data and the output is\nwhether or not the customer is likely to switch to a competitor in the next 30 days. This is a binary\noutcome, either yes or no, so binary classification is suitable for this problem. The other options are\nnot appropriate for this problem. Time-series prediction is a type of supervised learning that\nforecasts future values based on past and present data. Anomaly detection is a type of unsupervised\nlearning that identifies outliers or abnormal patterns in the data. Regression is a type of supervised\nlearning that estimates a continuous numerical value based on the input features.",
        "references": "Binary\nClassification, Time Series Prediction, Anomaly Detection, Regression"
    },
    {
        "question": "A Machine Learning Specialist is assigned a TensorFlow project using Amazon SageMaker for\ntraining, and needs to continue working for an extended period with no Wi-Fi access.\nWhich approach should the Specialist use to continue working?",
        "options": [
            "A. Install Python 3 and boto3 on their laptop and continue the code development using that",
            "B. Download the TensorFlow Docker container used in Amazon SageMaker from GitHub to their localIT Certification Guaranteed, The Easy Way!",
            "C. Download TensorFlow from tensorflow.org to emulate the TensorFlow kernel in the SageMaker",
            "D. Download the SageMaker notebook to their local environment then install Jupyter Notebooks on"
        ],
        "correct": "B. Download the TensorFlow Docker container used in Amazon SageMaker from GitHub to their localIT Certification Guaranteed, The Easy Way!",
        "explanation": "Amazon SageMaker is a fully managed service that enables developers and data scientists to quickly\nand easily build, train, and deploy machine learning models at any scale. SageMaker provides a\nvariety of tools and frameworks to support the entire machine learning workflow, from data\npreparation to model deployment.\nOne of the tools that SageMaker offers is the Amazon SageMaker Python SDK, which is a high-level\nlibrary that simplifies the interaction with SageMaker APIs and services. The SageMaker Python SDK\nallows you to write code in Python and use popular frameworks such as TensorFlow, PyTorch,\nMXNet, and more. You can use the SageMaker Python SDK to create and manage SageMaker\nresources such as notebook instances, training jobs, endpoints, and feature store.\nIf you need to continue working on a TensorFlow project using SageMaker for training without Wi-Fi\naccess, the best approach is to download the TensorFlow Docker container used in SageMaker from\nGitHub to your local environment, and use the SageMaker Python SDK to test the code. This way, you\ncan ensure that your code is compatible with the SageMaker environment and avoid any potential\nissues when you upload your code to SageMaker and start the training job. You can also use the same\ncode to deploy your model to a SageMaker endpoint when you have Wi-Fi access again.\nTo download the TensorFlow Docker container used in SageMaker, you can visit the SageMaker\nDocker GitHub repository and follow the instructions to build the image locally. You can also use the\nSageMaker Studio Image Build CLI to automate the process of building and pushing the Docker image\nto Amazon Elastic Container Registry (Amazon ECR). To use the SageMaker Python SDK to test the\ncode, you can install the SDK on your local machine by following the installation guide. You can also\nrefer to the TensorFlow documentation for more details on how to use the SageMaker Python SDK\nwith TensorFlow.",
        "references": "SageMaker Docker GitHub repository\nSageMaker Studio Image Build CLI\nSageMaker Python SDK installation guide\nSageMaker Python SDK TensorFlow documentation"
    },
    {
        "question": "A Data Scientist wants to gain real-time insights into a data stream of GZIP files. Which solution\nwould allow the use of SQL to query the stream with the LEAST latency?",
        "options": [
            "A. Amazon Kinesis Data Analytics with an AWS Lambda function to transform the data.",
            "B. AWS Glue with a custom ETL script to transform the data.",
            "C. An Amazon Kinesis Client Library to transform the data and save it to an Amazon ES cluster.",
            "D. Amazon Kinesis Data Firehose to transform the data and put it into an Amazon S3 bucket."
        ],
        "correct": "A. Amazon Kinesis Data Analytics with an AWS Lambda function to transform the data.",
        "explanation": "Amazon Kinesis Data Analytics is a service that enables you to analyze streaming data in real time\nusing SQL or Apache Flink applications. You can use Kinesis Data Analytics to process and gain insightsIT Certification Guaranteed, The Easy Way!\n3 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 2\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlfrom data streams such as web logs, clickstreams, IoT data, and more.\nTo use SQL to query a data stream of GZIP files, you need to first transform the data into a format\nthat Kinesis Data Analytics can understand, such as JSON, CSV, or Apache Parquet. You can use an\nAWS Lambda function to perform this transformation and send the output to a Kinesis data stream\nthat is connected to your Kinesis Data Analytics application. This way, you can use SQL to query the\nstream with the least latency, as Lambda functions are triggered in near real time by the incoming\ndata and Kinesis Data Analytics can process the data as soon as it arrives.\nThe other options are not optimal for this scenario, as they introduce more latency or complexity.\nAWS Glue is a serverless data integration service that can perform ETL (extract, transform, and load)\ntasks on data sources, but it is not designed for real-time streaming data analysis. An Amazon Kinesis\nClient Library is a Java library that enables you to build custom applications that process data from\nKinesis data streams, but it requires more coding and configuration than using a Lambda function.\nAmazon Kinesis Data Firehose is a service that can deliver streaming data to destinations such as\nAmazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, but it does not support SQL\nqueries on the data.",
        "references": "What Is Amazon Kinesis Data Analytics for SQL Applications?\nUsing AWS Lambda with Amazon Kinesis Data Streams\nUsing AWS Lambda with Amazon Kinesis Data Firehose"
    },
    {
        "question": "A Machine Learning Specialist must build out a process to query a dataset on Amazon S3 using\nAmazon Athena The dataset contains more than 800.000 records stored as plaintext CSV files Each\nrecord contains 200 columns and is approximately 1 5 MB in size Most queries will span 5 to 10\ncolumns only How should the Machine Learning Specialist transform the dataset to minimize query\nruntime?",
        "options": [
            "A. Convert the records to Apache Parquet format",
            "B. Convert the records to JSON format",
            "C. Convert the records to GZIP CSV format",
            "D. Convert the records to XML format",
            "C. Columnar formats store data by columns"
        ],
        "correct": "A. Convert the records to Apache Parquet format",
        "explanation": "To optimize the query performance of Athena, one of the best practices is to convert the data into a\ncolumnar format, such as Apache Parquet or Apache ORC. Columnar formats store data by columns\nrather than by rows, which allows Athena to scan only the columns that are relevant to the query,\nreducing the amount of data read and improving the query speed. Columnar formats also support\ncompression and encoding schemes that can reduce the storage space and the data scanned per\nquery, further enhancing the performance and reducing the cost.\nIn contrast, plaintext CSV files store data by rows, which means that Athena has to scan the entire\nrow even if only a few columns are needed for the query. This increases the amount of data read and\nthe query latency. Moreover, plaintext CSV files do not support compression or encoding, which\nmeans that they take up more storage space and incur higher query costs.\nTherefore, the Machine Learning Specialist should transform the dataset to Apache Parquet format\nto minimize query runtime.",
        "references": "Top 10 Performance Tuning Tips for Amazon Athena\nColumnar Storage FormatsIT Certification Guaranteed, The Easy Way!\n4 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 3\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlUsing compressions will reduce the amount of data scanned by Amazon Athena, and also reduce your\nS3 bucket storage. It's a Win-Win for your AWS bill. Supported formats: GZIP, LZO, SNAPPY (Parquet)\nand ZLIB."
    },
    {
        "question": "An agency collects census information within a country to determine healthcare and social\nprogram needs by province and city. The census form collects responses for approximately 500\nquestions from each citizen Which combination of algorithms would provide the appropriate\ninsights? (Select TWO )",
        "options": [
            "A. The factorization machines (FM) algorithm",
            "B. The Latent Dirichlet Allocation (LDA) algorithm",
            "C. The principal component analysis (PCA) algorithm",
            "D. The k-means algorithm"
        ],
        "correct": "C. The principal component analysis (PCA) algorithm",
        "explanation": "The agency wants to analyze the census data for population segmentation, which is a type of\nunsupervised learning problem that aims to group similar data points together based on their\nattributes. The agency can use a combination of algorithms that can perform dimensionality\nreduction and clustering on the data to achieve this goal.\nDimensionality reduction is a technique that reduces the number of features or variables in a dataset\nwhile preserving the essential information and relationships. Dimensionality reduction can help\nimprove the efficiency and performance of clustering algorithms, as well as facilitate data\nvisualization and interpretation. One of the most common algorithms for dimensionality reduction is\nprincipal component analysis (PCA), which transforms the original features into a new set of\northogonal features called principal components that capture the maximum variance in the data. PCA\ncan help reduce the noise and redundancy in the data and reveal the underlying structure and\npatterns.\nClustering is a technique that partitions the data into groups or clusters based on their similarity or\ndistance. Clustering can help discover the natural segments or categories in the data and understand\ntheir characteristics and differences. One of the most popular algorithms for clustering is k-means,\nwhich assigns each data point to one of k clusters based on the nearest mean or centroid. K-means\ncan handle large and high-dimensional datasets and produce compact and spherical clusters.\nTherefore, the combination of algorithms that would provide the appropriate insights for population\nsegmentation are PCA and k-means. The agency can use PCA to reduce the dimensionality of the\ncensus data from 500 features to a smaller number of principal components that capture most of theIT Certification Guaranteed, The Easy Way!\n6 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 5\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlvariation in the data. Then, the agency can use k-means to cluster the data based on the principal\ncomponents and identify the segments of the population that share similar characteristics.",
        "references": "Amazon SageMaker Principal Component Analysis (PCA)\nAmazon SageMaker K-Means Algorithm"
    },
    {
        "question": "A large consumer goods manufacturer has the following products on sale\n* 34 different toothpaste variants\n* 48 different toothbrush variants\n* 43 different mouthwash variants\nThe entire sales history of all these products is available in Amazon S3 Currently, the company is\nusing custom-built autoregressive integrated moving average (ARIMA) models to forecast demand for\nthese products The company wants to predict the demand for a new product that will soon be\nlaunched Which solution should a Machine Learning Specialist apply?",
        "options": [
            "A. Train a custom ARIMA model to forecast demand for the new product.",
            "B. Train an Amazon SageMaker DeepAR algorithm to forecast demand for the new product",
            "C. Train an Amazon SageMaker k-means clustering algorithm to forecast demand for the new",
            "D. Train a custom XGBoost model to forecast demand for the new product"
        ],
        "correct": "B. Train an Amazon SageMaker DeepAR algorithm to forecast demand for the new product",
        "explanation": "The company wants to predict the demand for a new product that will soon be launched, based on\nthe sales history of similar products. This is a time series forecasting problem, which requires a\nmachine learning algorithm that can learn from historical data and generate future predictions.\nOne of the most suitable solutions for this problem is to use the Amazon SageMaker DeepAR\nalgorithm, which is a supervised learning algorithm for forecasting scalar time series using recurrent\nneural networks (RNN). DeepAR can handle multiple related time series, such as the sales of different\nproducts, and learn a global model that captures the common patterns and trends across the time\nseries. DeepAR can also generate probabilistic forecasts that provide confidence intervals and\nquantify the uncertainty of the predictions.\nDeepAR can outperform traditional forecasting methods, such as ARIMA, especially when the dataset\ncontains hundreds or thousands of related time series. DeepAR can also use the trained model to\nforecast the demand for new products that are similar to the ones it has been trained on, by using\nthe categorical features that encode the product attributes. For example, the company can use the\nproduct type, brand, flavor, size, and price as categorical features to group the products and learn the\ntypical behavior for each group.\nTherefore, the Machine Learning Specialist should apply the Amazon SageMaker DeepAR algorithm\nto forecast the demand for the new product, by using the sales history of the existing products as the\ntraining dataset, and the product attributes as the categorical features.",
        "references": "DeepAR Forecasting Algorithm - Amazon SageMaker\nNow available in Amazon SageMaker: DeepAR algorithm for more accurate time series forecasting"
    },
    {
        "question": "A Data Scientist needs to migrate an existing on-premises ETL process to the cloud The current\nprocess runs at regular time intervals and uses PySpark to combine and format multiple large data\nsources into a single consolidated output for downstream processing The Data Scientist has beenIT Certification Guaranteed, The Easy Way!\n7 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 6\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlgiven the following requirements for the cloud solution\n* Combine multiple data sources\n* Reuse existing PySpark logic\n* Run the solution on the existing schedule\n* Minimize the number of servers that will need to be managed\nWhich architecture should the Data Scientist use to build this solution?",
        "options": [
            "A. Write the raw data to Amazon S3 Schedule an AWS Lambda function to submit a Spark step to a",
            "B. Write the raw data to Amazon S3 Create an AWS Glue ETL job to perform the ETL processing",
            "C. Write the raw data to Amazon S3 Schedule an AWS Lambda function to run on the existing",
            "D. Use Amazon Kinesis Data Analytics to stream the input data and perform realtime SQL queries"
        ],
        "correct": "B. Write the raw data to Amazon S3 Create an AWS Glue ETL job to perform the ETL processing",
        "explanation": "The Data Scientist needs to migrate an existing on-premises ETL process to the cloud, using a solution\nthat can combine multiple data sources, reuse existing PySpark logic, run on the existing schedule,\nand minimize the number of servers that need to be managed. The best architecture for this scenario\nis to use AWS Glue, which is a serverless data integration service that can create and run ETL jobs on\nAWS.\nAWS Glue can perform the following tasks to meet the requirements:\nCombine multiple data sources: AWS Glue can access data from various sources, such as Amazon S3,\nAmazon RDS, Amazon Redshift, Amazon DynamoDB, and more. AWS Glue can also crawl the data\nsources and discover their schemas, formats, and partitions, and store them in the AWS Glue Data\nCatalog, which is a centralized metadata repository for all the data assets.\nReuse existing PySpark logic: AWS Glue supports writing ETL scripts in Python or Scala, using Apache\nSpark as the underlying execution engine. AWS Glue provides a library of built-in transformations and\nconnectors that can simplify the ETL code. The Data Scientist can write the ETL job in PySpark and\nleverage the existing logic to perform the data processing.\nRun the solution on the existing schedule: AWS Glue can create triggers that can start ETL jobs based\non a schedule, an event, or a condition. The Data Scientist can create a new AWS Glue trigger to run\nthe ETL job based on the existing schedule, using a cron expression or a relative time interval.\nMinimize the number of servers that need to be managed: AWS Glue is a serverless service, which\nmeans that it automatically provisions, configures, scales, and manages the compute resources\nrequired to run the ETL jobs. The Data Scientist does not need to worry about setting up, maintaining,\nor monitoring any servers or clusters for the ETL process.\nTherefore, the Data Scientist should use the following architecture to build the cloud solution:IT Certification Guaranteed, The Easy Way!\n8 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 7\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlWrite the raw data to Amazon S3: The Data Scientist can use any method to upload the raw data\nfrom the on-premises sources to Amazon S3, such as AWS DataSync, AWS Storage Gateway, AWS\nSnowball, or AWS Direct Connect. Amazon S3 is a durable, scalable, and secure object storage service\nthat can store any amount and type of data.\nCreate an AWS Glue ETL job to perform the ETL processing against the input data: The Data Scientist\ncan use the AWS Glue console, AWS Glue API, AWS SDK, or AWS CLI to create and configure an AWS\nGlue ETL job. The Data Scientist can specify the input and output data sources, the IAM role, the\nsecurity configuration, the job parameters, and the PySpark script location. The Data Scientist can\nalso use the AWS Glue Studio, which is a graphical interface that can help design, run, and monitor\nETL jobs visually.\nWrite the ETL job in PySpark to leverage the existing logic: The Data Scientist can use a code editor of\ntheir choice to write the ETL script in PySpark, using the existing logic to transform the data. The Data\nScientist can also use the AWS Glue script editor, which is an integrated development environment\n(IDE) that can help write, debug, and test the ETL code. The Data Scientist can store the ETL script in\nAmazon S3 or GitHub, and reference it in the AWS Glue ETL job configuration.\nCreate a new AWS Glue trigger to trigger the ETL job based on the existing schedule: The Data\nScientist can use the AWS Glue console, AWS Glue API, AWS SDK, or AWS CLI to create and configure\nan AWS Glue trigger. The Data Scientist can specify the name, type, and schedule of the trigger, and\nassociate it with the AWS Glue ETL job. The trigger will start the ETL job according to the defined\nschedule.\nConfigure the output target of the ETL job to write to a \"processed\" location in Amazon S3 that is\naccessible for downstream use: The Data Scientist can specify the output location of the ETL job in\nthe PySpark script, using the AWS Glue DynamicFrame or Spark DataFrame APIs. The Data Scientist\ncan write the output data to a \"processed\" location in Amazon S3, using a format such as Parquet,\nORC, JSON, or CSV, that is suitable for downstream processing.",
        "references": "What Is AWS Glue?\nAWS Glue Components\nAWS Glue Studio\nAWS Glue Triggers"
    },
    {
        "question": "A large company has developed a B1 application that generates reports and dashboards using\ndata collected from various operational metrics The company wants to provide executives with an\nenhanced experience so they can use natural language to get data from the reports The company\nwants the executives to be able ask questions using written and spoken interlaces Which\ncombination of services can be used to build this conversational interface? (Select THREE)",
        "options": [
            "A. Alexa for Business",
            "B. Amazon Connect",
            "C. Amazon Lex",
            "D. Amazon Poly"
        ],
        "correct": "C. Amazon Lex",
        "explanation": "To build a conversational interface that can use natural language to get data from the reports, theIT Certification Guaranteed, The Easy Way!\n9 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 8\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlcompany can use a combination of services that can handle both written and spoken inputs,\nunderstand the user's intent and query, and extract the relevant information from the reports. The\nservices that can be used for this purpose are:\nAmazon Lex: A service for building conversational interfaces into any application using voice and text.\nAmazon Lex can create chatbots that can interact with users using natural language, and integrate\nwith other AWS services such as Amazon Connect, Amazon Comprehend, and Amazon Transcribe.\nAmazon Lex can also use lambda functions to implement the business logic and fulfill the user's\nrequests.\nAmazon Comprehend: A service for natural language processing and text analytics. Amazon\nComprehend can analyze text and speech inputs and extract insights such as entities, key phrases,\nsentiment, syntax, and topics. Amazon Comprehend can also use custom classifiers and entity\nrecognizers to identify specific terms and concepts that are relevant to the domain of the reports.\nAmazon Transcribe: A service for speech-to-text conversion. Amazon Transcribe can transcribe audio\ninputs into text outputs, and add punctuation and formatting. Amazon Transcribe can also use\ncustom vocabularies and language models to improve the accuracy and quality of the transcription\nfor the specific domain of the reports.\nTherefore, the company can use the following architecture to build the conversational interface:\nUse Amazon Lex to create a chatbot that can accept both written and spoken inputs from the\nexecutives. The chatbot can use intents, utterances, and slots to capture the user's query and\nparameters, such as the report name, date, metric, or filter.\nUse Amazon Transcribe to convert the spoken inputs into text outputs, and pass them to Amazon\nLex. Amazon Transcribe can use a custom vocabulary and language model to recognize the terms and\nconcepts related to the reports.\nUse Amazon Comprehend to analyze the text inputs and outputs, and extract the relevant\ninformation from the reports. Amazon Comprehend can use a custom classifier and entity recognizer\nto identify the report name, date, metric, or filter from the user's query, and the corresponding data\nfrom the reports.\nUse a lambda function to implement the business logic and fulfillment of the user's query, such as\nretrieving the data from the reports, performing calculations or aggregations, and formatting the\nresponse. The lambda function can also handle errors and validations, and provide feedback to the\nuser.\nUse Amazon Lex to return the response to the user, either in text or speech format, depending on the\nuser's preference.",
        "references": "What Is Amazon Lex?\nWhat Is Amazon Comprehend?\nWhat Is Amazon Transcribe?"
    },
    {
        "question": "A Machine Learning Specialist is applying a linear least squares regression model to a dataset\nwith 1 000 records and 50 features Prior to training, the ML Specialist notices that two features are\nperfectly linearly dependent Why could this be an issue for the linear least squares regression model\n?",
        "options": [
            "A. It could cause the backpropagation algorithm to fail during training",
            "B. It could create a singular matrix during optimization which fails to define a unique solution",
            "C. It could modify the loss function during optimization causing it to fail during training",
            "D. It could introduce non-linear dependencies within the data which could invalidate the linearIT Certification Guaranteed, The Easy Way!"
        ],
        "correct": "B. It could create a singular matrix during optimization which fails to define a unique solution",
        "explanation": "Linear least squares regression is a method of fitting a linear model to a set of data by minimizing the\nsum of squared errors between the observed and predicted values. The solution of the linear least\nsquares problem can be obtained by solving the normal equations, which are given by ATAx=ATb,\nwhere A is the matrix of explanatory variables, b is the vector of response variables, and x is the\nvector of unknown coefficients.\nHowever, if the matrix A has two features that are perfectly linearly dependent, then the matrix ATA\nwill be singular, meaning that it does not have a unique inverse. This implies that the normal\nequations do not have a unique solution, and the linear least squares problem is ill-posed. In other\nwords, there are infinitely many values of x that can satisfy the normal equations, and the linear\nmodel is not identifiable.\nThis can be an issue for the linear least squares regression model, as it can lead to instability,\ninconsistency, and poor generalization of the model. It can also cause numerical difficulties when\ntrying to solve the normal equations using computational methods, such as matrix inversion or\ndecomposition. Therefore, it is advisable to avoid or remove the linearly dependent features from\nthe matrix A before applying the linear least squares regression model.",
        "references": "Linear least squares (mathematics)\nLinear Regression in Matrix Form\nSingular Matrix Problem"
    },
    {
        "question": "A Machine Learning Specialist uploads a dataset to an Amazon S3 bucket protected with\nserver-side encryption using AWS KMS.\nHow should the ML Specialist define the Amazon SageMaker notebook instance so it can read the\nsame dataset from Amazon S3?",
        "options": [
            "A. Define security group(s) to allow all HTTP inbound/outbound traffic and assign those security",
            "B. Configure the Amazon SageMaker notebook instance to have access to the VPC. Grant permission",
            "C. Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant",
            "D. Assign the same KMS key used to encrypt data in Amazon S3 to the Amazon SageMaker notebook"
        ],
        "correct": "C. Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant",
        "explanation": "To read data from an Amazon S3 bucket that is protected with server-side encryption using AWS\nKMS, the Amazon SageMaker notebook instance needs to have an IAM role that has permission to\naccess the S3 bucket and the KMS key. The IAM role is an identity that defines the permissions for the\nnotebook instance to interact with other AWS services. The IAM role can be assigned to the notebook\ninstance when it is created or updated later.\nThe KMS key policy is a document that specifies who can use and manage the KMS key. The KMS key\npolicy can grant permission to the IAM role of the notebook instance to decrypt the data in the S3\nbucket. The KMS key policy can also grant permission to other principals, such as AWS accounts, IAMIT Certification Guaranteed, The Easy Way!\n11 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 10\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlusers, or IAM roles, to use the KMS key for encryption and decryption operations.\nTherefore, the Machine Learning Specialist should assign an IAM role to the Amazon SageMaker\nnotebook with S3 read access to the dataset. Grant permission in the KMS key policy to that role. This\nway, the notebook instance can use the IAM role credentials to access the S3 bucket and the KMS\nkey, and read the encrypted data from the S3 bucket.",
        "references": "Create an IAM Role to Grant Permissions to Your Notebook Instance\nUsing Key Policies in AWS KMS"
    },
    {
        "question": "A web-based company wants to improve its conversion rate on its landing page Using a large\nhistorical dataset of customer visits, the company has repeatedly trained a multi-class deep learning\nnetwork algorithm on Amazon SageMaker However there is an overfitting problem training data\nshows 90% accuracy in predictions, while test data shows 70% accuracy only The company needs to\nboost the generalization of its model before deploying it into production to maximize conversions of\nvisits to purchases Which action is recommended to provide the HIGHEST accuracy model for the\ncompany's test and validation data?",
        "options": [
            "A. Increase the randomization of training data in the mini-batches used in training.",
            "B. Allocate a higher proportion of the overall data to the training dataset",
            "C. Apply L1 or L2 regularization and dropouts to the training.",
            "D. Reduce the number of layers and units (or neurons) from the deep learning network."
        ],
        "correct": "C. Apply L1 or L2 regularization and dropouts to the training.",
        "explanation": "Regularization and dropouts are techniques that can help reduce overfitting in deep learning models.\nOverfitting occurs when the model learns too much from the training data and fails to generalize well\nto new data. Regularization adds a penalty term to the loss function that penalizes the model for\nhaving large or complex weights. This prevents the model from memorizing the noise or irrelevant\nfeatures in the training data. L1 and L2 are two types of regularization that differ in how they\ncalculate the penalty term. L1 regularization uses the absolute value of the weights, while L2\nregularization uses the square of the weights. Dropouts are another technique that randomly drops\nout some units or neurons from the network during training. This creates a thinner network that is\nless prone to overfitting. Dropouts also act as a form of ensemble learning, where multiple sub-\nmodels are combined to produce a better prediction. By applying regularization and dropouts to the\ntraining, the web-based company can improve the generalization and accuracy of its deep learning\nmodel on the test and validation data.",
        "references": "Regularization: A video that explains the concept and benefits of regularization in deep learning.\nDropout: A video that demonstrates how dropout works and why it helps reduce overfitting."
    },
    {
        "question": "A Data Scientist is building a model to predict customer churn using a dataset of 100\ncontinuous numerical features. The Marketing team has not provided any insight about which\nfeatures are relevant for churn prediction. The Marketing team wants to interpret the model and see\nthe direct impact of relevant features on the model outcome. While training a logistic regression\nmodel, the Data Scientist observes that there is a wide gap between the training and validation set\naccuracy.\nWhich methods can the Data Scientist use to improve the model performance and satisfy the\nMarketing team's needs? (Choose two.)IT Certification Guaranteed, The Easy Way!\n12 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 11\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.html",
        "options": [
            "A. Add L1 regularization to the classifier",
            "B. Add features to the dataset",
            "C. Perform recursive feature elimination",
            "D. Perform t-distributed stochastic neighbor embedding (t-SNE)"
        ],
        "correct": "A. Add L1 regularization to the classifier",
        "explanation": "The Data Scientist is building a model to predict customer churn using a dataset of 100 continuous\nnumerical features. The Marketing team wants to interpret the model and see the direct impact of\nrelevant features on the model outcome. However, the Data Scientist observes that there is a wide\ngap between the training and validation set accuracy, which indicates that the model is overfitting\nthe data and generalizing poorly to new data.\nTo improve the model performance and satisfy the Marketing team's needs, the Data Scientist can\nuse the following methods:\nAdd L1 regularization to the classifier: L1 regularization is a technique that adds a penalty term to the\nloss function of the logistic regression model, proportional to the sum of the absolute values of the\ncoefficients. L1 regularization can help reduce overfitting by shrinking the coefficients of the less\nimportant features to zero, effectively performing feature selection. This can simplify the model and\nmake it more interpretable, as well as improve the validation accuracy.\nPerform recursive feature elimination: Recursive feature elimination (RFE) is a feature selection\ntechnique that involves training a model on a subset of the features, and then iteratively removing\nthe least important features one by one until the desired number of features is reached. The idea\nbehind RFE is to determine the contribution of each feature to the model by measuring how well the\nmodel performs when that feature is removed. The features that are most important to the model\nwill have the greatest impact on performance when they are removed. RFE can help improve the\nmodel performance by eliminating the irrelevant or redundant features that may cause noise or\nmulticollinearity in the data. RFE can also help the Marketing team understand the direct impact of\nthe relevant features on the model outcome, as the remaining features will have the highest weights\nin the model.",
        "references": "Regularization for Logistic Regression\nRecursive Feature Elimination"
    },
    {
        "question": "An aircraft engine manufacturing company is measuring 200 performance metrics in a time-\nseries. Engineers want to detect critical manufacturing defects in near-real time during testing. All of\nthe data needs to be stored for offline analysis.\nWhat approach would be the MOST effective to perform near-real time defect detection?",
        "options": [
            "A. Use AWS IoT Analytics for ingestion, storage, and further analysis. Use Jupyter notebooks from",
            "B. Use Amazon S3 for ingestion, storage, and further analysis. Use an Amazon EMR cluster to carry",
            "C. Use Amazon S3 for ingestion, storage, and further analysis. Use the Amazon SageMaker Random",
            "D. Use Amazon Kinesis Data Firehose for ingestion and Amazon Kinesis Data Analytics Random Cut"
        ],
        "correct": "D. Use Amazon Kinesis Data Firehose for ingestion and Amazon Kinesis Data Analytics Random Cut",
        "explanation": "The company wants to perform near-real time defect detection on a time-series of 200 performance\nmetrics, and store all the data for offline analysis. The best approach for this scenario is to use\nAmazon Kinesis Data Firehose for ingestion and Amazon Kinesis Data Analytics Random Cut Forest\n(RCF) to perform anomaly detection. Use Kinesis Data Firehose to store data in Amazon S3 for further\nanalysis.\nAmazon Kinesis Data Firehose is a service that can capture, transform, and deliver streaming data to\ndestinations such as Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk. Kinesis\nData Firehose can handle any amount and frequency of data, and automatically scale to match the\nthroughput. Kinesis Data Firehose can also compress, encrypt, and batch the data before delivering it\nto the destination, reducing the storage cost and enhancing the security.\nAmazon Kinesis Data Analytics is a service that can analyze streaming data in real time using SQL or\nApache Flink applications. Kinesis Data Analytics can use built-in functions and algorithms to perform\nvarious analytics tasks, such as aggregations, joins, filters, windows, and anomaly detection. One of\nthe built-in algorithms that Kinesis Data Analytics supports is Random Cut Forest (RCF), which is a\nsupervised learning algorithm for forecasting scalar time series using recurrent neural networks. RCF\ncan detect anomalies in streaming data by assigning an anomaly score to each data point, based on\nhow distant it is from the rest of the data. RCF can handle multiple related time series, such as the\nperformance metrics of the aircraft engine, and learn a global model that captures the common\npatterns and trends across the time series.\nTherefore, the company can use the following architecture to build the near-real time defect\ndetection solution:\nUse Amazon Kinesis Data Firehose for ingestion: The company can use Kinesis Data Firehose to\ncapture the streaming data from the aircraft engine testing, and deliver it to two destinations:\nAmazon S3 and Amazon Kinesis Data Analytics. The company can configure the Kinesis Data Firehose\ndelivery stream to specify the source, the buffer size and interval, the compression and encryption\noptions, the error handling and retry logic, and the destination details.\nUse Amazon Kinesis Data Analytics Random Cut Forest (RCF) to perform anomaly detection: The\ncompany can use Kinesis Data Analytics to create a SQL application that can read the streaming data\nfrom the Kinesis Data Firehose delivery stream, and apply the RCF algorithm to detect anomalies. The\ncompany can use the RANDOM_CUT_FOREST or RANDOM_CUT_FOREST_WITH_EXPLANATION\nfunctions to compute the anomaly scores and attributions for each data point, and use the WHERE\nclause to filter out the normal data points. The company can also use the CURSOR function to specify\nthe input stream, and the PUMP function to write the output stream to another destination, such as\nAmazon Kinesis Data Streams or AWS Lambda.\nUse Kinesis Data Firehose to store data in Amazon S3 for further analysis: The company can use\nKinesis Data Firehose to store the raw and processed data in Amazon S3 for offline analysis. The\ncompany can use the S3 destination of the Kinesis Data Firehose delivery stream to store the raw\ndata, and use another Kinesis Data Firehose delivery stream to store the output of the Kinesis Data\nAnalytics application. The company can also use AWS Glue or Amazon Athena to catalog, query, and\nanalyze the data in Amazon S3.",
        "references": "What Is Amazon Kinesis Data Firehose?IT Certification Guaranteed, The Easy Way!\n14 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 13\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlWhat Is Amazon Kinesis Data Analytics for SQL Applications?\nDeepAR Forecasting Algorithm - Amazon SageMaker"
    },
    {
        "question": "A Machine Learning team runs its own training algorithm on Amazon SageMaker. The training\nalgorithm requires external assets. The team needs to submit both its own algorithm code and\nalgorithm-specific parameters to Amazon SageMaker.\nWhat combination of services should the team use to build a custom algorithm in Amazon\nSageMaker?\n(Choose two.)",
        "options": [
            "A. AWS Secrets Manager",
            "B. AWS CodeStar",
            "C. Amazon ECR",
            "D. Amazon ECS"
        ],
        "correct": "C. Amazon ECR",
        "explanation": "The Machine Learning team wants to use its own training algorithm on Amazon SageMaker, and\nsubmit both its own algorithm code and algorithm-specific parameters. The best combination of\nservices to build a custom algorithm in Amazon SageMaker are Amazon ECR and Amazon S3.\nAmazon ECR is a fully managed container registry service that allows you to store, manage, and\ndeploy Docker container images. You can use Amazon ECR to create a Docker image that contains\nyour training algorithm code and any dependencies or libraries that it requires. You can also use\nAmazon ECR to push, pull, and manage your Docker images securely and reliably.\nAmazon S3 is a durable, scalable, and secure object storage service that can store any amount and\ntype of data. You can use Amazon S3 to store your training data, model artifacts, and algorithm-\nspecific parameters. You can also use Amazon S3 to access your data and parameters from your\ntraining algorithm code, and to write your model output to a specified location.\nTherefore, the Machine Learning team can use the following steps to build a custom algorithm in\nAmazon SageMaker:\nWrite the training algorithm code in Python, using the Amazon SageMaker Python SDK or the\nAmazon SageMaker Containers library to interact with the Amazon SageMaker service. The code\nshould be able to read the input data and parameters from Amazon S3, and write the model output\nto Amazon S3.\nCreate a Dockerfile that defines the base image, the dependencies, the environment variables, and\nthe commands to run the training algorithm code. The Dockerfile should also expose the ports that\nAmazon SageMaker uses to communicate with the container.\nBuild the Docker image using the Dockerfile, and tag it with a meaningful name and version.\nPush the Docker image to Amazon ECR, and note the registry path of the image.\nUpload the training data, model artifacts, and algorithm-specific parameters to Amazon S3, and note\nthe S3 URIs of the objects.\nCreate an Amazon SageMaker training job, using the Amazon SageMaker Python SDK or the AWS CLI.\nSpecify the registry path of the Docker image, the S3 URIs of the input and output data, the\nalgorithm-specific parameters, and other configuration options, such as the instance type, the\nnumber of instances, the IAM role, and the hyperparameters.\nMonitor the status and logs of the training job, and retrieve the model output from Amazon S3.",
        "references": "IT Certification Guaranteed, The Easy Way!\n15 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 14\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlUse Your Own Training Algorithms\nAmazon ECR - Amazon Web Services\nAmazon S3 - Amazon Web Services"
    },
    {
        "question": "A company uses a long short-term memory (LSTM) model to evaluate the risk factors of a\nparticular energy sector. The model reviews multi-page text documents to analyze each sentence of\nthe text and categorize it as either a potential risk or no risk. The model is not performing well, even\nthough the Data Scientist has experimented with many different network structures and tuned the\ncorresponding hyperparameters.\nWhich approach will provide the MAXIMUM performance boost?",
        "options": [
            "A. Initialize the words by term frequency-inverse document frequency (TF-IDF) vectors pretrained on",
            "B. Use gated recurrent units (GRUs) instead of LSTM and run the training process until the validation",
            "C. Reduce the learning rate and run the training process until the training loss stops decreasing.",
            "D. Initialize the words by word2vec embeddings pretrained on a large collection of news articles"
        ],
        "correct": "D. Initialize the words by word2vec embeddings pretrained on a large collection of news articles",
        "explanation": "Initializing the words by word2vec embeddings pretrained on a large collection of news articles\nrelated to the energy sector will provide the maximum performance boost for the LSTM model.\nWord2vec is a technique that learns distributed representations of words based on their co-\noccurrence in a large corpus of text. These representations capture semantic and syntactic\nsimilarities between words, which can help the LSTM model better understand the meaning and\ncontext of the sentences in the text documents. Using word2vec embeddings that are pretrained on\na relevant domain (energy sector) can further improve the performance by reducing the vocabulary\nmismatch and increasing the coverage of the words in the text documents.",
        "references": "AWS Machine Learning Specialty Exam Guide\nAWS Machine Learning Training - Text Classification with TF-IDF, LSTM, BERT: a comparison of\nperformance AWS Machine Learning Training - Machine Learning - Exam Preparation Path"
    },
    {
        "question": "A Machine Learning Specialist previously trained a logistic regression model using scikit-learn\non a local machine, and the Specialist now wants to deploy it to production for inference only.\nWhat steps should be taken to ensure Amazon SageMaker can host a model that was trained locally?",
        "options": [
            "A. Build the Docker image with the inference code. Tag the Docker image with the registry hostname",
            "B. Serialize the trained model so the format is compressed for deployment. Tag the Docker image",
            "C. Serialize the trained model so the format is compressed for deployment. Build the image and",
            "D. Build the Docker image with the inference code. Configure Docker Hub and upload the image to"
        ],
        "correct": "A. Build the Docker image with the inference code. Tag the Docker image with the registry hostname",
        "explanation": "To deploy a model that was trained locally to Amazon SageMaker, the steps are:IT Certification Guaranteed, The Easy Way!\n16 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 15\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlBuild the Docker image with the inference code. The inference code should include the model\nloading, data preprocessing, prediction, and postprocessing logic. The Docker image should also\ninclude the dependencies and libraries required by the inference code and the model.\nTag the Docker image with the registry hostname and upload it to Amazon ECR. Amazon ECR is a fully\nmanaged container registry that makes it easy to store, manage, and deploy container images. The\nregistry hostname is the Amazon ECR registry URI for your account and Region. You can use the AWS\nCLI or the Amazon ECR console to tag and push the Docker image to Amazon ECR.\nCreate a SageMaker model entity that points to the Docker image in Amazon ECR and the model\nartifacts in Amazon S3. The model entity is a logical representation of the model that contains the\ninformation needed to deploy the model for inference. The model artifacts are the files generated by\nthe model training process, such as the model parameters and weights. You can use the AWS CLI, the\nSageMaker Python SDK, or the SageMaker console to create the model entity.\nCreate an endpoint configuration that specifies the instance type and number of instances to use for\nhosting the model. The endpoint configuration also defines the production variants, which are the\ndifferent versions of the model that you want to deploy. You can use the AWS CLI, the SageMaker\nPython SDK, or the SageMaker console to create the endpoint configuration.\nCreate an endpoint that uses the endpoint configuration to deploy the model. The endpoint is a web\nservice that exposes an HTTP API for inference requests. You can use the AWS CLI, the SageMaker\nPython SDK, or the SageMaker console to create the endpoint.",
        "references": "AWS Machine Learning Specialty Exam Guide\nAWS Machine Learning Training - Deploy a Model on Amazon SageMaker\nAWS Machine Learning Training - Use Your Own Inference Code with Amazon SageMaker Hosting\nServices"
    },
    {
        "question": "A trucking company is collecting live image data from its fleet of trucks across the globe. The\ndata is growing rapidly and approximately 100 GB of new data is generated every day. The company\nwants to explore machine learning uses cases while ensuring the data is only accessible to specific\nIAM users.\nWhich storage option provides the most processing flexibility and will allow access control with IAM?",
        "options": [
            "A. Use a database, such as Amazon DynamoDB, to store the images, and set the IAM policies to",
            "B. Use an Amazon S3-backed data lake to store the raw images, and set up the permissions using",
            "C. Setup up Amazon EMR with Hadoop Distributed File System (HDFS) to store the files, and restrict",
            "D. Configure Amazon EFS with IAM policies to make the data available to Amazon EC2 instances"
        ],
        "correct": "B. Use an Amazon S3-backed data lake to store the raw images, and set up the permissions using",
        "explanation": "The best storage option for the trucking company is to use an Amazon S3-backed data lake to store\nthe raw images, and set up the permissions using bucket policies. A data lake is a centralized\nrepository that allows you to store all your structured and unstructured data at any scale. Amazon S3\nis the ideal choice for building a data lake because it offers high durability, scalability, availability, and\nsecurity. You can store any type of data in Amazon S3, such as images, videos, audio, text, etc. You\ncan also use AWS services such as Amazon Rekognition, Amazon SageMaker, and Amazon EMR toIT Certification Guaranteed, The Easy Way!\n17 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 16\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlanalyze and process the data in the data lake. To ensure the data is only accessible to specific IAM\nusers, you can use bucket policies to grant or deny access to the S3 buckets based on the IAM user's\nidentity or role. Bucket policies are JSON documents that specify the permissions for the bucket and\nthe objects in it. You can use conditions to restrict access based on various factors, such as IP address,\ntime, source, etc. By using bucket policies, you can control who can access the data in the data lake\nand what actions they can perform on it.",
        "references": "AWS Machine Learning Specialty Exam Guide\nAWS Machine Learning Training - Build a Data Lake Foundation with Amazon S3 AWS Machine\nLearning Training - Using Bucket Policies and User Policies"
    },
    {
        "question": "A credit card company wants to build a credit scoring model to help predict whether a new\ncredit card applicant will default on a credit card payment. The company has collected data from a\nlarge number of sources with thousands of raw attributes. Early experiments to train a classification\nmodel revealed that many attributes are highly correlated, the large number of features slows down\nthe training speed significantly, and that there are some overfitting issues.\nThe Data Scientist on this project would like to speed up the model training time without losing a lot\nof information from the original dataset.\nWhich feature engineering technique should the Data Scientist use to meet the objectives?",
        "options": [
            "A. Run self-correlation on all features and remove highly correlated features",
            "B. Normalize all numerical values to be between 0 and 1",
            "C. Use an autoencoder or principal component analysis (PCA) to replace original features with new",
            "D. Cluster raw data using k-means and use sample data from each cluster to build a new dataset"
        ],
        "correct": "C. Use an autoencoder or principal component analysis (PCA) to replace original features with new",
        "explanation": "The best feature engineering technique to speed up the model training time without losing a lot of\ninformation from the original dataset is to use an autoencoder or principal component analysis (PCA)\nto replace original features with new features. An autoencoder is a type of neural network that learns\na compressed representation of the input data, called the latent space, by minimizing the\nreconstruction error between the input and the output. PCA is a statistical technique that reduces the\ndimensionality of the data by finding a set of orthogonal axes, called the principal components, that\ncapture the maximum variance of the data. Both techniques can help reduce the number of features\nand remove the noise and redundancy in the data, which can improve the model performance and\nspeed up the training process.",
        "references": "AWS Machine Learning Specialty Exam Guide\nAWS Machine Learning Training - Dimensionality Reduction for Machine Learning AWS Machine\nLearning Training - Deep Learning with Amazon SageMaker"
    },
    {
        "question": "A Data Scientist is training a multilayer perception (MLP) on a dataset with multiple classes.\nThe target class of interest is unique compared to the other classes within the dataset, but it does not\nachieve and acceptable ecall metric. The Data Scientist has already tried varying the number and size\nof the MLP's hidden layers, which has not significantly improved the results. A solution to improve\nrecall must be implemented as quickly as possible.\nWhich techniques should be used to meet these requirements?IT Certification Guaranteed, The Easy Way!\n18 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 17\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.html",
        "options": [
            "A. Gather more data using Amazon Mechanical Turk and then retrain",
            "B. Train an anomaly detection model instead of an MLP",
            "C. Train an XGBoost model instead of an MLP",
            "D. Add class weights to the MLP's loss function and then retrain"
        ],
        "correct": "D. Add class weights to the MLP's loss function and then retrain",
        "explanation": "The best technique to improve the recall of the MLP for the target class of interest is to add class\nweights to the MLP's loss function and then retrain. Class weights are a way of assigning different\nimportance to each class in the dataset, such that the model will pay more attention to the classes\nwith higher weights. This can help mitigate the class imbalance problem, where the model tends to\nfavor the majority class and ignore the minority class. By increasing the weight of the target class of\ninterest, the model will try to reduce the false negatives and increase the true positives, which will\nimprove the recall metric. Adding class weights to the loss function is also a quick and easy solution,\nas it does not require gathering more data, changing the model architecture, or switching to a\ndifferent algorithm.",
        "references": "AWS Machine Learning Specialty Exam Guide\nAWS Machine Learning Training - Deep Learning with Amazon SageMaker\nAWS Machine Learning Training - Class Imbalance and Weighted Loss Functions"
    },
    {
        "question": "A Machine Learning Specialist works for a credit card processing company and needs to\npredict which transactions may be fraudulent in near-real time. Specifically, the Specialist must train\na model that returns the probability that a given transaction may fraudulent.\nHow should the Specialist frame this business problem?",
        "options": [
            "A. Streaming classification",
            "B. Binary classification",
            "C. Multi-category classification",
            "D. Regression classification"
        ],
        "correct": "B. Binary classification",
        "explanation": "The business problem of predicting whether a new credit card applicant will default on a credit card\npayment can be framed as a binary classification problem. Binary classification is the task of\npredicting a discrete class label output for an example, where the class label can only take one of two\npossible values. In this case, the class label can be either \"default\" or \"no default\", indicating whether\nthe applicant will or will not default on a credit card payment. A binary classification model can\nreturn the probability that a given applicant belongs to each class, and then assign the applicant to\nthe class with the highest probability. For example, if the model predicts that an applicant has a 0.8\nprobability of defaulting and a 0.2 probability of not defaulting, then the model will classify the\napplicant as \"default\". Binary classification is suitable for this problem because the outcome of\ninterest is categorical and binary, and the model needs to return the probability of each outcome.",
        "references": "AWS Machine Learning Specialty Exam Guide\nAWS Machine Learning Training - Classification vs Regression in Machine Learning"
    },
    {
        "question": "A real estate company wants to create a machine learning model for predicting housingIT Certification Guaranteed, The Easy Way!\n19 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 18\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.htmlprices based on a historical dataset. The dataset contains 32 features.\nWhich model will meet the business requirement?",
        "options": [
            "A. Logistic regression",
            "B. Linear regression",
            "C. K-means",
            "D. Principal component analysis (PCA)"
        ],
        "correct": "B. Linear regression",
        "explanation": "The best model for predicting housing prices based on a historical dataset with 32 features is linear\nregression. Linear regression is a supervised learning algorithm that fits a linear relationship between\na dependent variable (housing price) and one or more independent variables (features). Linear\nregression can handle multiple features and output a continuous value for the housing price. Linear\nregression can also return the coefficients of the features, which indicate how each feature affects\nthe housing price. Linear regression is suitable for this problem because the outcome of interest is\nnumerical and continuous, and the model needs to capture the linear relationship between the\nfeatures and the outcome.",
        "references": "AWS Machine Learning Specialty Exam Guide\nAWS Machine Learning Training - Regression vs Classification in Machine Learning AWS Machine\nLearning Training - Linear Regression with Amazon SageMaker"
    },
    {
        "question": "A Machine Learning Specialist was given a dataset consisting of unlabeled data The Specialist\nmust create a model that can help the team classify the data into different buckets What model\nshould be used to complete this work?",
        "options": [
            "A. K-means clustering",
            "B. Random Cut Forest (RCF)",
            "C. XGBoost",
            "D. BlazingText"
        ],
        "correct": "A. K-means clustering",
        "explanation": "K-means clustering is a machine learning technique that can be used to classify unlabeled data into\ndifferent groups based on their similarity. It is an unsupervised learning method, which means it does\nnot require any prior knowledge or labels for the data. K-means clustering works by randomly\nassigning data points to a number of clusters, then iteratively updating the cluster centers and\nreassigning the data points until the clusters are stable. The result is a partition of the data into\ndistinct and homogeneous groups. K-means clustering can be useful for exploratory data analysis,\ndata compression, anomaly detection, and feature extraction.",
        "references": "K-Means Clustering: A tutorial on how to use K-means clustering with Amazon SageMaker.\nUnsupervised Learning: A video that explains the concept and applications of unsupervised learning."
    },
    {
        "question": "A Machine Learning Specialist wants to bring a custom algorithm to Amazon SageMaker. The\nSpecialist implements the algorithm in a Docker container supported by Amazon SageMaker.\nHow should the Specialist package the Docker container so that Amazon SageMaker can launch the\ntraining correctly?",
        "options": [
            "A. Modify the bash_profile file in the container and add a bash command to start the trainingIT Certification Guaranteed, The Easy Way!",
            "B. Use CMD config in the Dockerfile to add the training program as a CMD of the image",
            "C. Configure the training program as an ENTRYPOINT named train",
            "D. Copy the training program to directory /opt/ml/train"
        ],
        "correct": "C. Configure the training program as an ENTRYPOINT named train",
        "explanation": "To use a custom algorithm in Amazon SageMaker, the Docker container image must have an\nexecutable file named train that acts as the ENTRYPOINT for the container. This file is responsible for\nrunning the training code and communicating with the Amazon SageMaker service. The train file\nmust be in the PATH of the container and have execute permissions. The other options are not valid\nways to package the Docker container for Amazon SageMaker.",
        "references": "Use Docker containers to build models - Amazon SageMaker\nCreate a container with your own algorithms and models - Amazon SageMaker"
    },
    {
        "question": "A Data Scientist needs to analyze employment dat\na. The dataset contains approximately 10 million\nobservations on people across 10 different features. During the preliminary analysis, the Data\nScientist notices that income and age distributions are not normal. While income levels shows a right\nskew as expected, with fewer individuals having a higher income, the age distribution also show a\nright skew, with fewer older individuals participating in the workforce.\nWhich feature transformations can the Data Scientist apply to fix the incorrectly skewed data?\n(Choose two.)",
        "options": [
            "A. Cross-validation",
            "B. Numerical value binning",
            "C. High-degree polynomial transformation",
            "D. Logarithmic transformation"
        ],
        "correct": "B. Numerical value binning",
        "explanation": "To fix the incorrectly skewed data, the Data Scientist can apply two feature transformations:\nnumerical value binning and logarithmic transformation. Numerical value binning is a technique that\ngroups continuous values into discrete bins or categories. This can help reduce the skewness of the\ndata by creating more balanced frequency distributions. Logarithmic transformation is a technique\nthat applies the natural logarithm function to each value in the data. This can help reduce the right\nskewness of the data by compressing the large values and expanding the small values. Both of these\ntransformations can make the data more suitable for machine learning algorithms that assume\nnormality of the data.",
        "references": "Data Transformation - Amazon SageMaker\nTransforming Skewed Data for Machine Learning"
    },
    {
        "question": "A Machine Learning Specialist is given a structured dataset on the shopping habits of a\ncompany's customer base. The dataset contains thousands of columns of data and hundreds of\nnumerical columns for each customer. The Specialist wants to identify whether there are natural\ngroupings for these columns across all customers and visualize the results as quickly as possible.\nWhat approach should the Specialist take to accomplish these tasks?IT Certification Guaranteed, The Easy Way!\n21 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 20\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.html",
        "options": [
            "A. Embed the numerical features using the t-distributed stochastic neighbor embedding (t-SNE)",
            "B. Run k-means using the Euclidean distance measure for different values of k and create an elbow",
            "C. Embed the numerical features using the t-distributed stochastic neighbor embedding (t-SNE)",
            "D. Run k-means using the Euclidean distance measure for different values of k and create box plots"
        ],
        "correct": "A. Embed the numerical features using the t-distributed stochastic neighbor embedding (t-SNE)",
        "explanation": "The best approach to identify and visualize the natural groupings for the numerical columns across all\ncustomers is to embed the numerical features using the t-distributed stochastic neighbor embedding\n(t-SNE) algorithm and create a scatter plot. t-SNE is a dimensionality reduction technique that can\nproject high-dimensional data into a lower-dimensional space, while preserving the local structure\nand distances of the data points. A scatter plot can then show the clusters of data points in the\nreduced space, where each point represents a customer and the color indicates the cluster\nmembership. This approach can help the Specialist quickly explore the patterns and similarities\namong the customers based on their numerical features.\nThe other options are not as effective or efficient as the t-SNE approach. Running k-means for\ndifferent values of k and creating an elbow plot can help determine the optimal number of clusters,\nbut it does not provide a visual representation of the clusters or the customers. Embedding the\nnumerical features using t-SNE and creating a line graph does not make sense, as a line graph is used\nto show the change of a variable over time, not the distribution of data points in a space. Running k-\nmeans for different values of k and creating box plots for each numerical column within each cluster\ncan provide some insights into the statistics of each cluster, but it is very time-consuming and\ncumbersome to create and compare thousands of box plots.",
        "references": "Dimensionality Reduction - Amazon SageMaker\nVisualize high dimensional data using t-SNE - Amazon SageMaker"
    },
    {
        "question": "A Machine Learning Specialist is planning to create a long-running Amazon EMR cluster. The\nEMR cluster will have 1 master node, 10 core nodes, and 20 task nodes. To save on costs, the\nSpecialist will use Spot Instances in the EMR cluster.\nWhich nodes should the Specialist launch on Spot Instances?",
        "options": [
            "A. Master node",
            "B. Any of the core nodes",
            "C. Any of the task nodes",
            "D. Both core and task nodes"
        ],
        "correct": "C. Any of the task nodes",
        "explanation": "The best option for using Spot Instances in a long-running Amazon EMR cluster is to use them for the\ntask nodes. Task nodes are optional nodes that are used to increase the processing power of the\ncluster. They do not store any data and can be added or removed without affecting the cluster's\noperation. Therefore, they are more resilient to interruptions caused by Spot Instance termination.\nUsing Spot Instances for the master node or the core nodes is not recommended, as they store\nimportant data and metadata for the cluster. If they are terminated, the cluster may fail or lose data.IT Certification Guaranteed, The Easy Way!\n22 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 21\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.html",
        "references": "Amazon EMR on EC2 Spot Instances\nInstance purchasing options - Amazon EMRIT Certification Guaranteed, The Easy Way!\n23 Free Exam/Cram Practice Materials - Best Exam Practice Materials\nGet Latest & Valid Amazon Exam's Question and Answers from Freecram.net. 22\nhttps://www.freecram.net/exam/AWS-Certified-Machine-Learning-Specialty-aws-certified-machine-learning-specialty-\ne10949.html"
    }
]