{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBase  creator\n",
    "This is an example what we want to do, from a text we generate the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "NO.1  A Machine Learning Specialist is working with multiple data sources containing billions of\n",
    "records that need to be joined. What feature engineering and model development approach should\n",
    "the Specialist take with a dataset this large?\n",
    "A. Use an Amazon SageMaker notebook for both feature engineering and model development\n",
    "B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model\n",
    "development\n",
    "C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development\n",
    "D. Use Amazon ML for both feature engineering and model development.\n",
    "Answer:  C\n",
    "Explanation:\n",
    "Amazon EMR is a service that can process large amounts of data efficiently and cost-effectively. It can\n",
    "run distributed frameworks such as Apache Spark, which can perform feature engineering on big\n",
    "data. Amazon SageMaker SDK is a Python library that can interact with Amazon SageMaker service to\n",
    "train and deploy machine learning models. It can also use Amazon EMR as a data source for training\n",
    "data. References:\n",
    "Amazon EMR\n",
    "Amazon SageMaker SDK\n",
    "NO.2  A Machine Learning Specialist is building a supervised model that will evaluate customers'\n",
    "satisfaction with their mobile phone service based on recent usage The model's output should infer\n",
    "whether or not a customer is likely to switch to a competitor in the next 30 days Which of the\n",
    "following modeling techniques should the Specialist use1?\n",
    "A. Time-series prediction\n",
    "B. Anomaly detection\n",
    "C. Binary classification\n",
    "D. Regression\n",
    "Answer:  C\n",
    "Explanation:\n",
    "The modeling technique that the Machine Learning Specialist should use is binary classification.\n",
    "Binary classification is a type of supervised learning that predicts whether an input belongs to one of\n",
    "two possible classes. In this case, the input is the customer's recent usage data and the output is\n",
    "whether or not the customer is likely to switch to a competitor in the next 30 days. This is a binary\n",
    "outcome, either yes or no, so binary classification is suitable for this problem. The other options are\n",
    "not appropriate for this problem. Time-series prediction is a type of supervised learning that\n",
    "forecasts future values based on past and present data. Anomaly detection is a type of unsupervised\n",
    "learning that identifies outliers or abnormal patterns in the data. Regression is a type of supervised\n",
    "learning that estimates a continuous numerical value based on the input features. References: Binary\n",
    "Classification, Time Series Prediction, Anomaly Detection, Regression\n",
    "\n",
    "'''\n",
    "questions=[\n",
    "    {\n",
    "        'question': 'A Machine Learning Specialist is working with multiple data sources containing billions of records that need to be joined. What feature engineering and model development approach should the Specialist take with a dataset this large?',\n",
    "        'options': [\n",
    "            'A. Use an Amazon SageMaker notebook for both feature engineering and model development',\n",
    "            'B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model development',\n",
    "            'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
    "            'D. Use Amazon ML for both feature engineering and model development.'\n",
    "        ],\n",
    "        'correct': 'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
    "        'explanation': 'The modeling technique that the Machine Learning Specialist should use is not applicable in this case, as the question is about feature engineering and model development approach, not about modeling technique. The correct answer is based on the large size of the dataset.',\n",
    "        'references': 'Amazon SageMaker, Amazon EMR, Amazon ML'\n",
    "    },\n",
    "    {\n",
    "        'question': 'A Machine Learning Specialist is building a supervised model that will evaluate customers satisfaction with their mobile phone service based on recent usage. The model\\'s output should infer whether or not a customer is likely to switch to a competitor in the next 30 days. Which of the following modeling techniques should the Specialist use?',\n",
    "        'options': [\n",
    "            'A. Time-series prediction',\n",
    "            'B. Anomaly detection',\n",
    "            'C. Binary classification',\n",
    "            'D. Regression'\n",
    "        ],\n",
    "        'correct': 'C. Binary classification',\n",
    "        'explanation': 'The modeling technique that the Machine Learning Specialist should use is binary classification. Binary classification is a type of supervised learning that predicts whether an input belongs to one of two possible classes. In this case, the input is the customer\\'s recent usage data and the output is whether or not the customer is likely to switch to a competitor in the next 30 days. This is a binary outcome, either yes or no, so binary classification is suitable for this problem.',\n",
    "        'references': 'Binary Classification, Time Series Prediction, Anomaly Detection, Regression'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Define the file path by appending the file name to the current directory\n",
    "pdf_file_path = os.path.join(current_dir, 'exams', 'AWS', 'MLS-C01.pdf')\n",
    "\n",
    "def pdf_to_text(pdf_file_path):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    with open(pdf_file_path, 'rb') as f:\n",
    "        # Create a PyPDF2 reader object\n",
    "        pdf = PyPDF2.PdfReader(f)\n",
    "        \n",
    "        # Initialize an empty string to store the text\n",
    "        text = ''\n",
    "        \n",
    "        # Iterate through all the pages in the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract the text from the page and append it to the text string\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        # Return the extracted text\n",
    "        return text\n",
    "\n",
    "# Call the pdf_to_text function and print the result\n",
    "text = pdf_to_text(pdf_file_path)\n",
    "#print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_v1(text):\n",
    "    questions = []\n",
    "    current_question = None\n",
    "    in_explanation = False\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if re.match(r'NO\\.\\d+', line):\n",
    "            if current_question:\n",
    "                questions.append(current_question)\n",
    "            current_question = {\n",
    "                'question': '',\n",
    "                'options': [],\n",
    "                'correct': '',\n",
    "                'explanation': '',\n",
    "            }\n",
    "            current_question['question'] = re.sub(r'NO\\.\\d+', '', line).strip()\n",
    "            in_explanation = False\n",
    "        elif re.match(r'[A-Z]\\.', line):\n",
    "            current_question['options'].append(line.strip())\n",
    "        elif line.startswith('Answer:'):\n",
    "            current_question['correct'] = line.replace('Answer:', '').strip()\n",
    "        elif line.startswith('Explanation:'):\n",
    "            in_explanation = True\n",
    "            current_question['explanation'] = line.replace('Explanation:', '').strip()\n",
    "        elif line.startswith('References:'):\n",
    "            in_explanation = False\n",
    "            current_question['references'] = line.replace('References:', '').strip()\n",
    "        elif in_explanation:\n",
    "            current_question['explanation'] += ' ' + line\n",
    "    if current_question:\n",
    "        questions.append(current_question)\n",
    "    return questions\n",
    "# Extract questions and format as JSON\n",
    "questions = extract_questions_v1(text)\n",
    "json_questions = json.dumps(questions, indent=4)\n",
    "#print(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_questions(text):\n",
    "    questions = []\n",
    "    \n",
    "    # Split text into individual question blocks\n",
    "    question_blocks = re.split(r'NO\\.\\d+', text)\n",
    "    question_blocks = [block.strip() for block in question_blocks if block.strip()]\n",
    "\n",
    "    for block in question_blocks:\n",
    "        # Split the block into question part, options, answer, explanation, and references\n",
    "        question_part, *rest = re.split(r'(?:[A-D]\\. )', block)\n",
    "        options_part = ' '.join(rest).strip()\n",
    "        \n",
    "        # Extracting options\n",
    "        options = re.findall(r'[A-D]\\. [^\\n]+', block)\n",
    "        \n",
    "        # Extracting the answer and explanation\n",
    "        answer_match = re.search(r'Answer:\\s*([A-D])', block)\n",
    "        explanation_match = re.search(r'Explanation:\\s*(.*?)References:', block, re.DOTALL)\n",
    "        references_match = re.search(r'References:\\s*(.*)', block, re.DOTALL)\n",
    "\n",
    "        if answer_match and explanation_match:\n",
    "            correct_letter = answer_match.group(1)\n",
    "            explanation = explanation_match.group(1).strip()\n",
    "            references = references_match.group(1).strip() if references_match else \"\"\n",
    "\n",
    "            # Get correct option text\n",
    "            correct_index = ord(correct_letter) - ord('A')\n",
    "            if 0 <= correct_index < len(options):\n",
    "                correct_option = options[correct_index]\n",
    "            else:\n",
    "                correct_option = correct_letter\n",
    "\n",
    "            question = {\n",
    "                'question': question_part.strip(),\n",
    "                'options': options,\n",
    "                'correct': correct_option,\n",
    "                'explanation': explanation,\n",
    "                'references': references\n",
    "            }\n",
    "            \n",
    "            questions.append(question)\n",
    "\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"A Machine Learning Specialist is working with multiple data sources containing billions of\\nrecords that need to be joined. What feature engineering and model development approach should\\nthe Specialist take with a dataset this large?\",\n",
      "        \"options\": [\n",
      "            \"A. Use an Amazon SageMaker notebook for both feature engineering and model development\",\n",
      "            \"B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model\",\n",
      "            \"C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development\",\n",
      "            \"D. Use Amazon ML for both feature engineering and model development.\"\n",
      "        ],\n",
      "        \"correct\": \"C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development\",\n",
      "        \"explanation\": \"Amazon EMR is a service that can process large amounts of data efficiently and cost-effectively. It can\\nrun distributed frameworks such as Apache Spark, which can perform feature engineering on big\\ndata. Amazon SageMaker SDK is a Python library that can interact with Amazon SageMaker service to\\ntrain and deploy machine learning models. It can also use Amazon EMR as a data source for training\\ndata.\",\n",
      "        \"references\": \"Amazon EMR\\nAmazon SageMaker SDK\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "questions = extract_questions(text)\n",
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "current_dir = os.getcwd()\n",
    "folder_path = os.path.join(current_dir, 'exams', 'AWS')\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "file_path = os.path.join(folder_path, 'MLS-C01.json')\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(questions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_exam_(exam_name):\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, 'exams', 'AWS')\n",
    "    file_path = os.path.join(folder_path, f'{exam_name}.json')\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "        print(\"Loaded {} questions\".format(len(questions)))    \n",
    "        return questions\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27 questions\n"
     ]
    }
   ],
   "source": [
    "selected_questions = select_exam_('MLS-C01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"A Machine Learning Specialist is working with multiple data sources containing billions of\\nrecords that need to be joined. What feature engineering and model development approach should\\nthe Specialist take with a dataset this large?\",\n",
      "        \"options\": [\n",
      "            \"A. Use an Amazon SageMaker notebook for both feature engineering and model development\",\n",
      "            \"B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model\",\n",
      "            \"C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development\",\n",
      "            \"D. Use Amazon ML for both feature engineering and model development.\"\n",
      "        ],\n",
      "        \"correct\": \"C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development\",\n",
      "        \"explanation\": \"Amazon EMR is a service that can process large amounts of data efficiently and cost-effectively. It can\\nrun distributed frameworks such as Apache Spark, which can perform feature engineering on big\\ndata. Amazon SageMaker SDK is a Python library that can interact with Amazon SageMaker service to\\ntrain and deploy machine learning models. It can also use Amazon EMR as a data source for training\\ndata.\",\n",
      "        \"references\": \"Amazon EMR\\nAmazon SageMaker SDK\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_sets: ['MLS-C01']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Function to load question sets from a directory\n",
    "def load_question_sets(directory='questions'):\n",
    "    question_sets = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            question_sets.append(filename[:-5])  # remove the .json extension\n",
    "    return question_sets\n",
    "exams = load_question_sets('exams/AWS')\n",
    "print(\"question_sets:\", exams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27 questions\n"
     ]
    }
   ],
   "source": [
    "selected_questions = select_exam_('MLS-C01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A Machine Learning Specialist is working with multiple data sources containing billions of\\nrecords that need to be joined. What feature engineering and model development approach should\\nthe Specialist take with a dataset this large?',\n",
       " 'options': ['A. Use an Amazon SageMaker notebook for both feature engineering and model development',\n",
       "  'B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model',\n",
       "  'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
       "  'D. Use Amazon ML for both feature engineering and model development.'],\n",
       " 'correct': 'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
       " 'explanation': 'Amazon EMR is a service that can process large amounts of data efficiently and cost-effectively. It can\\nrun distributed frameworks such as Apache Spark, which can perform feature engineering on big\\ndata. Amazon SageMaker SDK is a Python library that can interact with Amazon SageMaker service to\\ntrain and deploy machine learning models. It can also use Amazon EMR as a data source for training\\ndata.',\n",
       " 'references': 'Amazon EMR\\nAmazon SageMaker SDK'}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_questions[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
