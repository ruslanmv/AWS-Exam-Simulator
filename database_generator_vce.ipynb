{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBase  creator vce format\n",
    "This is an example what we want to do, from a text we generate the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "MLS-C01 \n",
    "Number : 000-000 \n",
    "Passing Score : 800 \n",
    "Time Limit : 120 min \n",
    "File Version : 1.0 \n",
    "  Exam A \n",
    "QUESTION 1 \n",
    "A Machine Learning Specialist is working with multi ple data sources containing billions of records \n",
    "that need to be joined. What feature engineering an d model development approach should the \n",
    "Specialist take with a dataset this large? \n",
    "A. Use an Amazon SageMaker notebook for both feature  engineering and model development \n",
    "B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development \n",
    "C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development \n",
    "D. Use Amazon ML for both feature engineering and mo del development. \n",
    "Correct Answer: C\n",
    "Section: (none) \n",
    "Explanation \n",
    "Explanation/Reference: \n",
    "Explanation: \n",
    "Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It \n",
    "can run distributed frameworks such as Apache Spark , which can perform feature engineering on big \n",
    "data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to \n",
    "train and deploy machine learning models. It can al so use Amazon EMR as a data source for training \n",
    "data. Reference: \n",
    "Amazon EMR \n",
    "Amazon SageMaker SDK \n",
    "'''\n",
    "questions=[\n",
    "    {\n",
    "        'question': 'A Machine Learning Specialist is working with multi ple data sources containing billions of records that need to be joined. What feature engineering an d model development approach should the Specialist take with a dataset this large? ',\n",
    "        'options': [\n",
    "            'A. Use an Amazon SageMaker notebook for both feature  engineering and model development ',\n",
    "            'B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development ',\n",
    "            'C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development ',\n",
    "            'D. Use Amazon ML for both feature engineering and mo del development. '\n",
    "        ],\n",
    "        'correct': 'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
    "        'explanation': 'Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It can run distributed frameworks such as Apache Spark , which can perform feature engineering on big data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to train and deploy machine learning models. It can al so use Amazon EMR as a data source for training data',\n",
    "        'references': 'Amazon SageMaker, Amazon EMR, Amazon ML'\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider=\"GOOGLE\"\n",
    "format='vce'\n",
    "exam='GCP-ML-vA.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Machine Learning Engineer : Practice Test \n",
      "Number : Machine Learning \n",
      "Passing Score : 800 \n",
      "Time Limit : 120 min \n",
      "    \n",
      "Exam Code: Machine Learning Engineer \n",
      "Title : Google Professional Machine Learning Engine er \n",
      "Sections \n",
      "1. I&O&T Managed Services Traversing the Core \n",
      "2. Volume A \n",
      "3. Volume B Exam A \n",
      "QUESTION 1 \n",
      "You are building an ML model to detect anomalies in  real-time sensor data. You will use Pub/Sub to han dle \n",
      "incoming requests. You want to store the results fo r analytics and visualization. How should you confi gure the \n",
      "pipeline? \n",
      "A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery \n",
      "B. 1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable \n",
      "C. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions \n",
      "D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage \n",
      "Correct Answer: A\n",
      "Section: (none) \n",
      "Explanation \n",
      "Explanation/Reference: \n",
      "https://cloud.google.com/solutions/building-anomaly -detection-dataflow-bigqueryml-dlp \n",
      "QUESTION 2 \n",
      "Your organization wants to make its internal shuttl e service route more \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Define the file path by appending the file name to the current directory\n",
    "pdf_file_path = os.path.join(current_dir, 'exams', provider,format ,exam)\n",
    "\n",
    "def pdf_to_text(pdf_file_path):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    with open(pdf_file_path, 'rb') as f:\n",
    "        # Create a PyPDF2 reader object\n",
    "        pdf = PyPDF2.PdfReader(f)\n",
    "        # Initialize an empty string to store the text\n",
    "        text = ''\n",
    "        \n",
    "        # Iterate through all the pages in the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract the text from the page and append it to the text string\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        # Return the extracted text\n",
    "        return text\n",
    "\n",
    "# Call the pdf_to_text function and print the result\n",
    "text = pdf_to_text(pdf_file_path)\n",
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "def extract_questions(text):\n",
    "    questions = []\n",
    "    current_question = None\n",
    "    in_explanation = False\n",
    "    in_references = False\n",
    "    current_section = ''\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Match question number\n",
    "        if re.match(r'QUESTION \\d+', line):\n",
    "            if current_question:\n",
    "                questions.append(current_question)\n",
    "            current_question = {\n",
    "                'question': '',\n",
    "                'options': [],\n",
    "                'correct': '',\n",
    "                'explanation': '',\n",
    "                'references': ''\n",
    "            }\n",
    "            current_section = 'question'\n",
    "            current_question['question'] = ''\n",
    "        # Match options\n",
    "        elif re.match(r'[A-D]\\.', line):\n",
    "            current_section = 'options'\n",
    "            current_question['options'].append(line)\n",
    "        # Match correct answer\n",
    "        elif line.startswith('Correct Answer:'):\n",
    "            current_section = 'correct'\n",
    "            correct_option_letter = line.replace('Correct Answer:', '').strip()\n",
    "            for option in current_question['options']:\n",
    "                if option.startswith(correct_option_letter + '.'):\n",
    "                    current_question['correct'] = option\n",
    "        # Match explanation\n",
    "        elif line.startswith('Explanation'):\n",
    "            current_section = 'explanation'\n",
    "            current_question['explanation'] = line.replace('Explanation:', '').strip()\n",
    "        # Match references\n",
    "        elif line.startswith('Reference:'):\n",
    "            current_section = 'references'\n",
    "            current_question['references'] = line.replace('Reference:', '').strip()\n",
    "        elif current_section == 'question':\n",
    "            current_question['question'] += ' ' + line\n",
    "        elif current_section == 'explanation':\n",
    "            current_question['explanation'] += ' ' + line\n",
    "        elif current_section == 'references':\n",
    "            current_question['references'] += ' ' + line\n",
    "\n",
    "    if current_question:\n",
    "        questions.append(current_question)\n",
    "        \n",
    "    # Clean up extra spaces and format references\n",
    "    for question in questions:\n",
    "        question['question'] = question['question'].strip()\n",
    "        question['explanation'] = question['explanation'].strip()\n",
    "        question['references'] = ' '.join(question['references'].split()).strip()\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"You are building an ML model to detect anomalies in  real-time sensor data. You will use Pub/Sub to han dle incoming requests. You want to store the results fo r analytics and visualization. How should you confi gure the pipeline?\",\n",
      "        \"options\": [\n",
      "            \"A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery\",\n",
      "            \"B. 1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable\",\n",
      "            \"C. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions\",\n",
      "            \"D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage\"\n",
      "        ],\n",
      "        \"correct\": \"A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery\",\n",
      "        \"explanation\": \"Explanation/Reference: https://cloud.google.com/solutions/building-anomaly -detection-dataflow-bigqueryml-dlp\",\n",
      "        \"references\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "questions = extract_questions(text)\n",
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCP-ML-vA.json'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=exam.replace(\".pdf\",\".json\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "current_dir = os.getcwd()\n",
    "folder_path = os.path.join(current_dir, 'exams', provider)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "file_path = os.path.join(folder_path, output)\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(questions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_exam_(exam_name):\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, 'exams', provider )\n",
    "    file_path = os.path.join(folder_path, f'{exam_name}.json')\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "        print(\"Loaded {} questions\".format(len(questions)))    \n",
    "        return questions\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_name='GCP-ML-vA'\n",
    "#exam_name='MLS-C01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75 questions\n"
     ]
    }
   ],
   "source": [
    "selected_questions = select_exam_(exam_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"You are building an ML model to detect anomalies in  real-time sensor data. You will use Pub/Sub to han dle incoming requests. You want to store the results fo r analytics and visualization. How should you confi gure the pipeline?\",\n",
      "        \"options\": [\n",
      "            \"A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery\",\n",
      "            \"B. 1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable\",\n",
      "            \"C. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions\",\n",
      "            \"D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage\"\n",
      "        ],\n",
      "        \"correct\": \"A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery\",\n",
      "        \"explanation\": \"Explanation/Reference: https://cloud.google.com/solutions/building-anomaly -detection-dataflow-bigqueryml-dlp\",\n",
      "        \"references\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_sets: ['DOP-C02-v1', 'MLS-C01-v1', 'MLS-C01-v2', 'MLS-C01-v3', 'MLS-C01-v4', 'MLS-C01', 'SAA-C03-v1', 'SAA-C03-v2', 'SAP-C02-v1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "# Function to load question sets from a directory\n",
    "def load_question_sets(directory='questions'):\n",
    "    question_sets = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            question_sets.append(filename[:-5])  # remove the .json extension\n",
    "    return question_sets\n",
    "exams = load_question_sets('exams/AWS')\n",
    "print(\"question_sets:\", exams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_sets: ['DOP-C02-v1', 'MLS-C01-v1', 'MLS-C01-v2', 'MLS-C01-v3', 'MLS-C01-v4', 'MLS-C01', 'SAA-C03-v1', 'SAA-C03-v2', 'SAP-C02-v1', 'GCP-ML-vA']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_question_sets(directory='questions'):\n",
    "    question_sets = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                question_sets.append(os.path.join( file)[:-5])  # remove the .json extension\n",
    "    return question_sets\n",
    "\n",
    "exams = load_question_sets('exams/')\n",
    "print(\"question_sets:\", exams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP-ML-vA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 264 questions\n"
     ]
    }
   ],
   "source": [
    "provider=\"AWS\"\n",
    "selected_questions = select_exam_('MLS-C01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75 questions\n"
     ]
    }
   ],
   "source": [
    "provider=\"GOOGLE\"\n",
    "selected_questions = select_exam_('GCP-ML-vA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'You are building an ML model to detect anomalies in  real-time sensor data. You will use Pub/Sub to han dle incoming requests. You want to store the results fo r analytics and visualization. How should you confi gure the pipeline?',\n",
       " 'options': ['A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery',\n",
       "  'B. 1 = DataProc, 2 = AutoML, 3 = Cloud Bigtable',\n",
       "  'C. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions',\n",
       "  'D. 1 = BigQuery, 2 = AI Platform, 3 = Cloud Storage'],\n",
       " 'correct': 'A. 1 = Dataflow, 2 = AI Platform, 3 = BigQuery',\n",
       " 'explanation': 'Explanation/Reference: https://cloud.google.com/solutions/building-anomaly -detection-dataflow-bigqueryml-dlp',\n",
       " 'references': ''}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of all json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files: ['MLS-C01-vJune.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pdf_files = []\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Define the file path to search for PDF files\n",
    "pdf_path = os.path.join(current_dir, 'exams', 'AWS', 'vce')\n",
    "# Iterate through all files in the pdf_path directory\n",
    "for filename in os.listdir(pdf_path):\n",
    "    # Check if the file is a PDF file\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        # Append the file name (with extension) to the pdf_files list\n",
    "        pdf_files.append(filename)\n",
    "\n",
    "print(\"PDF files:\", pdf_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files: ['c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\AWS\\\\vce\\\\MLS-C01-vJune.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\GOOGLE\\\\vce\\\\GCP-CA.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\GOOGLE\\\\vce\\\\GCP-ML-vA.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\GOOGLE\\\\vce\\\\GCP-ML-vB.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-102.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-900-v1.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-900-v2.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-900-v3.pdf', 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\DP-100-v1.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "pdf_file_paths=[]\n",
    "current_dir = os.getcwd()\n",
    "directory = os.path.join(current_dir,\"exams\")\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            #pdf_file_paths.append(file)  # remove the .json extension\n",
    "            pdf_file_paths.append(os.path.join(root, file))  # include the full path\n",
    "\n",
    "print(\"PDF files:\", pdf_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\AWS\\\\vce\\\\MLS-C01-vJune.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\GOOGLE\\\\vce\\\\GCP-CA.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\GOOGLE\\\\vce\\\\GCP-ML-vA.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\GOOGLE\\\\vce\\\\GCP-ML-vB.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-102.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-900-v1.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-900-v2.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\AI-900-v3.pdf',\n",
       " 'c:\\\\Dropbox\\\\23-GITHUB\\\\Projects\\\\AWS-Exam-Simulator\\\\exams\\\\MICROSOFT\\\\vce\\\\DP-100-v1.pdf']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed MLS-C01-vJune.pdf and saved as MLS-C01-vJune.json\n",
      "Processed GCP-CA.pdf and saved as GCP-CA.json\n",
      "Processed GCP-ML-vA.pdf and saved as GCP-ML-vA.json\n",
      "Processed GCP-ML-vB.pdf and saved as GCP-ML-vB.json\n",
      "Processed AI-102.pdf and saved as AI-102.json\n",
      "Processed AI-900-v1.pdf and saved as AI-900-v1.json\n",
      "Processed AI-900-v2.pdf and saved as AI-900-v2.json\n",
      "Processed AI-900-v3.pdf and saved as AI-900-v3.json\n",
      "Processed DP-100-v1.pdf and saved as DP-100-v1.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "directory_save = os.path.join(current_dir, \"questions\")\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file_path in pdf_file_paths:\n",
    "    text = pdf_to_text(pdf_file_path)\n",
    "    questions = extract_questions(text)\n",
    "    \n",
    "    # Extract the filename of the pdf_file_path\n",
    "    filename = os.path.basename(pdf_file_path)\n",
    "    \n",
    "    # Save the JSON file\n",
    "    name = filename[:-4]  # remove the .pdf extension\n",
    "    file_path = os.path.join(directory_save, f\"{name}.json\")\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(questions, f, indent=4)\n",
    "    \n",
    "    print(f\"Processed {filename} and saved as {name}.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def select_exam_(exam_name):\n",
    "    file_path = os.path.join(os.getcwd(), 'questions', f'{exam_name}.json')\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "            print(f\"Loaded {len(questions)} questions\")\n",
    "            return questions  # Ensure the questions are returned here\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return []  # Return an empty list to indicate no questions were found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 264 questions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_questions = select_exam_('MLS-C01-v0624')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A Machine Learning Specialist is working with multi ple data sources containing billions of records that need to be joined. What feature engineering an d model development approach should the Specialist take with a dataset this large?',\n",
       " 'options': ['A. Use an Amazon SageMaker notebook for both feature  engineering and model development',\n",
       "  'B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development',\n",
       "  'C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development',\n",
       "  'D. Use Amazon ML for both feature engineering and mo del development.'],\n",
       " 'correct': 'C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development',\n",
       " 'explanation': 'Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It can run distributed frameworks such as Apache Spark , which can perform feature engineering on big data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to train and deploy machine learning models. It can al so use Amazon EMR as a data source for training data. Reference: Amazon EMR Amazon SageMaker SDK',\n",
       " 'references': ''}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
