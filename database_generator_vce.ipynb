{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBase  creator vce format\n",
    "This is an example what we want to do, from a text we generate the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "MLS-C01 \n",
    "Number : 000-000 \n",
    "Passing Score : 800 \n",
    "Time Limit : 120 min \n",
    "File Version : 1.0 \n",
    "  Exam A \n",
    "QUESTION 1 \n",
    "A Machine Learning Specialist is working with multi ple data sources containing billions of records \n",
    "that need to be joined. What feature engineering an d model development approach should the \n",
    "Specialist take with a dataset this large? \n",
    "A. Use an Amazon SageMaker notebook for both feature  engineering and model development \n",
    "B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development \n",
    "C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development \n",
    "D. Use Amazon ML for both feature engineering and mo del development. \n",
    "Correct Answer: C\n",
    "Section: (none) \n",
    "Explanation \n",
    "Explanation/Reference: \n",
    "Explanation: \n",
    "Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It \n",
    "can run distributed frameworks such as Apache Spark , which can perform feature engineering on big \n",
    "data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to \n",
    "train and deploy machine learning models. It can al so use Amazon EMR as a data source for training \n",
    "data. Reference: \n",
    "Amazon EMR \n",
    "Amazon SageMaker SDK \n",
    "'''\n",
    "questions=[\n",
    "    {\n",
    "        'question': 'A Machine Learning Specialist is working with multi ple data sources containing billions of records that need to be joined. What feature engineering an d model development approach should the Specialist take with a dataset this large? ',\n",
    "        'options': [\n",
    "            'A. Use an Amazon SageMaker notebook for both feature  engineering and model development ',\n",
    "            'B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development ',\n",
    "            'C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development ',\n",
    "            'D. Use Amazon ML for both feature engineering and mo del development. '\n",
    "        ],\n",
    "        'correct': 'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
    "        'explanation': 'Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It can run distributed frameworks such as Apache Spark , which can perform feature engineering on big data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to train and deploy machine learning models. It can al so use Amazon EMR as a data source for training data',\n",
    "        'references': 'Amazon SageMaker, Amazon EMR, Amazon ML'\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Define the file path by appending the file name to the current directory\n",
    "pdf_file_path = os.path.join(current_dir, 'exams', 'AWS','vce' ,'MLS-C01-v4.pdf')\n",
    "\n",
    "def pdf_to_text(pdf_file_path):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    with open(pdf_file_path, 'rb') as f:\n",
    "        # Create a PyPDF2 reader object\n",
    "        pdf = PyPDF2.PdfReader(f)\n",
    "        # Initialize an empty string to store the text\n",
    "        text = ''\n",
    "        \n",
    "        # Iterate through all the pages in the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract the text from the page and append it to the text string\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        # Return the extracted text\n",
    "        return text\n",
    "\n",
    "# Call the pdf_to_text function and print the result\n",
    "text = pdf_to_text(pdf_file_path)\n",
    "#print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(text):\n",
    "    questions = []\n",
    "    current_question = None\n",
    "    in_explanation = False\n",
    "    in_references = False\n",
    "    current_section = ''\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Match question number\n",
    "        if re.match(r'QUESTION \\d+', line):\n",
    "            if current_question:\n",
    "                questions.append(current_question)\n",
    "            current_question = {\n",
    "                'question': '',\n",
    "                'options': [],\n",
    "                'correct': '',\n",
    "                'explanation': '',\n",
    "                'references': ''\n",
    "            }\n",
    "            current_section = 'question'\n",
    "            current_question['question'] = ''\n",
    "        # Match options\n",
    "        elif re.match(r'[A-D]\\.', line):\n",
    "            current_section = 'options'\n",
    "            current_question['options'].append(line)\n",
    "        # Match correct answer\n",
    "        elif line.startswith('Correct Answer:'):\n",
    "            current_section = 'correct'\n",
    "            correct_option_letter = line.replace('Correct Answer:', '').strip()\n",
    "            for option in current_question['options']:\n",
    "                if option.startswith(correct_option_letter + '.'):\n",
    "                    current_question['correct'] = option\n",
    "        # Match explanation\n",
    "        elif line.startswith('Explanation'):\n",
    "            current_section = 'explanation'\n",
    "            current_question['explanation'] = line.replace('Explanation:', '').strip()\n",
    "        # Match references\n",
    "        elif line.startswith('Reference:'):\n",
    "            current_section = 'references'\n",
    "            current_question['references'] = line.replace('Reference:', '').strip()\n",
    "        elif current_section == 'question':\n",
    "            current_question['question'] += ' ' + line\n",
    "        elif current_section == 'explanation':\n",
    "            current_question['explanation'] += ' ' + line\n",
    "        elif current_section == 'references':\n",
    "            current_question['references'] += ' ' + line\n",
    "\n",
    "    if current_question:\n",
    "        questions.append(current_question)\n",
    "        \n",
    "    # Clean up extra spaces and format references\n",
    "    for question in questions:\n",
    "        question['question'] = question['question'].strip()\n",
    "        question['explanation'] = question['explanation'].strip()\n",
    "        question['references'] = ' '.join(question['references'].split()).strip()\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"A Machine Learning Specialist is working with multi ple data sources containing billions of records that need to be joined. What feature engineering an d model development approach should the Specialist take with a dataset this large?\",\n",
      "        \"options\": [\n",
      "            \"A. Use an Amazon SageMaker notebook for both feature  engineering and model development\",\n",
      "            \"B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development\",\n",
      "            \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "            \"D. Use Amazon ML for both feature engineering and mo del development.\"\n",
      "        ],\n",
      "        \"correct\": \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "        \"explanation\": \"Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It can run distributed frameworks such as Apache Spark , which can perform feature engineering on big data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to train and deploy machine learning models. It can al so use Amazon EMR as a data source for training data. Reference: Amazon EMR Amazon SageMaker SDK\",\n",
      "        \"references\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "questions = extract_questions(text)\n",
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"A Machine Learning Specialist is working with multi ple data sources containing billions of records that need to be joined. What feature engineering an d model development approach should the Specialist take with a dataset this large?\",\n",
      "        \"options\": [\n",
      "            \"A. Use an Amazon SageMaker notebook for both feature  engineering and model development\",\n",
      "            \"B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development\",\n",
      "            \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "            \"D. Use Amazon ML for both feature engineering and mo del development.\"\n",
      "        ],\n",
      "        \"correct\": \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "        \"explanation\": \"Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It can run distributed frameworks such as Apache Spark , which can perform feature engineering on big data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to train and deploy machine learning models. It can al so use Amazon EMR as a data source for training data. Reference: Amazon EMR Amazon SageMaker SDK\",\n",
      "        \"references\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "questions = extract_questions(text)\n",
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = extract_questions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"A Machine Learning Specialist is working with multi ple data sources containing billions of records that need to be joined. What feature engineering an d model development approach should the Specialist take with a dataset this large?\",\n",
      "        \"options\": [\n",
      "            \"A. Use an Amazon SageMaker notebook for both feature  engineering and model development\",\n",
      "            \"B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development\",\n",
      "            \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "            \"D. Use Amazon ML for both feature engineering and mo del development.\"\n",
      "        ],\n",
      "        \"correct\": \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "        \"explanation\": \"Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It can run distributed frameworks such as Apache Spark , which can perform feature engineering on big data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to train and deploy machine learning models. It can al so use Amazon EMR as a data source for training data. Reference: Amazon EMR Amazon SageMaker SDK\",\n",
      "        \"references\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "current_dir = os.getcwd()\n",
    "folder_path = os.path.join(current_dir, 'exams', 'AWS')\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "file_path = os.path.join(folder_path, 'MLS-C01.json')\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(questions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_exam_(exam_name):\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, 'exams', 'AWS')\n",
    "    file_path = os.path.join(folder_path, f'{exam_name}.json')\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            questions = json.load(f)\n",
    "        print(\"Loaded {} questions\".format(len(questions)))    \n",
    "        return questions\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 264 questions\n"
     ]
    }
   ],
   "source": [
    "selected_questions = select_exam_('MLS-C01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"QUESTION 1\",\n",
      "        \"options\": [\n",
      "            \"A. Use an Amazon SageMaker notebook for both feature  engineering and model development\",\n",
      "            \"B. Use an Amazon SageMaker notebook for feature engi neering and Amazon ML for model development\",\n",
      "            \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "            \"D. Use Amazon ML for both feature engineering and mo del development.\"\n",
      "        ],\n",
      "        \"correct\": \"C. Use Amazon EMR for feature engineering and Amazon  SageMaker SDK for model development\",\n",
      "        \"explanation\": \" Amazon EMR is a service that can process large amou nts of data efficiently and cost-effectively. It can run distributed frameworks such as Apache Spark , which can perform feature engineering on big data. Amazon SageMaker SDK is a Python library that  can interact with Amazon SageMaker service to train and deploy machine learning models. It can al so use Amazon EMR as a data source for training data.\",\n",
      "        \"references\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(questions[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_sets: ['MLS-C01']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Function to load question sets from a directory\n",
    "def load_question_sets(directory='questions'):\n",
    "    question_sets = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            question_sets.append(filename[:-5])  # remove the .json extension\n",
    "    return question_sets\n",
    "exams = load_question_sets('exams/AWS')\n",
    "print(\"question_sets:\", exams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27 questions\n"
     ]
    }
   ],
   "source": [
    "selected_questions = select_exam_('MLS-C01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A Machine Learning Specialist is working with multiple data sources containing billions of\\nrecords that need to be joined. What feature engineering and model development approach should\\nthe Specialist take with a dataset this large?',\n",
       " 'options': ['A. Use an Amazon SageMaker notebook for both feature engineering and model development',\n",
       "  'B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model',\n",
       "  'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
       "  'D. Use Amazon ML for both feature engineering and model development.'],\n",
       " 'correct': 'C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development',\n",
       " 'explanation': 'Amazon EMR is a service that can process large amounts of data efficiently and cost-effectively. It can\\nrun distributed frameworks such as Apache Spark, which can perform feature engineering on big\\ndata. Amazon SageMaker SDK is a Python library that can interact with Amazon SageMaker service to\\ntrain and deploy machine learning models. It can also use Amazon EMR as a data source for training\\ndata.',\n",
       " 'references': 'Amazon EMR\\nAmazon SageMaker SDK'}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of all json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files: ['DOP-C02-v1.pdf', 'MLS-C01-v1.pdf', 'MLS-C01-v2.pdf', 'MLS-C01-v3.pdf', 'MLS-C01-v4.pdf', 'SAA-C03-v1.pdf', 'SAA-C03-v2.pdf', 'SAP-C02-v1.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pdf_files = []\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Define the file path to search for PDF files\n",
    "pdf_path = os.path.join(current_dir, 'exams', 'AWS', 'vce')\n",
    "# Iterate through all files in the pdf_path directory\n",
    "for filename in os.listdir(pdf_path):\n",
    "    # Check if the file is a PDF file\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        # Append the file name (with extension) to the pdf_files list\n",
    "        pdf_files.append(filename)\n",
    "\n",
    "print(\"PDF files:\", pdf_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOP-C02-v1.pdf',\n",
       " 'MLS-C01-v1.pdf',\n",
       " 'MLS-C01-v2.pdf',\n",
       " 'MLS-C01-v3.pdf',\n",
       " 'MLS-C01-v4.pdf',\n",
       " 'SAA-C03-v1.pdf',\n",
       " 'SAA-C03-v2.pdf',\n",
       " 'SAP-C02-v1.pdf']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DOP-C02-v1.pdf and saved as DOP-C02-v1.json\n",
      "Processed MLS-C01-v1.pdf and saved as MLS-C01-v1.json\n",
      "Processed MLS-C01-v2.pdf and saved as MLS-C01-v2.json\n",
      "Processed MLS-C01-v3.pdf and saved as MLS-C01-v3.json\n",
      "Processed MLS-C01-v4.pdf and saved as MLS-C01-v4.json\n",
      "Processed SAA-C03-v1.pdf and saved as SAA-C03-v1.json\n",
      "Processed SAA-C03-v2.pdf and saved as SAA-C03-v2.json\n",
      "Processed SAP-C02-v1.pdf and saved as SAP-C02-v1.json\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Define the folder paths\n",
    "pdf_folder_path = os.path.join(current_dir, 'exams', 'AWS', 'vce')\n",
    "output_folder_path= os.path.join(current_dir, 'exams', 'AWS')\n",
    "# Get the list of PDF files\n",
    "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]\n",
    "# Process each PDF file\n",
    "for filename in pdf_files:\n",
    "    pdf_file_path = os.path.join(pdf_folder_path, filename)\n",
    "    text = pdf_to_text(pdf_file_path)\n",
    "    questions = extract_questions(text)\n",
    "    \n",
    "    # Save the JSON file\n",
    "    name = filename[:-4]  # remove the .pdf extension\n",
    "    file_path = os.path.join(output_folder_path, name + '.json')\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(questions, f, indent=4)\n",
    "\n",
    "    print(f\"Processed {filename} and saved as {name}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
